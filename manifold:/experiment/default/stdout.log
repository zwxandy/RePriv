Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 7 days, 13:47:33 lr 0.100000	data 1.5561 (1.5561)	batch 65.3865 (65.3865)	loss 37.5579 (37.5579)	grad_norm 123.6529 (123.6529)	mem 22427MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 19:49:38 lr 0.100000	data 1.6211 (1.6211)	batch 7.1315 (7.1315)	loss 36.5797 (36.5797)	grad_norm 111.4124 (111.4124)	mem 22427MB
Train: [0/180][1/10009]	eta 1 day, 10:49:09 lr 0.100000	data 0.0008 (0.8109)	batch 17.9184 (12.5249)	loss 5.6998 (21.1398)	grad_norm 7.5937 (59.5030)	mem 22550MB
Train: [0/180][2/10009]	eta 1 day, 4:50:40 lr 0.100000	data 0.0007 (0.5408)	batch 6.0805 (10.3768)	loss 4.4724 (15.5840)	grad_norm 6.3348 (41.7803)	mem 22550MB
Train: [0/180][3/10009]	eta 1 day, 2:03:43 lr 0.100000	data 0.0007 (0.4058)	batch 6.3766 (9.3767)	loss 5.2940 (13.0115)	grad_norm 7.2955 (33.1591)	mem 22550MB
Train: [0/180][4/10009]	eta 23:42:38 lr 0.100000	data 0.0006 (0.3248)	batch 5.1512 (8.5316)	loss 6.3475 (11.6787)	grad_norm 5.6510 (27.6575)	mem 22550MB
Train: [0/180][5/10009]	eta 21:50:44 lr 0.100000	data 0.0007 (0.2708)	batch 4.5097 (7.8613)	loss 6.1862 (10.7633)	grad_norm 5.2645 (23.9253)	mem 22550MB
Train: [0/180][6/10009]	eta 21:25:04 lr 0.100000	data 0.0006 (0.2322)	batch 6.7889 (7.7081)	loss 6.0189 (10.0855)	grad_norm 4.9786 (21.2187)	mem 22550MB
Train: [0/180][7/10009]	eta 1 day, 0:25:01 lr 0.100000	data 0.0007 (0.2032)	batch 16.3502 (8.7884)	loss 5.8864 (9.5606)	grad_norm 4.9322 (19.1828)	mem 22550MB
Train: [0/180][8/10009]	eta 1 day, 1:19:31 lr 0.100000	data 0.0007 (0.1807)	batch 11.7389 (9.1162)	loss 5.6609 (9.1273)	grad_norm 4.9548 (17.6019)	mem 22550MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 11:39:03 lr 0.100000	data 1.7003 (1.7003)	batch 4.1906 (4.1906)	loss 38.0797 (38.0797)	grad_norm 114.4667 (114.4667)	mem 22427MB
Train: [0/180][1/10009]	eta 12:07:45 lr 0.100000	data 0.0008 (0.8505)	batch 4.5355 (4.3630)	loss 7.0630 (22.5714)	grad_norm 5.1022 (59.7845)	mem 22550MB
Train: [0/180][2/10009]	eta 12:06:20 lr 0.100000	data 0.0007 (0.5673)	batch 4.3390 (4.3550)	loss 6.4472 (17.1967)	grad_norm 4.5140 (41.3610)	mem 22550MB
Train: [0/180][3/10009]	eta 10:24:25 lr 0.100000	data 0.0007 (0.4256)	batch 1.9121 (3.7443)	loss 6.0345 (14.4061)	grad_norm 4.5444 (32.1568)	mem 22550MB
Train: [0/180][4/10009]	eta 11:13:50 lr 0.100000	data 0.0007 (0.3406)	batch 5.2281 (4.0410)	loss 6.1712 (12.7591)	grad_norm 5.1904 (26.7635)	mem 22550MB
Train: [0/180][5/10009]	eta 22:19:00 lr 0.100000	data 0.0008 (0.2840)	batch 27.9797 (8.0308)	loss 5.8603 (11.6093)	grad_norm 4.8613 (23.1132)	mem 22550MB
Train: [0/180][6/10009]	eta 20:44:53 lr 0.100000	data 0.0007 (0.2435)	batch 4.0850 (7.4671)	loss 5.4003 (10.7223)	grad_norm 5.0483 (20.5325)	mem 22550MB
Train: [0/180][7/10009]	eta 18:43:44 lr 0.100000	data 0.0007 (0.2132)	batch 1.6592 (6.7411)	loss 5.1221 (10.0223)	grad_norm 5.2639 (18.6239)	mem 22550MB
Train: [0/180][8/10009]	eta 17:29:35 lr 0.100000	data 0.0007 (0.1896)	batch 2.7434 (6.2969)	loss 5.7369 (9.5462)	grad_norm 6.5965 (17.2875)	mem 22550MB
Train: [0/180][9/10009]	eta 16:11:44 lr 0.100000	data 0.0009 (0.1707)	batch 1.6318 (5.8304)	loss 5.6569 (9.1572)	grad_norm 6.0482 (16.1636)	mem 22550MB
Train: [0/180][10/10009]	eta 15:05:48 lr 0.100000	data 0.0006 (0.1552)	batch 1.4850 (5.4354)	loss 5.8271 (8.8545)	grad_norm 5.6088 (15.2041)	mem 22550MB
Train: [0/180][11/10009]	eta 14:24:30 lr 0.100000	data 0.0007 (0.1424)	batch 2.4680 (5.1881)	loss 5.4406 (8.5700)	grad_norm 5.7020 (14.4122)	mem 22550MB
Train: [0/180][12/10009]	eta 13:43:07 lr 0.100000	data 0.0008 (0.1315)	batch 1.9664 (4.9403)	loss 5.5726 (8.3394)	grad_norm 4.9851 (13.6871)	mem 22550MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 9:23:32 lr 0.100000	data 1.7071 (1.7071)	batch 3.3782 (3.3782)	loss 26.3974 (26.3974)	grad_norm 73.4608 (73.4608)	mem 26855MB
Train: [0/180][1/10009]	eta 5:58:35 lr 0.100000	data 0.0008 (0.8540)	batch 0.9215 (2.1499)	loss 22.7330 (24.5652)	grad_norm 40.8019 (57.1313)	mem 26906MB
Train: [0/180][2/10009]	eta 4:50:25 lr 0.100000	data 0.0006 (0.5695)	batch 0.9241 (1.7413)	loss 20.1207 (23.0837)	grad_norm 34.1161 (49.4596)	mem 26906MB
Train: [0/180][3/10009]	eta 4:15:53 lr 0.100000	data 0.0006 (0.4273)	batch 0.9139 (1.5345)	loss 18.7545 (22.0014)	grad_norm 40.0926 (47.1178)	mem 26906MB
Train: [0/180][4/10009]	eta 3:55:03 lr 0.100000	data 0.0007 (0.3420)	batch 0.9102 (1.4096)	loss 19.1915 (21.4394)	grad_norm 37.7239 (45.2390)	mem 26906MB
Train: [0/180][5/10009]	eta 3:41:24 lr 0.100000	data 0.0006 (0.2851)	batch 0.9197 (1.3280)	loss 19.0198 (21.0362)	grad_norm 41.6765 (44.6453)	mem 26906MB
Train: [0/180][6/10009]	eta 3:31:30 lr 0.100000	data 0.0007 (0.2444)	batch 0.9132 (1.2687)	loss 19.1422 (20.7656)	grad_norm 49.1384 (45.2872)	mem 26906MB
Train: [0/180][7/10009]	eta 3:24:02 lr 0.100000	data 0.0007 (0.2140)	batch 0.9112 (1.2240)	loss 20.8601 (20.7774)	grad_norm 47.3300 (45.5425)	mem 26906MB
Train: [0/180][8/10009]	eta 3:18:10 lr 0.100000	data 0.0006 (0.1903)	batch 0.9084 (1.1889)	loss 16.7898 (20.3343)	grad_norm 33.2599 (44.1778)	mem 26906MB
Train: [0/180][9/10009]	eta 3:14:30 lr 0.100000	data 0.0006 (0.1713)	batch 0.9701 (1.1671)	loss 19.0978 (20.2107)	grad_norm 65.5538 (46.3154)	mem 26906MB
Train: [0/180][10/10009]	eta 3:10:36 lr 0.100000	data 0.0006 (0.1558)	batch 0.9106 (1.1437)	loss 18.9018 (20.0917)	grad_norm 28.8896 (44.7312)	mem 26906MB
Train: [0/180][11/10009]	eta 3:07:29 lr 0.100000	data 0.0006 (0.1429)	batch 0.9204 (1.1251)	loss 22.2769 (20.2738)	grad_norm 56.1119 (45.6796)	mem 26906MB
Train: [0/180][12/10009]	eta 3:05:05 lr 0.100000	data 0.0007 (0.1319)	batch 0.9393 (1.1108)	loss 21.1005 (20.3374)	grad_norm 33.2298 (44.7219)	mem 26906MB
Train: [0/180][13/10009]	eta 3:02:40 lr 0.100000	data 0.0006 (0.1225)	batch 0.9105 (1.0965)	loss 25.2456 (20.6880)	grad_norm 99.5603 (48.6390)	mem 26906MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 8:09:41 lr 0.100000	data 1.5474 (1.5474)	batch 2.9355 (2.9355)	loss 27.8142 (27.8142)	grad_norm 75.3061 (75.3061)	mem 26855MB
Train: [0/180][1/10009]	eta 4:51:52 lr 0.100000	data 0.0007 (0.7741)	batch 0.5641 (1.7498)	loss 23.0293 (25.4218)	grad_norm 39.7099 (57.5080)	mem 26906MB
Train: [0/180][2/10009]	eta 3:45:50 lr 0.100000	data 0.0007 (0.5163)	batch 0.5628 (1.3541)	loss 19.5020 (23.4485)	grad_norm 32.5058 (49.1739)	mem 26906MB
Train: [0/180][3/10009]	eta 3:12:55 lr 0.100000	data 0.0007 (0.3874)	batch 0.5650 (1.1569)	loss 19.8951 (22.5601)	grad_norm 47.6515 (48.7933)	mem 26906MB
Train: [0/180][4/10009]	eta 2:52:50 lr 0.100000	data 0.0006 (0.3100)	batch 0.5554 (1.0366)	loss 17.8192 (21.6120)	grad_norm 39.0718 (46.8490)	mem 26906MB
Train: [0/180][5/10009]	eta 2:39:44 lr 0.100000	data 0.0007 (0.2585)	batch 0.5656 (0.9581)	loss 21.1146 (21.5291)	grad_norm 52.5144 (47.7933)	mem 26906MB
Train: [0/180][6/10009]	eta 2:30:21 lr 0.100000	data 0.0007 (0.2216)	batch 0.5644 (0.9018)	loss 17.9364 (21.0158)	grad_norm 29.1966 (45.1366)	mem 26906MB
Train: [0/180][7/10009]	eta 2:23:21 lr 0.100000	data 0.0006 (0.1940)	batch 0.5674 (0.8600)	loss 19.4460 (20.8196)	grad_norm 50.2979 (45.7818)	mem 26906MB
Train: [0/180][8/10009]	eta 2:18:02 lr 0.100000	data 0.0008 (0.1725)	batch 0.5731 (0.8281)	loss 17.5239 (20.4534)	grad_norm 36.8144 (44.7854)	mem 26906MB
Train: [0/180][9/10009]	eta 2:13:36 lr 0.100000	data 0.0008 (0.1554)	batch 0.5635 (0.8017)	loss 20.2752 (20.4356)	grad_norm 45.0071 (44.8076)	mem 26906MB
Train: [0/180][10/10009]	eta 2:09:53 lr 0.100000	data 0.0008 (0.1413)	batch 0.5573 (0.7795)	loss 19.8334 (20.3808)	grad_norm 32.2409 (43.6651)	mem 26906MB
Train: [0/180][11/10009]	eta 2:06:49 lr 0.100000	data 0.0007 (0.1296)	batch 0.5587 (0.7611)	loss 22.4452 (20.5529)	grad_norm 43.9204 (43.6864)	mem 26906MB
Train: [0/180][12/10009]	eta 2:04:11 lr 0.100000	data 0.0007 (0.1197)	batch 0.5572 (0.7454)	loss 19.5837 (20.4783)	grad_norm 36.4902 (43.1329)	mem 26906MB
Train: [0/180][13/10009]	eta 2:01:52 lr 0.100000	data 0.0007 (0.1112)	batch 0.5516 (0.7315)	loss 23.0028 (20.6586)	grad_norm 39.1077 (42.8453)	mem 26906MB
Train: [0/180][14/10009]	eta 1:59:56 lr 0.100000	data 0.0006 (0.1038)	batch 0.5587 (0.7200)	loss 24.0332 (20.8836)	grad_norm 65.0774 (44.3275)	mem 26906MB
Train: [0/180][15/10009]	eta 1:58:16 lr 0.100000	data 0.0007 (0.0974)	batch 0.5613 (0.7101)	loss 23.4919 (21.0466)	grad_norm 46.3392 (44.4532)	mem 26906MB
Train: [0/180][16/10009]	eta 1:56:51 lr 0.100000	data 0.0007 (0.0917)	batch 0.5660 (0.7016)	loss 23.3136 (21.1800)	grad_norm 31.9986 (43.7206)	mem 26906MB
Train: [0/180][17/10009]	eta 1:55:28 lr 0.100000	data 0.0008 (0.0866)	batch 0.5546 (0.6935)	loss 21.4087 (21.1927)	grad_norm 65.5917 (44.9356)	mem 26906MB
Train: [0/180][18/10009]	eta 1:54:17 lr 0.100000	data 0.0007 (0.0821)	batch 0.5597 (0.6864)	loss 25.0056 (21.3934)	grad_norm 49.4138 (45.1713)	mem 26906MB
Train: [0/180][19/10009]	eta 1:53:17 lr 0.100000	data 0.0006 (0.0780)	batch 0.5661 (0.6804)	loss 23.2456 (21.4860)	grad_norm 37.0731 (44.7664)	mem 26906MB
Train: [0/180][20/10009]	eta 1:52:21 lr 0.100000	data 0.0007 (0.0743)	batch 0.5640 (0.6749)	loss 29.6875 (21.8765)	grad_norm 66.8582 (45.8184)	mem 26906MB
Train: [0/180][21/10009]	eta 1:51:29 lr 0.100000	data 0.0007 (0.0710)	batch 0.5628 (0.6698)	loss 23.9024 (21.9686)	grad_norm 35.2160 (45.3365)	mem 26906MB
Train: [0/180][22/10009]	eta 1:51:22 lr 0.100000	data 0.0007 (0.0679)	batch 0.6546 (0.6691)	loss 26.0955 (22.1480)	grad_norm 83.3698 (46.9901)	mem 26906MB
Train: [0/180][23/10009]	eta 1:50:35 lr 0.100000	data 0.0007 (0.0651)	batch 0.5578 (0.6645)	loss 32.4012 (22.5753)	grad_norm 29.6938 (46.2694)	mem 26906MB
Train: [0/180][24/10009]	eta 1:49:51 lr 0.100000	data 0.0006 (0.0626)	batch 0.5557 (0.6601)	loss 26.0180 (22.7130)	grad_norm 46.6810 (46.2859)	mem 26906MB
Train: [0/180][25/10009]	eta 1:49:12 lr 0.100000	data 0.0005 (0.0602)	batch 0.5604 (0.6563)	loss 27.3069 (22.8897)	grad_norm 46.6611 (46.3003)	mem 26906MB
Train: [0/180][26/10009]	eta 1:48:33 lr 0.100000	data 0.0008 (0.0580)	batch 0.5522 (0.6524)	loss 24.1532 (22.9365)	grad_norm 66.5449 (47.0501)	mem 26906MB
Train: [0/180][27/10009]	eta 1:47:59 lr 0.100000	data 0.0007 (0.0559)	batch 0.5606 (0.6491)	loss 27.5579 (23.1015)	grad_norm 40.1352 (46.8032)	mem 26906MB
Train: [0/180][28/10009]	eta 1:47:30 lr 0.100000	data 0.0009 (0.0540)	batch 0.5657 (0.6463)	loss 32.0930 (23.4116)	grad_norm 76.0107 (47.8103)	mem 26906MB
Train: [0/180][29/10009]	eta 1:47:01 lr 0.100000	data 0.0007 (0.0522)	batch 0.5619 (0.6435)	loss 35.4199 (23.8118)	grad_norm 60.5442 (48.2348)	mem 26906MB
Train: [0/180][30/10009]	eta 1:46:33 lr 0.100000	data 0.0007 (0.0506)	batch 0.5571 (0.6407)	loss 31.4892 (24.0595)	grad_norm 78.9522 (49.2257)	mem 26906MB
Train: [0/180][31/10009]	eta 1:46:08 lr 0.100000	data 0.0007 (0.0490)	batch 0.5635 (0.6383)	loss 35.5518 (24.4186)	grad_norm 44.2880 (49.0714)	mem 26906MB
Train: [0/180][32/10009]	eta 1:45:45 lr 0.100000	data 0.0007 (0.0476)	batch 0.5627 (0.6360)	loss 36.8290 (24.7947)	grad_norm 81.7821 (50.0626)	mem 26906MB
Train: [0/180][33/10009]	eta 1:45:21 lr 0.100000	data 0.0007 (0.0462)	batch 0.5593 (0.6337)	loss 33.7792 (25.0589)	grad_norm 38.2020 (49.7138)	mem 26906MB
Train: [0/180][34/10009]	eta 1:45:01 lr 0.100000	data 0.0007 (0.0449)	batch 0.5635 (0.6317)	loss 40.6846 (25.5054)	grad_norm 67.2346 (50.2144)	mem 26906MB
Train: [0/180][35/10009]	eta 1:44:41 lr 0.100000	data 0.0007 (0.0437)	batch 0.5638 (0.6298)	loss 36.4452 (25.8093)	grad_norm 43.1692 (50.0187)	mem 26906MB
Train: [0/180][36/10009]	eta 1:44:22 lr 0.100000	data 0.0006 (0.0425)	batch 0.5622 (0.6280)	loss 34.2690 (26.0379)	grad_norm 51.4872 (50.0583)	mem 26906MB
Train: [0/180][37/10009]	eta 1:44:01 lr 0.100000	data 0.0008 (0.0414)	batch 0.5490 (0.6259)	loss 39.4214 (26.3901)	grad_norm 82.5382 (50.9131)	mem 26906MB
Train: [0/180][38/10009]	eta 1:43:44 lr 0.100000	data 0.0007 (0.0403)	batch 0.5604 (0.6242)	loss 32.8820 (26.5566)	grad_norm 29.6064 (50.3668)	mem 26906MB
Train: [0/180][39/10009]	eta 1:43:27 lr 0.100000	data 0.0007 (0.0394)	batch 0.5611 (0.6227)	loss 31.4674 (26.6793)	grad_norm 83.2780 (51.1895)	mem 26906MB
Train: [0/180][40/10009]	eta 1:43:15 lr 0.100000	data 0.0007 (0.0384)	batch 0.5728 (0.6214)	loss 37.5128 (26.9436)	grad_norm 46.2465 (51.0690)	mem 26906MB
Train: [0/180][41/10009]	eta 1:43:00 lr 0.100000	data 0.0008 (0.0375)	batch 0.5606 (0.6200)	loss 55.2209 (27.6168)	grad_norm 77.7599 (51.7045)	mem 26906MB
Train: [0/180][42/10009]	eta 1:42:44 lr 0.100000	data 0.0007 (0.0367)	batch 0.5564 (0.6185)	loss 35.5596 (27.8016)	grad_norm 57.0173 (51.8280)	mem 26906MB
Train: [0/180][43/10009]	eta 1:42:33 lr 0.100000	data 0.0007 (0.0358)	batch 0.5709 (0.6174)	loss 47.0875 (28.2399)	grad_norm 82.2402 (52.5192)	mem 26906MB
Train: [0/180][44/10009]	eta 1:42:21 lr 0.100000	data 0.0007 (0.0351)	batch 0.5674 (0.6163)	loss 36.6585 (28.4270)	grad_norm 55.1354 (52.5774)	mem 26906MB
Train: [0/180][45/10009]	eta 1:42:08 lr 0.100000	data 0.0006 (0.0343)	batch 0.5589 (0.6151)	loss 44.0075 (28.7657)	grad_norm 74.0005 (53.0431)	mem 26906MB
Train: [0/180][46/10009]	eta 1:41:59 lr 0.100000	data 0.0007 (0.0336)	batch 0.5736 (0.6142)	loss 43.3645 (29.0763)	grad_norm 39.6486 (52.7581)	mem 26906MB
Train: [0/180][47/10009]	eta 1:41:48 lr 0.100000	data 0.0007 (0.0329)	batch 0.5642 (0.6131)	loss 44.2165 (29.3917)	grad_norm 112.4771 (54.0022)	mem 26906MB
Train: [0/180][48/10009]	eta 1:41:38 lr 0.100000	data 0.0008 (0.0323)	batch 0.5681 (0.6122)	loss 45.8858 (29.7283)	grad_norm 67.9671 (54.2872)	mem 26906MB
Train: [0/180][49/10009]	eta 1:41:28 lr 0.100000	data 0.0007 (0.0316)	batch 0.5647 (0.6113)	loss 50.3188 (30.1401)	grad_norm 36.5513 (53.9325)	mem 26906MB
Train: [0/180][50/10009]	eta 1:41:18 lr 0.100000	data 0.0007 (0.0310)	batch 0.5633 (0.6103)	loss 51.8610 (30.5660)	grad_norm 75.1870 (54.3493)	mem 26906MB
Train: [0/180][51/10009]	eta 1:41:10 lr 0.100000	data 0.0006 (0.0304)	batch 0.5712 (0.6096)	loss 56.8093 (31.0707)	grad_norm 65.9274 (54.5719)	mem 26906MB
Train: [0/180][52/10009]	eta 1:41:00 lr 0.100000	data 0.0007 (0.0299)	batch 0.5613 (0.6087)	loss 50.0195 (31.4282)	grad_norm 42.9911 (54.3534)	mem 26906MB
Train: [0/180][53/10009]	eta 1:40:53 lr 0.100000	data 0.0007 (0.0293)	batch 0.5715 (0.6080)	loss 45.6238 (31.6911)	grad_norm 84.4222 (54.9102)	mem 26906MB
Train: [0/180][54/10009]	eta 1:40:44 lr 0.100000	data 0.0006 (0.0288)	batch 0.5661 (0.6072)	loss 60.4728 (32.2144)	grad_norm 47.7884 (54.7807)	mem 26906MB
Train: [0/180][55/10009]	eta 1:40:36 lr 0.100000	data 0.0007 (0.0283)	batch 0.5644 (0.6065)	loss 41.7914 (32.3854)	grad_norm 56.2078 (54.8062)	mem 26906MB
Train: [0/180][56/10009]	eta 1:40:29 lr 0.100000	data 0.0007 (0.0278)	batch 0.5684 (0.6058)	loss 49.6795 (32.6888)	grad_norm 81.5393 (55.2752)	mem 26906MB
Train: [0/180][57/10009]	eta 1:40:23 lr 0.100000	data 0.0007 (0.0274)	batch 0.5747 (0.6053)	loss 61.2709 (33.1816)	grad_norm 54.1279 (55.2555)	mem 26906MB
Train: [0/180][58/10009]	eta 1:40:16 lr 0.100000	data 0.0006 (0.0269)	batch 0.5674 (0.6046)	loss 54.8076 (33.5482)	grad_norm 44.0820 (55.0661)	mem 26906MB
Train: [0/180][59/10009]	eta 1:40:09 lr 0.100000	data 0.0007 (0.0265)	batch 0.5680 (0.6040)	loss 51.9100 (33.8542)	grad_norm 56.3014 (55.0867)	mem 26906MB
Train: [0/180][60/10009]	eta 1:40:02 lr 0.100000	data 0.0007 (0.0260)	batch 0.5602 (0.6033)	loss 72.9574 (34.4952)	grad_norm 37.0034 (54.7902)	mem 26906MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 8:24:24 lr 0.100000	data 1.8413 (1.8413)	batch 3.0237 (3.0237)	loss 28.7230 (28.7230)	grad_norm 77.1342 (77.1342)	mem 26855MB
Train: [0/180][1/10009]	eta 4:58:55 lr 0.100000	data 0.0008 (0.9211)	batch 0.5605 (1.7921)	loss 21.7650 (25.2440)	grad_norm 34.6042 (55.8692)	mem 26906MB
Train: [0/180][2/10009]	eta 3:50:32 lr 0.100000	data 0.0011 (0.6144)	batch 0.5626 (1.3823)	loss 18.6644 (23.0508)	grad_norm 36.1296 (49.2893)	mem 26906MB
Train: [0/180][3/10009]	eta 3:15:52 lr 0.100000	data 0.0007 (0.4610)	batch 0.5515 (1.1746)	loss 19.6869 (22.2098)	grad_norm 37.4128 (46.3202)	mem 26906MB
Train: [0/180][4/10009]	eta 2:55:31 lr 0.100000	data 0.0007 (0.3689)	batch 0.5650 (1.0527)	loss 18.4910 (21.4661)	grad_norm 32.9085 (43.6378)	mem 26906MB
Train: [0/180][5/10009]	eta 2:41:54 lr 0.100000	data 0.0007 (0.3075)	batch 0.5631 (0.9711)	loss 18.6240 (20.9924)	grad_norm 43.6065 (43.6326)	mem 26906MB
Train: [0/180][6/10009]	eta 2:32:11 lr 0.100000	data 0.0006 (0.2637)	batch 0.5639 (0.9129)	loss 21.1713 (21.0179)	grad_norm 65.5995 (46.7707)	mem 26906MB
Train: [0/180][7/10009]	eta 2:25:03 lr 0.100000	data 0.0006 (0.2308)	batch 0.5710 (0.8702)	loss 18.1696 (20.6619)	grad_norm 25.6874 (44.1353)	mem 26906MB
Train: [0/180][8/10009]	eta 2:19:23 lr 0.100000	data 0.0007 (0.2052)	batch 0.5650 (0.8363)	loss 19.8518 (20.5719)	grad_norm 44.6850 (44.1964)	mem 26906MB
Train: [0/180][9/10009]	eta 2:14:43 lr 0.100000	data 0.0007 (0.1848)	batch 0.5574 (0.8084)	loss 20.3611 (20.5508)	grad_norm 60.8952 (45.8663)	mem 26906MB
Train: [0/180][10/10009]	eta 2:11:02 lr 0.100000	data 0.0006 (0.1680)	batch 0.5658 (0.7863)	loss 20.7567 (20.5695)	grad_norm 27.2558 (44.1744)	mem 26906MB
Train: [0/180][11/10009]	eta 2:08:01 lr 0.100000	data 0.0006 (0.1541)	batch 0.5695 (0.7683)	loss 19.4089 (20.4728)	grad_norm 43.3519 (44.1059)	mem 26906MB
Train: [0/180][12/10009]	eta 2:05:19 lr 0.100000	data 0.0007 (0.1423)	batch 0.5590 (0.7522)	loss 22.0644 (20.5952)	grad_norm 71.9917 (46.2509)	mem 26906MB
Train: [0/180][13/10009]	eta 2:03:01 lr 0.100000	data 0.0006 (0.1322)	batch 0.5598 (0.7384)	loss 20.6247 (20.5973)	grad_norm 35.3024 (45.4689)	mem 26906MB
Train: [0/180][14/10009]	eta 2:01:07 lr 0.100000	data 0.0006 (0.1234)	batch 0.5689 (0.7271)	loss 22.2699 (20.7088)	grad_norm 34.5500 (44.7410)	mem 26906MB
Train: [0/180][15/10009]	eta 1:59:20 lr 0.100000	data 0.0006 (0.1157)	batch 0.5572 (0.7165)	loss 25.2127 (20.9903)	grad_norm 116.1570 (49.2045)	mem 26906MB
Train: [0/180][16/10009]	eta 1:57:50 lr 0.100000	data 0.0008 (0.1090)	batch 0.5637 (0.7075)	loss 22.0969 (21.0554)	grad_norm 43.3849 (48.8622)	mem 26906MB
Train: [0/180][17/10009]	eta 1:56:30 lr 0.100000	data 0.0006 (0.1029)	batch 0.5654 (0.6996)	loss 22.7717 (21.1508)	grad_norm 80.4408 (50.6165)	mem 26906MB
Train: [0/180][18/10009]	eta 1:55:11 lr 0.100000	data 0.0008 (0.0976)	batch 0.5515 (0.6918)	loss 25.7288 (21.3917)	grad_norm 36.7960 (49.8891)	mem 26906MB
Train: [0/180][19/10009]	eta 1:54:07 lr 0.100000	data 0.0006 (0.0927)	batch 0.5632 (0.6854)	loss 25.7033 (21.6073)	grad_norm 37.5661 (49.2730)	mem 26906MB
Train: [0/180][20/10009]	eta 1:53:13 lr 0.100000	data 0.0007 (0.0883)	batch 0.5751 (0.6801)	loss 28.7707 (21.9484)	grad_norm 64.4394 (49.9952)	mem 26906MB
Train: [0/180][21/10009]	eta 1:52:18 lr 0.100000	data 0.0010 (0.0844)	batch 0.5605 (0.6747)	loss 29.1158 (22.2742)	grad_norm 34.9182 (49.3099)	mem 26906MB
Train: [0/180][22/10009]	eta 1:52:15 lr 0.100000	data 0.0007 (0.0807)	batch 0.6674 (0.6744)	loss 24.8678 (22.3870)	grad_norm 38.7750 (48.8518)	mem 26906MB
Train: [0/180][23/10009]	eta 1:51:31 lr 0.100000	data 0.0007 (0.0774)	batch 0.5712 (0.6701)	loss 32.7293 (22.8179)	grad_norm 53.3377 (49.0387)	mem 26906MB
Train: [0/180][24/10009]	eta 1:50:47 lr 0.100000	data 0.0008 (0.0743)	batch 0.5614 (0.6657)	loss 28.9735 (23.0641)	grad_norm 51.6710 (49.1440)	mem 26906MB
Train: [0/180][25/10009]	eta 1:50:07 lr 0.100000	data 0.0007 (0.0715)	batch 0.5629 (0.6618)	loss 32.7713 (23.4375)	grad_norm 76.3217 (50.1893)	mem 26906MB
Train: [0/180][26/10009]	eta 1:49:27 lr 0.100000	data 0.0007 (0.0689)	batch 0.5572 (0.6579)	loss 27.1392 (23.5746)	grad_norm 29.9385 (49.4393)	mem 26906MB
Train: [0/180][27/10009]	eta 1:48:56 lr 0.100000	data 0.0008 (0.0664)	batch 0.5713 (0.6548)	loss 28.5346 (23.7517)	grad_norm 62.2167 (49.8956)	mem 26906MB
Train: [0/180][28/10009]	eta 1:48:27 lr 0.100000	data 0.0007 (0.0642)	batch 0.5721 (0.6520)	loss 35.9955 (24.1739)	grad_norm 54.0282 (50.0381)	mem 26906MB
Train: [0/180][29/10009]	eta 1:47:54 lr 0.100000	data 0.0008 (0.0621)	batch 0.5561 (0.6488)	loss 25.2564 (24.2100)	grad_norm 79.2116 (51.0106)	mem 26906MB
Train: [0/180][30/10009]	eta 1:47:24 lr 0.100000	data 0.0007 (0.0601)	batch 0.5560 (0.6458)	loss 33.2489 (24.5016)	grad_norm 46.4558 (50.8637)	mem 26906MB
Train: [0/180][31/10009]	eta 1:46:56 lr 0.100000	data 0.0006 (0.0582)	batch 0.5578 (0.6430)	loss 26.5656 (24.5661)	grad_norm 55.8835 (51.0205)	mem 26906MB
Train: [0/180][32/10009]	eta 1:46:31 lr 0.100000	data 0.0006 (0.0565)	batch 0.5640 (0.6406)	loss 32.7904 (24.8153)	grad_norm 62.3089 (51.3626)	mem 26906MB
Train: [0/180][33/10009]	eta 1:46:08 lr 0.100000	data 0.0007 (0.0548)	batch 0.5633 (0.6384)	loss 36.7843 (25.1673)	grad_norm 31.2786 (50.7719)	mem 26906MB
Train: [0/180][34/10009]	eta 1:45:45 lr 0.100000	data 0.0007 (0.0533)	batch 0.5617 (0.6362)	loss 44.0309 (25.7063)	grad_norm 83.4683 (51.7061)	mem 26906MB
Train: [0/180][35/10009]	eta 1:45:25 lr 0.100000	data 0.0006 (0.0518)	batch 0.5652 (0.6342)	loss 41.5720 (26.1470)	grad_norm 35.0630 (51.2438)	mem 26906MB
Train: [0/180][36/10009]	eta 1:45:05 lr 0.100000	data 0.0007 (0.0504)	batch 0.5636 (0.6323)	loss 34.4310 (26.3709)	grad_norm 75.2352 (51.8922)	mem 26906MB
Train: [0/180][37/10009]	eta 1:44:48 lr 0.100000	data 0.0009 (0.0491)	batch 0.5678 (0.6306)	loss 40.8533 (26.7520)	grad_norm 65.7778 (52.2576)	mem 26906MB
Train: [0/180][38/10009]	eta 1:44:31 lr 0.100000	data 0.0006 (0.0479)	batch 0.5663 (0.6289)	loss 34.3278 (26.9463)	grad_norm 27.8759 (51.6324)	mem 26906MB
Train: [0/180][39/10009]	eta 1:44:15 lr 0.100000	data 0.0008 (0.0467)	batch 0.5675 (0.6274)	loss 38.1191 (27.2256)	grad_norm 99.0382 (52.8176)	mem 26906MB
Train: [0/180][40/10009]	eta 1:43:58 lr 0.100000	data 0.0007 (0.0456)	batch 0.5619 (0.6258)	loss 36.0969 (27.4420)	grad_norm 43.7684 (52.5968)	mem 26906MB
Train: [0/180][41/10009]	eta 1:43:43 lr 0.100000	data 0.0006 (0.0445)	batch 0.5651 (0.6244)	loss 35.7616 (27.6400)	grad_norm 60.4346 (52.7835)	mem 26906MB
Train: [0/180][42/10009]	eta 1:43:26 lr 0.100000	data 0.0007 (0.0435)	batch 0.5520 (0.6227)	loss 38.8055 (27.8997)	grad_norm 33.0358 (52.3242)	mem 26906MB
Train: [0/180][43/10009]	eta 1:43:11 lr 0.100000	data 0.0007 (0.0425)	batch 0.5599 (0.6213)	loss 50.0087 (28.4022)	grad_norm 108.2059 (53.5942)	mem 26906MB
Train: [0/180][44/10009]	eta 1:42:57 lr 0.100000	data 0.0007 (0.0416)	batch 0.5633 (0.6200)	loss 45.1612 (28.7746)	grad_norm 42.6523 (53.3511)	mem 26906MB
Train: [0/180][45/10009]	eta 1:42:46 lr 0.100000	data 0.0007 (0.0407)	batch 0.5718 (0.6189)	loss 48.6807 (29.2073)	grad_norm 36.6495 (52.9880)	mem 26906MB
Train: [0/180][46/10009]	eta 1:42:33 lr 0.100000	data 0.0006 (0.0399)	batch 0.5576 (0.6176)	loss 41.5446 (29.4698)	grad_norm 127.5923 (54.5753)	mem 26906MB
Train: [0/180][47/10009]	eta 1:42:21 lr 0.100000	data 0.0006 (0.0390)	batch 0.5627 (0.6165)	loss 47.9733 (29.8553)	grad_norm 51.2293 (54.5056)	mem 26906MB
Train: [0/180][48/10009]	eta 1:42:09 lr 0.100000	data 0.0007 (0.0383)	batch 0.5604 (0.6153)	loss 41.3544 (30.0900)	grad_norm 78.3402 (54.9921)	mem 26906MB
Train: [0/180][49/10009]	eta 1:41:56 lr 0.100000	data 0.0006 (0.0375)	batch 0.5551 (0.6141)	loss 47.7704 (30.4436)	grad_norm 36.5838 (54.6239)	mem 26906MB
Train: [0/180][50/10009]	eta 1:41:46 lr 0.100000	data 0.0007 (0.0368)	batch 0.5628 (0.6131)	loss 43.0322 (30.6904)	grad_norm 44.2806 (54.4211)	mem 26906MB
Train: [0/180][51/10009]	eta 1:41:36 lr 0.100000	data 0.0007 (0.0361)	batch 0.5689 (0.6123)	loss 49.5006 (31.0522)	grad_norm 86.9426 (55.0465)	mem 26906MB
Train: [0/180][52/10009]	eta 1:41:26 lr 0.100000	data 0.0006 (0.0354)	batch 0.5611 (0.6113)	loss 49.3766 (31.3979)	grad_norm 36.2742 (54.6923)	mem 26906MB
Train: [0/180][53/10009]	eta 1:41:17 lr 0.100000	data 0.0006 (0.0348)	batch 0.5654 (0.6105)	loss 40.2980 (31.5627)	grad_norm 53.8339 (54.6764)	mem 26906MB
Train: [0/180][54/10009]	eta 1:41:08 lr 0.100000	data 0.0007 (0.0342)	batch 0.5616 (0.6096)	loss 35.2618 (31.6300)	grad_norm 73.3208 (55.0154)	mem 26906MB
Train: [0/180][55/10009]	eta 1:40:59 lr 0.100000	data 0.0006 (0.0336)	batch 0.5651 (0.6088)	loss 50.5523 (31.9679)	grad_norm 42.7497 (54.7964)	mem 26906MB
Train: [0/180][56/10009]	eta 1:40:51 lr 0.100000	data 0.0008 (0.0330)	batch 0.5627 (0.6080)	loss 41.9973 (32.1439)	grad_norm 58.9164 (54.8686)	mem 26906MB
Train: [0/180][57/10009]	eta 1:40:42 lr 0.100000	data 0.0007 (0.0324)	batch 0.5642 (0.6072)	loss 45.8533 (32.3802)	grad_norm 55.1549 (54.8736)	mem 26906MB
Train: [0/180][58/10009]	eta 1:40:33 lr 0.100000	data 0.0008 (0.0319)	batch 0.5523 (0.6063)	loss 56.7043 (32.7925)	grad_norm 66.6071 (55.0725)	mem 26906MB
Train: [0/180][59/10009]	eta 1:40:25 lr 0.100000	data 0.0006 (0.0314)	batch 0.5654 (0.6056)	loss 52.2608 (33.1170)	grad_norm 30.9190 (54.6699)	mem 26906MB
Train: [0/180][60/10009]	eta 1:40:18 lr 0.100000	data 0.0007 (0.0309)	batch 0.5628 (0.6049)	loss 43.9959 (33.2953)	grad_norm 71.3177 (54.9428)	mem 26906MB
Train: [0/180][61/10009]	eta 1:40:11 lr 0.100000	data 0.0006 (0.0304)	batch 0.5681 (0.6043)	loss 49.5930 (33.5582)	grad_norm 47.7364 (54.8266)	mem 26906MB
Train: [0/180][62/10009]	eta 1:40:04 lr 0.100000	data 0.0006 (0.0299)	batch 0.5619 (0.6036)	loss 48.5813 (33.7966)	grad_norm 50.8470 (54.7634)	mem 26906MB
Train: [0/180][63/10009]	eta 1:39:58 lr 0.100000	data 0.0007 (0.0295)	batch 0.5691 (0.6031)	loss 54.7481 (34.1240)	grad_norm 58.1497 (54.8163)	mem 26906MB
Train: [0/180][64/10009]	eta 1:39:51 lr 0.100000	data 0.0007 (0.0290)	batch 0.5637 (0.6025)	loss 53.9134 (34.4285)	grad_norm 66.6405 (54.9982)	mem 26906MB
Train: [0/180][65/10009]	eta 1:39:45 lr 0.100000	data 0.0006 (0.0286)	batch 0.5683 (0.6020)	loss 52.1937 (34.6976)	grad_norm 41.3632 (54.7916)	mem 26906MB
Train: [0/180][66/10009]	eta 1:39:39 lr 0.100000	data 0.0007 (0.0282)	batch 0.5613 (0.6014)	loss 49.8754 (34.9242)	grad_norm 53.7673 (54.7764)	mem 26906MB
Train: [0/180][67/10009]	eta 1:39:32 lr 0.100000	data 0.0006 (0.0278)	batch 0.5596 (0.6007)	loss 62.6637 (35.3321)	grad_norm 39.2696 (54.5483)	mem 26906MB
Train: [0/180][68/10009]	eta 1:39:25 lr 0.100000	data 0.0006 (0.0274)	batch 0.5556 (0.6001)	loss 65.2547 (35.7658)	grad_norm 47.4969 (54.4461)	mem 26906MB
Train: [0/180][69/10009]	eta 1:39:18 lr 0.100000	data 0.0007 (0.0270)	batch 0.5589 (0.5995)	loss 71.0150 (36.2693)	grad_norm 51.3486 (54.4019)	mem 26906MB
Train: [0/180][70/10009]	eta 1:39:13 lr 0.100000	data 0.0007 (0.0266)	batch 0.5655 (0.5990)	loss 52.0154 (36.4911)	grad_norm 80.3486 (54.7673)	mem 26906MB
Train: [0/180][71/10009]	eta 1:39:07 lr 0.100000	data 0.0007 (0.0263)	batch 0.5613 (0.5985)	loss 45.2544 (36.6128)	grad_norm 88.6619 (55.2381)	mem 26906MB
Train: [0/180][72/10009]	eta 1:39:01 lr 0.100000	data 0.0007 (0.0259)	batch 0.5601 (0.5980)	loss 57.1748 (36.8945)	grad_norm 52.7596 (55.2041)	mem 26906MB
Train: [0/180][73/10009]	eta 1:38:58 lr 0.100000	data 0.0008 (0.0256)	batch 0.5765 (0.5977)	loss 65.8183 (37.2853)	grad_norm 67.3118 (55.3677)	mem 26906MB
Train: [0/180][74/10009]	eta 1:38:54 lr 0.100000	data 0.0009 (0.0252)	batch 0.5754 (0.5974)	loss 70.3593 (37.7263)	grad_norm 37.8427 (55.1341)	mem 26906MB
Train: [0/180][75/10009]	eta 1:38:49 lr 0.100000	data 0.0007 (0.0249)	batch 0.5638 (0.5969)	loss 57.7634 (37.9900)	grad_norm 35.5531 (54.8764)	mem 26906MB
Train: [0/180][76/10009]	eta 1:38:46 lr 0.100000	data 0.0007 (0.0246)	batch 0.5717 (0.5966)	loss 69.0880 (38.3938)	grad_norm 66.9291 (55.0330)	mem 26906MB
Train: [0/180][77/10009]	eta 1:38:41 lr 0.100000	data 0.0008 (0.0243)	batch 0.5642 (0.5962)	loss 47.0212 (38.5044)	grad_norm 100.1320 (55.6111)	mem 26906MB
Train: [0/180][78/10009]	eta 1:38:36 lr 0.100000	data 0.0008 (0.0240)	batch 0.5654 (0.5958)	loss 75.8727 (38.9775)	grad_norm 55.5506 (55.6104)	mem 26906MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 4 days, 18:41:59 lr 0.100000	data 1.9079 (1.9079)	batch 41.2549 (41.2549)	loss 27.8915 (27.8915)	grad_norm 77.3803 (77.3803)	mem 26855MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 15:41:50 lr 0.100000	data 1.5433 (1.5433)	batch 5.6460 (5.6460)	loss 25.3435 (25.3435)	grad_norm 77.8235 (77.8235)	mem 26855MB
Train: [0/180][1/10009]	eta 3 days, 15:17:38 lr 0.100000	data 0.0008 (0.7720)	batch 57.1554 (31.4007)	loss 23.0186 (24.1811)	grad_norm 65.5280 (71.6757)	mem 26980MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 15:33:06 lr 0.100000	data 1.7937 (1.7937)	batch 22.8581 (22.8581)	loss 27.1495 (27.1495)	grad_norm 76.5213 (76.5213)	mem 26855MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 8:00:34 lr 0.100000	data 1.7804 (1.7804)	batch 2.8809 (2.8809)	loss 27.2288 (27.2288)	grad_norm 75.4537 (75.4537)	mem 26855MB
Train: [0/180][1/10009]	eta 4:47:26 lr 0.100000	data 0.0006 (0.8905)	batch 0.5658 (1.7233)	loss 28.4450 (27.8369)	grad_norm 88.5443 (81.9990)	mem 26906MB
Train: [0/180][2/10009]	eta 3:43:22 lr 0.100000	data 0.0006 (0.5939)	batch 0.5712 (1.3393)	loss 23.4637 (26.3792)	grad_norm 44.8125 (69.6035)	mem 26906MB
Train: [0/180][3/10009]	eta 3:11:31 lr 0.100000	data 0.0006 (0.4456)	batch 0.5762 (1.1485)	loss 21.0407 (25.0446)	grad_norm 40.5368 (62.3368)	mem 26906MB
Train: [0/180][4/10009]	eta 2:52:18 lr 0.100000	data 0.0007 (0.3566)	batch 0.5728 (1.0334)	loss 20.8820 (24.2121)	grad_norm 54.7426 (60.8180)	mem 26906MB
Train: [0/180][5/10009]	eta 2:39:30 lr 0.100000	data 0.0005 (0.2972)	batch 0.5730 (0.9566)	loss 19.7041 (23.4607)	grad_norm 40.6945 (57.4641)	mem 26906MB
Train: [0/180][6/10009]	eta 2:30:18 lr 0.100000	data 0.0007 (0.2549)	batch 0.5715 (0.9016)	loss 17.5795 (22.6206)	grad_norm 31.6132 (53.7711)	mem 26906MB
Train: [0/180][7/10009]	eta 2:23:25 lr 0.100000	data 0.0005 (0.2231)	batch 0.5720 (0.8604)	loss 18.9080 (22.1565)	grad_norm 62.6734 (54.8839)	mem 26906MB
Train: [0/180][8/10009]	eta 2:17:51 lr 0.100000	data 0.0006 (0.1983)	batch 0.5604 (0.8271)	loss 17.8111 (21.6737)	grad_norm 30.8510 (52.2136)	mem 26906MB
Train: [0/180][9/10009]	eta 2:13:25 lr 0.100000	data 0.0006 (0.1786)	batch 0.5616 (0.8005)	loss 21.3591 (21.6422)	grad_norm 39.0096 (50.8932)	mem 26906MB
Train: [0/180][10/10009]	eta 2:09:47 lr 0.100000	data 0.0006 (0.1624)	batch 0.5622 (0.7789)	loss 20.5659 (21.5444)	grad_norm 30.6740 (49.0551)	mem 26906MB
Train: [0/180][11/10009]	eta 2:06:59 lr 0.100000	data 0.0006 (0.1489)	batch 0.5773 (0.7621)	loss 19.9947 (21.4152)	grad_norm 29.3552 (47.4134)	mem 26906MB
Train: [0/180][12/10009]	eta 2:04:27 lr 0.100000	data 0.0007 (0.1375)	batch 0.5654 (0.7469)	loss 18.1997 (21.1679)	grad_norm 38.0824 (46.6956)	mem 26906MB
Train: [0/180][13/10009]	eta 2:02:16 lr 0.100000	data 0.0007 (0.1277)	batch 0.5654 (0.7340)	loss 18.6748 (20.9898)	grad_norm 26.0215 (45.2189)	mem 26906MB
Train: [0/180][14/10009]	eta 2:00:23 lr 0.100000	data 0.0006 (0.1193)	batch 0.5643 (0.7227)	loss 25.7967 (21.3103)	grad_norm 64.4218 (46.4991)	mem 26906MB
Train: [0/180][15/10009]	eta 1:58:41 lr 0.100000	data 0.0007 (0.1118)	batch 0.5612 (0.7126)	loss 19.5651 (21.2012)	grad_norm 28.1210 (45.3505)	mem 26906MB
Train: [0/180][16/10009]	eta 1:57:17 lr 0.100000	data 0.0006 (0.1053)	batch 0.5703 (0.7042)	loss 18.9426 (21.0683)	grad_norm 29.6789 (44.4286)	mem 26906MB
Train: [0/180][17/10009]	eta 1:55:58 lr 0.100000	data 0.0006 (0.0995)	batch 0.5640 (0.6964)	loss 17.8213 (20.8879)	grad_norm 27.9237 (43.5117)	mem 26906MB
Train: [0/180][18/10009]	eta 1:54:47 lr 0.100000	data 0.0006 (0.0943)	batch 0.5628 (0.6894)	loss 20.8393 (20.8854)	grad_norm 30.9978 (42.8531)	mem 26906MB
Train: [0/180][19/10009]	eta 1:53:46 lr 0.100000	data 0.0006 (0.0896)	batch 0.5682 (0.6833)	loss 21.8712 (20.9347)	grad_norm 29.6457 (42.1927)	mem 26906MB
Train: [0/180][20/10009]	eta 1:52:50 lr 0.100000	data 0.0006 (0.0854)	batch 0.5674 (0.6778)	loss 19.5326 (20.8679)	grad_norm 40.8163 (42.1271)	mem 26906MB
Train: [0/180][21/10009]	eta 1:51:55 lr 0.100000	data 0.0006 (0.0815)	batch 0.5587 (0.6724)	loss 20.0255 (20.8296)	grad_norm 24.5729 (41.3292)	mem 26906MB
Train: [0/180][22/10009]	eta 1:51:56 lr 0.100000	data 0.0007 (0.0780)	batch 0.6757 (0.6725)	loss 20.1677 (20.8008)	grad_norm 23.8037 (40.5672)	mem 26906MB
Train: [0/180][23/10009]	eta 1:51:13 lr 0.100000	data 0.0007 (0.0748)	batch 0.5698 (0.6683)	loss 22.7011 (20.8800)	grad_norm 37.1524 (40.4250)	mem 26906MB
Train: [0/180][24/10009]	eta 1:50:32 lr 0.100000	data 0.0006 (0.0718)	batch 0.5676 (0.6642)	loss 22.3037 (20.9370)	grad_norm 25.9975 (39.8479)	mem 26906MB
Train: [0/180][25/10009]	eta 1:49:50 lr 0.100000	data 0.0007 (0.0691)	batch 0.5582 (0.6602)	loss 26.8991 (21.1663)	grad_norm 24.8030 (39.2692)	mem 26906MB
Train: [0/180][26/10009]	eta 1:49:15 lr 0.100000	data 0.0006 (0.0665)	batch 0.5652 (0.6566)	loss 18.1268 (21.0537)	grad_norm 26.3047 (38.7891)	mem 26906MB
Train: [0/180][27/10009]	eta 1:48:44 lr 0.100000	data 0.0007 (0.0642)	batch 0.5724 (0.6536)	loss 23.8626 (21.1540)	grad_norm 36.2628 (38.6988)	mem 26906MB
Train: [0/180][28/10009]	eta 1:48:10 lr 0.100000	data 0.0007 (0.0620)	batch 0.5566 (0.6503)	loss 23.0831 (21.2205)	grad_norm 27.9210 (38.3272)	mem 26906MB
Train: [0/180][29/10009]	eta 1:47:42 lr 0.100000	data 0.0007 (0.0600)	batch 0.5686 (0.6476)	loss 19.9889 (21.1795)	grad_norm 43.7405 (38.5076)	mem 26906MB
Train: [0/180][30/10009]	eta 1:47:13 lr 0.100000	data 0.0006 (0.0580)	batch 0.5602 (0.6447)	loss 25.2255 (21.3100)	grad_norm 27.2877 (38.1457)	mem 26906MB
Train: [0/180][31/10009]	eta 1:46:49 lr 0.100000	data 0.0006 (0.0562)	batch 0.5699 (0.6424)	loss 21.3944 (21.3126)	grad_norm 26.1450 (37.7707)	mem 26906MB
Train: [0/180][32/10009]	eta 1:46:24 lr 0.100000	data 0.0007 (0.0546)	batch 0.5603 (0.6399)	loss 29.1514 (21.5502)	grad_norm 80.3114 (39.0598)	mem 26906MB
Train: [0/180][33/10009]	eta 1:46:01 lr 0.100000	data 0.0007 (0.0530)	batch 0.5651 (0.6377)	loss 25.9299 (21.6790)	grad_norm 27.7988 (38.7286)	mem 26906MB
Train: [0/180][34/10009]	eta 1:45:42 lr 0.100000	data 0.0007 (0.0515)	batch 0.5710 (0.6358)	loss 28.1659 (21.8643)	grad_norm 34.4336 (38.6059)	mem 26906MB
Train: [0/180][35/10009]	eta 1:45:22 lr 0.100000	data 0.0007 (0.0501)	batch 0.5659 (0.6339)	loss 23.9469 (21.9222)	grad_norm 24.1344 (38.2039)	mem 26906MB
Train: [0/180][36/10009]	eta 1:45:03 lr 0.100000	data 0.0006 (0.0487)	batch 0.5658 (0.6320)	loss 31.5406 (22.1821)	grad_norm 73.9769 (39.1707)	mem 26906MB
Train: [0/180][37/10009]	eta 1:44:44 lr 0.100000	data 0.0006 (0.0475)	batch 0.5628 (0.6302)	loss 30.9978 (22.4141)	grad_norm 75.7348 (40.1329)	mem 26906MB
Train: [0/180][38/10009]	eta 1:44:28 lr 0.100000	data 0.0007 (0.0463)	batch 0.5723 (0.6287)	loss 33.8131 (22.7064)	grad_norm 30.0179 (39.8736)	mem 26906MB
Train: [0/180][39/10009]	eta 1:44:15 lr 0.100000	data 0.0007 (0.0451)	batch 0.5773 (0.6274)	loss 28.2306 (22.8445)	grad_norm 27.2412 (39.5578)	mem 26906MB
Train: [0/180][40/10009]	eta 1:43:59 lr 0.100000	data 0.0007 (0.0440)	batch 0.5638 (0.6259)	loss 30.8466 (23.0397)	grad_norm 28.8101 (39.2956)	mem 26906MB
Train: [0/180][41/10009]	eta 1:43:46 lr 0.100000	data 0.0008 (0.0430)	batch 0.5749 (0.6247)	loss 29.0185 (23.1820)	grad_norm 31.2905 (39.1050)	mem 26906MB
Train: [0/180][42/10009]	eta 1:43:33 lr 0.100000	data 0.0006 (0.0420)	batch 0.5706 (0.6234)	loss 28.8772 (23.3145)	grad_norm 28.8891 (38.8674)	mem 26906MB
Train: [0/180][43/10009]	eta 1:43:20 lr 0.100000	data 0.0007 (0.0411)	batch 0.5702 (0.6222)	loss 31.5014 (23.5005)	grad_norm 48.8344 (39.0940)	mem 26906MB
Train: [0/180][44/10009]	eta 1:43:08 lr 0.100000	data 0.0005 (0.0402)	batch 0.5670 (0.6210)	loss 28.1755 (23.6044)	grad_norm 30.1119 (38.8944)	mem 26906MB
Train: [0/180][45/10009]	eta 1:42:55 lr 0.100000	data 0.0006 (0.0393)	batch 0.5653 (0.6198)	loss 31.2645 (23.7710)	grad_norm 37.1016 (38.8554)	mem 26906MB
Train: [0/180][46/10009]	eta 1:42:46 lr 0.100000	data 0.0007 (0.0385)	batch 0.5796 (0.6189)	loss 31.3274 (23.9317)	grad_norm 32.7300 (38.7251)	mem 26906MB
Train: [0/180][47/10009]	eta 1:42:34 lr 0.100000	data 0.0007 (0.0377)	batch 0.5636 (0.6178)	loss 30.0928 (24.0601)	grad_norm 38.5805 (38.7220)	mem 26906MB
Train: [0/180][48/10009]	eta 1:42:24 lr 0.100000	data 0.0006 (0.0370)	batch 0.5726 (0.6168)	loss 27.6462 (24.1333)	grad_norm 32.0565 (38.5860)	mem 26906MB
Train: [0/180][49/10009]	eta 1:42:15 lr 0.100000	data 0.0007 (0.0362)	batch 0.5746 (0.6160)	loss 34.9953 (24.3505)	grad_norm 34.8666 (38.5116)	mem 26906MB
Train: [0/180][50/10009]	eta 1:42:04 lr 0.100000	data 0.0007 (0.0355)	batch 0.5623 (0.6149)	loss 28.0536 (24.4231)	grad_norm 31.8334 (38.3807)	mem 26906MB
Train: [0/180][51/10009]	eta 1:41:52 lr 0.100000	data 0.0007 (0.0349)	batch 0.5598 (0.6139)	loss 39.3181 (24.7096)	grad_norm 36.3245 (38.3411)	mem 26906MB
Train: [0/180][52/10009]	eta 1:41:44 lr 0.100000	data 0.0007 (0.0342)	batch 0.5731 (0.6131)	loss 31.6019 (24.8396)	grad_norm 35.4898 (38.2873)	mem 26906MB
Train: [0/180][53/10009]	eta 1:41:35 lr 0.100000	data 0.0007 (0.0336)	batch 0.5646 (0.6122)	loss 31.5589 (24.9640)	grad_norm 63.8544 (38.7608)	mem 26906MB
Train: [0/180][54/10009]	eta 1:41:26 lr 0.100000	data 0.0006 (0.0330)	batch 0.5689 (0.6114)	loss 27.6303 (25.0125)	grad_norm 34.7960 (38.6887)	mem 26906MB
Train: [0/180][55/10009]	eta 1:41:17 lr 0.100000	data 0.0008 (0.0324)	batch 0.5618 (0.6105)	loss 39.8003 (25.2766)	grad_norm 62.4370 (39.1128)	mem 26906MB
Train: [0/180][56/10009]	eta 1:41:09 lr 0.100000	data 0.0006 (0.0319)	batch 0.5668 (0.6098)	loss 31.9277 (25.3933)	grad_norm 30.8026 (38.9670)	mem 26906MB
Train: [0/180][57/10009]	eta 1:41:00 lr 0.100000	data 0.0006 (0.0313)	batch 0.5649 (0.6090)	loss 36.2791 (25.5810)	grad_norm 66.1379 (39.4355)	mem 26906MB
Train: [0/180][58/10009]	eta 1:40:53 lr 0.100000	data 0.0006 (0.0308)	batch 0.5706 (0.6083)	loss 32.2375 (25.6938)	grad_norm 38.4559 (39.4189)	mem 26906MB
Train: [0/180][59/10009]	eta 1:40:46 lr 0.100000	data 0.0007 (0.0303)	batch 0.5677 (0.6077)	loss 31.8737 (25.7968)	grad_norm 67.5470 (39.8877)	mem 26906MB
Train: [0/180][60/10009]	eta 1:40:39 lr 0.100000	data 0.0007 (0.0298)	batch 0.5678 (0.6070)	loss 34.7613 (25.9437)	grad_norm 46.2239 (39.9915)	mem 26906MB
Train: [0/180][61/10009]	eta 1:40:31 lr 0.100000	data 0.0008 (0.0294)	batch 0.5623 (0.6063)	loss 30.6484 (26.0196)	grad_norm 58.3566 (40.2878)	mem 26906MB
Train: [0/180][62/10009]	eta 1:40:24 lr 0.100000	data 0.0006 (0.0289)	batch 0.5640 (0.6056)	loss 32.8472 (26.1280)	grad_norm 44.6567 (40.3571)	mem 26906MB
Train: [0/180][63/10009]	eta 1:40:16 lr 0.100000	data 0.0006 (0.0285)	batch 0.5628 (0.6050)	loss 38.0427 (26.3142)	grad_norm 34.8200 (40.2706)	mem 26906MB
Train: [0/180][64/10009]	eta 1:40:09 lr 0.100000	data 0.0005 (0.0280)	batch 0.5632 (0.6043)	loss 37.2669 (26.4827)	grad_norm 56.0232 (40.5129)	mem 26906MB
Train: [0/180][65/10009]	eta 1:40:04 lr 0.100000	data 0.0007 (0.0276)	batch 0.5723 (0.6038)	loss 43.1921 (26.7358)	grad_norm 38.4049 (40.4810)	mem 26906MB
Train: [0/180][66/10009]	eta 1:39:58 lr 0.100000	data 0.0005 (0.0272)	batch 0.5679 (0.6033)	loss 40.8103 (26.9459)	grad_norm 44.9734 (40.5480)	mem 26906MB
Train: [0/180][67/10009]	eta 1:39:53 lr 0.100000	data 0.0006 (0.0268)	batch 0.5727 (0.6028)	loss 33.9375 (27.0487)	grad_norm 47.1599 (40.6453)	mem 26906MB
Train: [0/180][68/10009]	eta 1:39:48 lr 0.100000	data 0.0007 (0.0264)	batch 0.5719 (0.6024)	loss 39.4783 (27.2289)	grad_norm 39.7318 (40.6320)	mem 26906MB
Train: [0/180][69/10009]	eta 1:39:42 lr 0.100000	data 0.0007 (0.0261)	batch 0.5626 (0.6018)	loss 30.2699 (27.2723)	grad_norm 37.3892 (40.5857)	mem 26906MB
Train: [0/180][70/10009]	eta 1:39:36 lr 0.100000	data 0.0005 (0.0257)	batch 0.5650 (0.6013)	loss 43.1605 (27.4961)	grad_norm 60.2612 (40.8628)	mem 26906MB
Train: [0/180][71/10009]	eta 1:39:31 lr 0.100000	data 0.0009 (0.0254)	batch 0.5690 (0.6009)	loss 38.1363 (27.6439)	grad_norm 29.2578 (40.7016)	mem 26906MB
Train: [0/180][72/10009]	eta 1:39:26 lr 0.100000	data 0.0008 (0.0250)	batch 0.5704 (0.6004)	loss 34.1295 (27.7327)	grad_norm 34.1977 (40.6126)	mem 26906MB
Train: [0/180][73/10009]	eta 1:39:21 lr 0.100000	data 0.0006 (0.0247)	batch 0.5684 (0.6000)	loss 40.7462 (27.9086)	grad_norm 56.8001 (40.8313)	mem 26906MB
Train: [0/180][74/10009]	eta 1:39:16 lr 0.100000	data 0.0007 (0.0244)	batch 0.5656 (0.5995)	loss 31.6963 (27.9591)	grad_norm 50.8302 (40.9646)	mem 26906MB
Train: [0/180][75/10009]	eta 1:39:11 lr 0.100000	data 0.0006 (0.0241)	batch 0.5663 (0.5991)	loss 34.3603 (28.0433)	grad_norm 39.7155 (40.9482)	mem 26906MB
Train: [0/180][76/10009]	eta 1:39:09 lr 0.100000	data 0.0009 (0.0238)	batch 0.5865 (0.5989)	loss 48.4717 (28.3086)	grad_norm 54.1005 (41.1190)	mem 26906MB
Train: [0/180][77/10009]	eta 1:39:06 lr 0.100000	data 0.0008 (0.0235)	batch 0.5846 (0.5988)	loss 46.4268 (28.5409)	grad_norm 40.5189 (41.1113)	mem 26906MB
Train: [0/180][78/10009]	eta 1:39:00 lr 0.100000	data 0.0007 (0.0232)	batch 0.5551 (0.5982)	loss 66.2344 (29.0180)	grad_norm 70.6404 (41.4851)	mem 26906MB
Train: [0/180][79/10009]	eta 1:38:55 lr 0.100000	data 0.0007 (0.0229)	batch 0.5638 (0.5978)	loss 59.6014 (29.4003)	grad_norm 64.1996 (41.7690)	mem 26906MB
Train: [0/180][80/10009]	eta 1:38:51 lr 0.100000	data 0.0008 (0.0226)	batch 0.5630 (0.5973)	loss 43.5275 (29.5747)	grad_norm 96.3710 (42.4431)	mem 26906MB
Train: [0/180][81/10009]	eta 1:38:47 lr 0.100000	data 0.0007 (0.0224)	batch 0.5744 (0.5971)	loss 48.8496 (29.8098)	grad_norm 53.4582 (42.5774)	mem 26906MB
Train: [0/180][82/10009]	eta 1:38:43 lr 0.100000	data 0.0008 (0.0221)	batch 0.5710 (0.5968)	loss 42.2928 (29.9602)	grad_norm 89.8131 (43.1466)	mem 26906MB
Train: [0/180][83/10009]	eta 1:38:40 lr 0.100000	data 0.0007 (0.0218)	batch 0.5747 (0.5965)	loss 48.9625 (30.1864)	grad_norm 41.7292 (43.1297)	mem 26906MB
Train: [0/180][84/10009]	eta 1:38:37 lr 0.100000	data 0.0007 (0.0216)	batch 0.5768 (0.5963)	loss 48.9366 (30.4070)	grad_norm 89.2374 (43.6721)	mem 26906MB
Train: [0/180][85/10009]	eta 1:38:34 lr 0.100000	data 0.0007 (0.0213)	batch 0.5700 (0.5960)	loss 40.1612 (30.5204)	grad_norm 85.5642 (44.1592)	mem 26906MB
Train: [0/180][86/10009]	eta 1:38:29 lr 0.100000	data 0.0008 (0.0211)	batch 0.5635 (0.5956)	loss 36.0137 (30.5835)	grad_norm 29.2018 (43.9873)	mem 26906MB
Train: [0/180][87/10009]	eta 1:38:26 lr 0.100000	data 0.0006 (0.0209)	batch 0.5680 (0.5953)	loss 50.2861 (30.8074)	grad_norm 95.0818 (44.5679)	mem 26906MB
Train: [0/180][88/10009]	eta 1:38:22 lr 0.100000	data 0.0006 (0.0207)	batch 0.5646 (0.5949)	loss 50.4932 (31.0286)	grad_norm 62.7769 (44.7725)	mem 26906MB
Train: [0/180][89/10009]	eta 1:38:17 lr 0.100000	data 0.0007 (0.0204)	batch 0.5599 (0.5945)	loss 61.2907 (31.3649)	grad_norm 44.9806 (44.7748)	mem 26906MB
Train: [0/180][90/10009]	eta 1:38:13 lr 0.100000	data 0.0006 (0.0202)	batch 0.5601 (0.5942)	loss 55.8964 (31.6344)	grad_norm 64.3673 (44.9901)	mem 26906MB
Train: [0/180][91/10009]	eta 1:38:15 lr 0.100000	data 0.0007 (0.0200)	batch 0.6213 (0.5945)	loss 50.0858 (31.8350)	grad_norm 44.3068 (44.9827)	mem 26906MB
Train: [0/180][92/10009]	eta 1:38:11 lr 0.100000	data 0.0006 (0.0198)	batch 0.5625 (0.5941)	loss 63.0423 (32.1706)	grad_norm 48.1270 (45.0165)	mem 26906MB
Train: [0/180][93/10009]	eta 1:38:08 lr 0.100000	data 0.0005 (0.0196)	batch 0.5730 (0.5939)	loss 46.7594 (32.3258)	grad_norm 33.7011 (44.8961)	mem 26906MB
Train: [0/180][94/10009]	eta 1:38:05 lr 0.100000	data 0.0006 (0.0194)	batch 0.5705 (0.5936)	loss 60.6659 (32.6241)	grad_norm 61.5833 (45.0718)	mem 26906MB
Train: [0/180][95/10009]	eta 1:38:02 lr 0.100000	data 0.0008 (0.0192)	batch 0.5681 (0.5934)	loss 53.6998 (32.8436)	grad_norm 45.8677 (45.0801)	mem 26906MB
Train: [0/180][96/10009]	eta 1:37:59 lr 0.100000	data 0.0005 (0.0190)	batch 0.5720 (0.5932)	loss 46.9920 (32.9895)	grad_norm 38.4345 (45.0116)	mem 26906MB
Train: [0/180][97/10009]	eta 1:37:56 lr 0.100000	data 0.0007 (0.0188)	batch 0.5686 (0.5929)	loss 46.6270 (33.1286)	grad_norm 112.7668 (45.7030)	mem 26906MB
Train: [0/180][98/10009]	eta 1:37:53 lr 0.100000	data 0.0006 (0.0186)	batch 0.5652 (0.5926)	loss 57.4402 (33.3742)	grad_norm 72.2564 (45.9712)	mem 26906MB
Train: [0/180][99/10009]	eta 1:37:50 lr 0.100000	data 0.0007 (0.0184)	batch 0.5656 (0.5924)	loss 44.7936 (33.4884)	grad_norm 76.2400 (46.2739)	mem 26906MB
Train: [0/180][100/10009]	eta 1:37:48 lr 0.100000	data 0.0008 (0.0183)	batch 0.5854 (0.5923)	loss 56.9554 (33.7208)	grad_norm 89.8843 (46.7057)	mem 26906MB
Train: [0/180][101/10009]	eta 1:37:46 lr 0.100000	data 0.0008 (0.0181)	batch 0.5687 (0.5921)	loss 47.2383 (33.8533)	grad_norm 30.4327 (46.5461)	mem 26906MB
Train: [0/180][102/10009]	eta 1:37:43 lr 0.100000	data 0.0008 (0.0179)	batch 0.5710 (0.5918)	loss 50.3835 (34.0138)	grad_norm 73.2264 (46.8051)	mem 26906MB
Train: [0/180][103/10009]	eta 1:37:39 lr 0.100000	data 0.0006 (0.0178)	batch 0.5591 (0.5915)	loss 58.6787 (34.2509)	grad_norm 126.3668 (47.5702)	mem 26906MB
Train: [0/180][104/10009]	eta 1:37:36 lr 0.100000	data 0.0006 (0.0176)	batch 0.5668 (0.5913)	loss 60.6145 (34.5020)	grad_norm 70.7392 (47.7908)	mem 26906MB
Train: [0/180][105/10009]	eta 1:37:33 lr 0.100000	data 0.0007 (0.0174)	batch 0.5627 (0.5910)	loss 47.4697 (34.6243)	grad_norm 41.1390 (47.7281)	mem 26906MB
Train: [0/180][106/10009]	eta 1:37:31 lr 0.100000	data 0.0006 (0.0173)	batch 0.5720 (0.5908)	loss 55.9188 (34.8234)	grad_norm 93.3106 (48.1541)	mem 26906MB
Train: [0/180][107/10009]	eta 1:37:28 lr 0.100000	data 0.0007 (0.0171)	batch 0.5646 (0.5906)	loss 48.5105 (34.9501)	grad_norm 46.1813 (48.1358)	mem 26906MB
Train: [0/180][108/10009]	eta 1:37:24 lr 0.100000	data 0.0006 (0.0170)	batch 0.5588 (0.5903)	loss 59.2565 (35.1731)	grad_norm 75.8238 (48.3898)	mem 26906MB
Train: [0/180][109/10009]	eta 1:37:22 lr 0.100000	data 0.0007 (0.0168)	batch 0.5744 (0.5902)	loss 58.3678 (35.3840)	grad_norm 43.1219 (48.3419)	mem 26906MB
Train: [0/180][110/10009]	eta 1:37:19 lr 0.100000	data 0.0006 (0.0167)	batch 0.5612 (0.5899)	loss 78.7383 (35.7745)	grad_norm 125.3435 (49.0356)	mem 26906MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108814
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 8:50:43 lr 0.100000	data 1.7635 (1.7635)	batch 3.1815 (3.1815)	loss 38.3904 (38.3904)	grad_norm 120.2768 (120.2768)	mem 22427MB
Train: [0/180][1/10009]	eta 4:56:50 lr 0.100000	data 0.0011 (0.8823)	batch 0.3778 (1.7796)	loss 6.4627 (22.4266)	grad_norm 6.4814 (63.3791)	mem 22477MB
Train: [0/180][2/10009]	eta 3:37:25 lr 0.100000	data 0.0007 (0.5884)	batch 0.3516 (1.3036)	loss 5.5379 (16.7970)	grad_norm 7.1268 (44.6283)	mem 22477MB
Train: [0/180][3/10009]	eta 2:58:17 lr 0.100000	data 0.0007 (0.4415)	batch 0.3658 (1.0691)	loss 5.0132 (13.8511)	grad_norm 6.8695 (35.1886)	mem 22477MB
Train: [0/180][4/10009]	eta 2:34:20 lr 0.100000	data 0.0007 (0.3533)	batch 0.3512 (0.9256)	loss 4.8926 (12.0594)	grad_norm 6.3875 (29.4284)	mem 22477MB
Train: [0/180][5/10009]	eta 2:19:23 lr 0.100000	data 0.0007 (0.2946)	batch 0.3885 (0.8360)	loss 4.4712 (10.7947)	grad_norm 5.8610 (25.5005)	mem 22477MB
Train: [0/180][6/10009]	eta 2:08:03 lr 0.100000	data 0.0007 (0.2526)	batch 0.3603 (0.7681)	loss 4.5826 (9.9072)	grad_norm 5.8730 (22.6966)	mem 22477MB
Train: [0/180][7/10009]	eta 1:59:27 lr 0.100000	data 0.0008 (0.2211)	batch 0.3563 (0.7166)	loss 4.2988 (9.2062)	grad_norm 5.5250 (20.5501)	mem 22477MB
Train: [0/180][8/10009]	eta 1:52:40 lr 0.100000	data 0.0006 (0.1966)	batch 0.3507 (0.6760)	loss 4.7844 (8.7149)	grad_norm 5.6192 (18.8911)	mem 22477MB
Train: [0/180][9/10009]	eta 1:47:13 lr 0.100000	data 0.0007 (0.1770)	batch 0.3495 (0.6433)	loss 5.0247 (8.3459)	grad_norm 5.6006 (17.5621)	mem 22477MB
Train: [0/180][10/10009]	eta 1:42:44 lr 0.100000	data 0.0007 (0.1610)	batch 0.3481 (0.6165)	loss 4.5782 (8.0033)	grad_norm 5.3028 (16.4476)	mem 22477MB
Train: [0/180][11/10009]	eta 1:39:05 lr 0.100000	data 0.0006 (0.1476)	batch 0.3548 (0.5947)	loss 4.6747 (7.7259)	grad_norm 5.4298 (15.5294)	mem 22477MB
Train: [0/180][12/10009]	eta 1:35:54 lr 0.100000	data 0.0006 (0.1363)	batch 0.3471 (0.5756)	loss 4.6381 (7.4884)	grad_norm 5.3980 (14.7501)	mem 22477MB
Train: [0/180][13/10009]	eta 1:33:25 lr 0.100000	data 0.0007 (0.1266)	batch 0.3670 (0.5607)	loss 4.8172 (7.2976)	grad_norm 5.0804 (14.0594)	mem 22477MB
Train: [0/180][14/10009]	eta 1:31:21 lr 0.100000	data 0.0007 (0.1182)	batch 0.3755 (0.5484)	loss 4.8424 (7.1339)	grad_norm 5.0996 (13.4621)	mem 22477MB
Train: [0/180][15/10009]	eta 1:29:28 lr 0.100000	data 0.0007 (0.1109)	batch 0.3688 (0.5372)	loss 4.6420 (6.9782)	grad_norm 5.0506 (12.9364)	mem 22477MB
Train: [0/180][16/10009]	eta 1:27:47 lr 0.100000	data 0.0007 (0.1044)	batch 0.3668 (0.5271)	loss 4.5615 (6.8360)	grad_norm 4.7915 (12.4573)	mem 22477MB
Train: [0/180][17/10009]	eta 1:26:17 lr 0.100000	data 0.0008 (0.0986)	batch 0.3649 (0.5181)	loss 4.7533 (6.7203)	grad_norm 4.6059 (12.0211)	mem 22477MB
Train: [0/180][18/10009]	eta 1:24:50 lr 0.100000	data 0.0007 (0.0935)	batch 0.3552 (0.5095)	loss 4.6520 (6.6115)	grad_norm 4.7511 (11.6384)	mem 22477MB
Train: [0/180][19/10009]	eta 1:23:30 lr 0.100000	data 0.0007 (0.0888)	batch 0.3493 (0.5015)	loss 4.9198 (6.5269)	grad_norm 4.6859 (11.2908)	mem 22477MB
Train: [0/180][20/10009]	eta 1:22:23 lr 0.100000	data 0.0007 (0.0846)	batch 0.3617 (0.4949)	loss 4.7972 (6.4445)	grad_norm 4.6874 (10.9764)	mem 22477MB
Train: [0/180][21/10009]	eta 1:21:22 lr 0.100000	data 0.0007 (0.0808)	batch 0.3613 (0.4888)	loss 4.3941 (6.3513)	grad_norm 4.4661 (10.6804)	mem 22477MB
Train: [0/180][22/10009]	eta 1:20:23 lr 0.100000	data 0.0007 (0.0773)	batch 0.3554 (0.4830)	loss 4.9999 (6.2926)	grad_norm 4.7253 (10.4215)	mem 22477MB
Train: [0/180][23/10009]	eta 1:19:27 lr 0.100000	data 0.0006 (0.0742)	batch 0.3497 (0.4774)	loss 4.6268 (6.2232)	grad_norm 4.5558 (10.1771)	mem 22477MB
Train: [0/180][24/10009]	eta 1:18:47 lr 0.100000	data 0.0008 (0.0712)	batch 0.3784 (0.4735)	loss 4.6558 (6.1605)	grad_norm 4.5301 (9.9512)	mem 22477MB
Train: [0/180][25/10009]	eta 1:17:58 lr 0.100000	data 0.0008 (0.0685)	batch 0.3471 (0.4686)	loss 5.2730 (6.1263)	grad_norm 4.5965 (9.7453)	mem 22477MB
Train: [0/180][26/10009]	eta 1:17:14 lr 0.100000	data 0.0005 (0.0660)	batch 0.3508 (0.4643)	loss 5.2829 (6.0951)	grad_norm 4.2800 (9.5429)	mem 22477MB
Train: [0/180][27/10009]	eta 1:16:32 lr 0.100000	data 0.0007 (0.0637)	batch 0.3457 (0.4600)	loss 5.0795 (6.0588)	grad_norm 4.3161 (9.3562)	mem 22477MB
Train: [0/180][28/10009]	eta 1:15:59 lr 0.100000	data 0.0008 (0.0615)	batch 0.3682 (0.4569)	loss 4.9523 (6.0207)	grad_norm 4.2055 (9.1786)	mem 22477MB
Train: [0/180][29/10009]	eta 1:15:28 lr 0.100000	data 0.0007 (0.0595)	batch 0.3627 (0.4537)	loss 4.9366 (5.9845)	grad_norm 4.1301 (9.0103)	mem 22477MB
Train: [0/180][30/10009]	eta 1:14:52 lr 0.100000	data 0.0005 (0.0576)	batch 0.3440 (0.4502)	loss 5.0805 (5.9554)	grad_norm 3.9953 (8.8485)	mem 22477MB
Train: [0/180][31/10009]	eta 1:14:22 lr 0.100000	data 0.0007 (0.0558)	batch 0.3557 (0.4472)	loss 4.5879 (5.9126)	grad_norm 3.9540 (8.6956)	mem 22477MB
Train: [0/180][32/10009]	eta 1:13:56 lr 0.100000	data 0.0007 (0.0541)	batch 0.3638 (0.4447)	loss 4.9907 (5.8847)	grad_norm 3.9277 (8.5511)	mem 22477MB
Train: [0/180][33/10009]	eta 1:13:27 lr 0.100000	data 0.0007 (0.0525)	batch 0.3470 (0.4418)	loss 4.8545 (5.8544)	grad_norm 4.0683 (8.4192)	mem 22477MB
Train: [0/180][34/10009]	eta 1:13:00 lr 0.100000	data 0.0006 (0.0511)	batch 0.3468 (0.4391)	loss 5.0408 (5.8311)	grad_norm 4.0772 (8.2952)	mem 22477MB
Train: [0/180][35/10009]	eta 1:12:35 lr 0.100000	data 0.0007 (0.0497)	batch 0.3502 (0.4366)	loss 4.5771 (5.7963)	grad_norm 3.9312 (8.1740)	mem 22477MB
Train: [0/180][36/10009]	eta 1:12:12 lr 0.100000	data 0.0007 (0.0483)	batch 0.3552 (0.4344)	loss 4.6551 (5.7655)	grad_norm 3.7969 (8.0557)	mem 22477MB
Train: [0/180][37/10009]	eta 1:11:54 lr 0.100000	data 0.0006 (0.0471)	batch 0.3676 (0.4327)	loss 4.8264 (5.7408)	grad_norm 4.0012 (7.9490)	mem 22477MB
Train: [0/180][38/10009]	eta 1:11:35 lr 0.100000	data 0.0007 (0.0459)	batch 0.3601 (0.4308)	loss 5.0705 (5.7236)	grad_norm 3.8630 (7.8442)	mem 22477MB
Train: [0/180][39/10009]	eta 1:11:18 lr 0.100000	data 0.0006 (0.0448)	batch 0.3620 (0.4291)	loss 4.8208 (5.7010)	grad_norm 3.9578 (7.7470)	mem 22477MB
Train: [0/180][40/10009]	eta 1:11:00 lr 0.100000	data 0.0007 (0.0437)	batch 0.3589 (0.4274)	loss 4.7924 (5.6788)	grad_norm 3.7880 (7.6505)	mem 22477MB
Train: [0/180][41/10009]	eta 1:10:42 lr 0.100000	data 0.0007 (0.0427)	batch 0.3541 (0.4256)	loss 5.1253 (5.6657)	grad_norm 3.7225 (7.5570)	mem 22477MB
Train: [0/180][42/10009]	eta 1:10:27 lr 0.100000	data 0.0007 (0.0417)	batch 0.3635 (0.4242)	loss 4.9483 (5.6490)	grad_norm 3.8735 (7.4713)	mem 22477MB
Train: [0/180][43/10009]	eta 1:10:11 lr 0.100000	data 0.0006 (0.0408)	batch 0.3534 (0.4226)	loss 5.0004 (5.6342)	grad_norm 3.7297 (7.3863)	mem 22477MB
Train: [0/180][44/10009]	eta 1:09:58 lr 0.100000	data 0.0008 (0.0399)	batch 0.3678 (0.4214)	loss 4.9805 (5.6197)	grad_norm 3.7092 (7.3046)	mem 22477MB
Train: [0/180][45/10009]	eta 1:09:43 lr 0.100000	data 0.0007 (0.0390)	batch 0.3504 (0.4198)	loss 4.7412 (5.6006)	grad_norm 3.6382 (7.2248)	mem 22477MB
Train: [0/180][46/10009]	eta 1:09:29 lr 0.100000	data 0.0006 (0.0382)	batch 0.3562 (0.4185)	loss 4.8718 (5.5851)	grad_norm 3.5849 (7.1474)	mem 22477MB
Train: [0/180][47/10009]	eta 1:09:17 lr 0.100000	data 0.0006 (0.0374)	batch 0.3636 (0.4173)	loss 4.6031 (5.5646)	grad_norm 3.6093 (7.0737)	mem 22477MB
Train: [0/180][48/10009]	eta 1:09:05 lr 0.100000	data 0.0007 (0.0367)	batch 0.3589 (0.4161)	loss 4.8471 (5.5500)	grad_norm 3.5208 (7.0012)	mem 22477MB
Train: [0/180][49/10009]	eta 1:08:52 lr 0.100000	data 0.0007 (0.0359)	batch 0.3537 (0.4149)	loss 5.0561 (5.5401)	grad_norm 3.5687 (6.9325)	mem 22477MB
Train: [0/180][50/10009]	eta 1:08:41 lr 0.100000	data 0.0007 (0.0353)	batch 0.3628 (0.4139)	loss 4.7353 (5.5243)	grad_norm 3.3818 (6.8629)	mem 22477MB
Train: [0/180][51/10009]	eta 1:08:29 lr 0.100000	data 0.0007 (0.0346)	batch 0.3521 (0.4127)	loss 4.7384 (5.5092)	grad_norm 3.4694 (6.7977)	mem 22477MB
Train: [0/180][52/10009]	eta 1:08:17 lr 0.100000	data 0.0007 (0.0339)	batch 0.3520 (0.4115)	loss 4.6944 (5.4939)	grad_norm 3.5424 (6.7362)	mem 22477MB
Train: [0/180][53/10009]	eta 1:08:11 lr 0.100000	data 0.0007 (0.0333)	batch 0.3808 (0.4110)	loss 4.8930 (5.4827)	grad_norm 3.4577 (6.6755)	mem 22477MB
Train: [0/180][54/10009]	eta 1:08:01 lr 0.100000	data 0.0006 (0.0327)	batch 0.3570 (0.4100)	loss 4.8586 (5.4714)	grad_norm 3.6812 (6.6211)	mem 22477MB
Train: [0/180][55/10009]	eta 1:07:48 lr 0.100000	data 0.0006 (0.0322)	batch 0.3420 (0.4088)	loss 4.7950 (5.4593)	grad_norm 3.4756 (6.5649)	mem 22477MB
Train: [0/180][56/10009]	eta 1:07:37 lr 0.100000	data 0.0006 (0.0316)	batch 0.3472 (0.4077)	loss 4.9061 (5.4496)	grad_norm 3.5596 (6.5122)	mem 22477MB
Train: [0/180][57/10009]	eta 1:07:27 lr 0.100000	data 0.0006 (0.0311)	batch 0.3516 (0.4067)	loss 4.9167 (5.4404)	grad_norm 3.4362 (6.4591)	mem 22477MB
Train: [0/180][58/10009]	eta 1:07:17 lr 0.100000	data 0.0006 (0.0306)	batch 0.3494 (0.4058)	loss 4.7019 (5.4279)	grad_norm 3.3775 (6.4069)	mem 22477MB
Train: [0/180][59/10009]	eta 1:07:07 lr 0.100000	data 0.0005 (0.0301)	batch 0.3481 (0.4048)	loss 4.9035 (5.4192)	grad_norm 3.5574 (6.3594)	mem 22477MB
Train: [0/180][60/10009]	eta 1:07:00 lr 0.100000	data 0.0006 (0.0296)	batch 0.3661 (0.4042)	loss 4.3031 (5.4009)	grad_norm 3.3241 (6.3097)	mem 22477MB
Train: [0/180][61/10009]	eta 1:06:55 lr 0.100000	data 0.0006 (0.0291)	batch 0.3728 (0.4037)	loss 4.7052 (5.3896)	grad_norm 3.4722 (6.2639)	mem 22477MB
Train: [0/180][62/10009]	eta 1:06:46 lr 0.100000	data 0.0006 (0.0287)	batch 0.3505 (0.4028)	loss 4.7656 (5.3797)	grad_norm 3.4783 (6.2197)	mem 22477MB
Train: [0/180][63/10009]	eta 1:06:38 lr 0.100000	data 0.0006 (0.0282)	batch 0.3504 (0.4020)	loss 4.5374 (5.3666)	grad_norm 3.4485 (6.1764)	mem 22477MB
Train: [0/180][64/10009]	eta 1:06:30 lr 0.100000	data 0.0007 (0.0278)	batch 0.3544 (0.4013)	loss 4.7967 (5.3578)	grad_norm 3.4064 (6.1338)	mem 22477MB
Train: [0/180][65/10009]	eta 1:06:24 lr 0.100000	data 0.0006 (0.0274)	batch 0.3626 (0.4007)	loss 4.9045 (5.3509)	grad_norm 3.5119 (6.0940)	mem 22477MB
Train: [0/180][66/10009]	eta 1:06:16 lr 0.100000	data 0.0006 (0.0270)	batch 0.3485 (0.3999)	loss 4.6283 (5.3401)	grad_norm 3.3423 (6.0530)	mem 22477MB
Train: [0/180][67/10009]	eta 1:06:07 lr 0.100000	data 0.0006 (0.0266)	batch 0.3414 (0.3990)	loss 4.7596 (5.3316)	grad_norm 3.4952 (6.0154)	mem 22477MB
Train: [0/180][68/10009]	eta 1:06:01 lr 0.100000	data 0.0006 (0.0262)	batch 0.3635 (0.3985)	loss 4.6139 (5.3212)	grad_norm 3.3800 (5.9772)	mem 22477MB
Train: [0/180][69/10009]	eta 1:05:56 lr 0.100000	data 0.0006 (0.0259)	batch 0.3615 (0.3980)	loss 5.0326 (5.3171)	grad_norm 3.6527 (5.9440)	mem 22477MB
Train: [0/180][70/10009]	eta 1:05:48 lr 0.100000	data 0.0006 (0.0255)	batch 0.3498 (0.3973)	loss 4.7738 (5.3094)	grad_norm 3.4609 (5.9090)	mem 22477MB
Train: [0/180][71/10009]	eta 1:05:41 lr 0.100000	data 0.0005 (0.0251)	batch 0.3450 (0.3966)	loss 4.9776 (5.3048)	grad_norm 3.4213 (5.8744)	mem 22477MB
Train: [0/180][72/10009]	eta 1:05:36 lr 0.100000	data 0.0006 (0.0248)	batch 0.3654 (0.3962)	loss 4.6091 (5.2953)	grad_norm 3.2044 (5.8379)	mem 22477MB
Train: [0/180][73/10009]	eta 1:05:44 lr 0.100000	data 0.0007 (0.0245)	batch 0.4598 (0.3970)	loss 4.9797 (5.2910)	grad_norm 3.3196 (5.8038)	mem 22477MB
Train: [0/180][74/10009]	eta 1:05:39 lr 0.100000	data 0.0006 (0.0242)	batch 0.3608 (0.3965)	loss 4.8150 (5.2847)	grad_norm 3.3372 (5.7709)	mem 22477MB
Train: [0/180][75/10009]	eta 1:05:34 lr 0.100000	data 0.0006 (0.0239)	batch 0.3596 (0.3961)	loss 4.9419 (5.2802)	grad_norm 3.3241 (5.7387)	mem 22477MB
Train: [0/180][76/10009]	eta 1:05:29 lr 0.100000	data 0.0006 (0.0236)	batch 0.3593 (0.3956)	loss 4.8319 (5.2743)	grad_norm 3.4282 (5.7087)	mem 22477MB
Train: [0/180][77/10009]	eta 1:05:22 lr 0.100000	data 0.0007 (0.0233)	batch 0.3438 (0.3949)	loss 4.4252 (5.2635)	grad_norm 3.2809 (5.6776)	mem 22477MB
Train: [0/180][78/10009]	eta 1:05:17 lr 0.100000	data 0.0006 (0.0230)	batch 0.3617 (0.3945)	loss 4.6304 (5.2554)	grad_norm 3.2135 (5.6464)	mem 22477MB
Train: [0/180][79/10009]	eta 1:05:14 lr 0.100000	data 0.0007 (0.0227)	batch 0.3739 (0.3942)	loss 4.6000 (5.2473)	grad_norm 3.2258 (5.6162)	mem 22477MB
Train: [0/180][80/10009]	eta 1:05:10 lr 0.100000	data 0.0007 (0.0224)	batch 0.3606 (0.3938)	loss 4.7534 (5.2412)	grad_norm 3.4482 (5.5894)	mem 22477MB
Train: [0/180][81/10009]	eta 1:05:04 lr 0.100000	data 0.0006 (0.0222)	batch 0.3496 (0.3933)	loss 4.7650 (5.2354)	grad_norm 3.4134 (5.5629)	mem 22477MB
Train: [0/180][82/10009]	eta 1:04:59 lr 0.100000	data 0.0005 (0.0219)	batch 0.3574 (0.3928)	loss 5.0162 (5.2327)	grad_norm 3.2800 (5.5354)	mem 22477MB
Train: [0/180][83/10009]	eta 1:04:55 lr 0.100000	data 0.0006 (0.0216)	batch 0.3599 (0.3925)	loss 5.0435 (5.2305)	grad_norm 3.5277 (5.5115)	mem 22477MB
Train: [0/180][84/10009]	eta 1:04:51 lr 0.100000	data 0.0006 (0.0214)	batch 0.3615 (0.3921)	loss 4.5461 (5.2224)	grad_norm 3.5228 (5.4881)	mem 22477MB
Train: [0/180][85/10009]	eta 1:04:46 lr 0.100000	data 0.0007 (0.0212)	batch 0.3561 (0.3917)	loss 4.6196 (5.2154)	grad_norm 3.4420 (5.4643)	mem 22477MB
Train: [0/180][86/10009]	eta 1:04:42 lr 0.100000	data 0.0006 (0.0209)	batch 0.3595 (0.3913)	loss 4.6096 (5.2084)	grad_norm 3.4268 (5.4408)	mem 22477MB
Train: [0/180][87/10009]	eta 1:04:37 lr 0.100000	data 0.0006 (0.0207)	batch 0.3497 (0.3908)	loss 4.6768 (5.2024)	grad_norm 3.5061 (5.4189)	mem 22477MB
Train: [0/180][88/10009]	eta 1:04:33 lr 0.100000	data 0.0007 (0.0205)	batch 0.3541 (0.3904)	loss 4.8602 (5.1985)	grad_norm 3.4332 (5.3966)	mem 22477MB
Train: [0/180][89/10009]	eta 1:04:27 lr 0.100000	data 0.0005 (0.0202)	batch 0.3413 (0.3899)	loss 4.5089 (5.1909)	grad_norm 3.3757 (5.3741)	mem 22477MB
Train: [0/180][90/10009]	eta 1:04:25 lr 0.100000	data 0.0006 (0.0200)	batch 0.3710 (0.3897)	loss 4.4445 (5.1827)	grad_norm 3.3599 (5.3520)	mem 22477MB
Train: [0/180][91/10009]	eta 1:04:21 lr 0.100000	data 0.0006 (0.0198)	batch 0.3565 (0.3893)	loss 4.5062 (5.1753)	grad_norm 3.4860 (5.3317)	mem 22477MB
Train: [0/180][92/10009]	eta 1:04:17 lr 0.100000	data 0.0007 (0.0196)	batch 0.3568 (0.3890)	loss 4.5416 (5.1685)	grad_norm 3.2545 (5.3093)	mem 22477MB
Train: [0/180][93/10009]	eta 1:04:12 lr 0.100000	data 0.0006 (0.0194)	batch 0.3520 (0.3886)	loss 4.4250 (5.1606)	grad_norm 3.4466 (5.2895)	mem 22477MB
Train: [0/180][94/10009]	eta 1:04:08 lr 0.100000	data 0.0006 (0.0192)	batch 0.3516 (0.3882)	loss 4.9509 (5.1584)	grad_norm 3.3985 (5.2696)	mem 22477MB
Train: [0/180][95/10009]	eta 1:04:06 lr 0.100000	data 0.0005 (0.0190)	batch 0.3702 (0.3880)	loss 4.7415 (5.1541)	grad_norm 3.3996 (5.2501)	mem 22477MB
Train: [0/180][96/10009]	eta 1:04:02 lr 0.100000	data 0.0007 (0.0188)	batch 0.3554 (0.3876)	loss 4.6358 (5.1487)	grad_norm 3.4597 (5.2317)	mem 22477MB
Train: [0/180][97/10009]	eta 1:03:59 lr 0.100000	data 0.0007 (0.0186)	batch 0.3622 (0.3874)	loss 4.6141 (5.1433)	grad_norm 3.4005 (5.2130)	mem 22477MB
Train: [0/180][98/10009]	eta 1:03:56 lr 0.100000	data 0.0006 (0.0185)	batch 0.3573 (0.3871)	loss 4.7409 (5.1392)	grad_norm 3.5631 (5.1963)	mem 22477MB
Train: [0/180][99/10009]	eta 1:03:52 lr 0.100000	data 0.0006 (0.0183)	batch 0.3470 (0.3867)	loss 4.6432 (5.1342)	grad_norm 3.3622 (5.1780)	mem 22477MB
Train: [0/180][100/10009]	eta 1:03:47 lr 0.100000	data 0.0007 (0.0181)	batch 0.3492 (0.3863)	loss 5.0013 (5.1329)	grad_norm 3.3521 (5.1599)	mem 22477MB
Train: [0/180][101/10009]	eta 1:03:44 lr 0.100000	data 0.0005 (0.0179)	batch 0.3531 (0.3860)	loss 4.5235 (5.1269)	grad_norm 3.3961 (5.1426)	mem 22477MB
Train: [0/180][102/10009]	eta 1:03:41 lr 0.100000	data 0.0006 (0.0178)	batch 0.3578 (0.3857)	loss 4.5255 (5.1211)	grad_norm 3.3441 (5.1252)	mem 22477MB
Train: [0/180][103/10009]	eta 1:03:38 lr 0.100000	data 0.0006 (0.0176)	batch 0.3584 (0.3855)	loss 4.8371 (5.1184)	grad_norm 3.3100 (5.1077)	mem 22477MB
Train: [0/180][104/10009]	eta 1:03:34 lr 0.100000	data 0.0007 (0.0174)	batch 0.3497 (0.3851)	loss 4.8383 (5.1157)	grad_norm 3.3708 (5.0912)	mem 22477MB
Train: [0/180][105/10009]	eta 1:03:30 lr 0.100000	data 0.0006 (0.0173)	batch 0.3428 (0.3847)	loss 4.3577 (5.1086)	grad_norm 3.3120 (5.0744)	mem 22477MB
Train: [0/180][106/10009]	eta 1:03:28 lr 0.100000	data 0.0006 (0.0171)	batch 0.3664 (0.3845)	loss 4.7233 (5.1050)	grad_norm 3.3016 (5.0578)	mem 22477MB
Train: [0/180][107/10009]	eta 1:03:25 lr 0.100000	data 0.0010 (0.0170)	batch 0.3623 (0.3843)	loss 4.4267 (5.0987)	grad_norm 3.2844 (5.0414)	mem 22477MB
Train: [0/180][108/10009]	eta 1:03:24 lr 0.100000	data 0.0006 (0.0168)	batch 0.3768 (0.3843)	loss 4.7928 (5.0959)	grad_norm 3.3379 (5.0258)	mem 22477MB
Train: [0/180][109/10009]	eta 1:03:22 lr 0.100000	data 0.0008 (0.0167)	batch 0.3635 (0.3841)	loss 4.5215 (5.0906)	grad_norm 3.1165 (5.0084)	mem 22477MB
Train: [0/180][110/10009]	eta 1:03:19 lr 0.100000	data 0.0007 (0.0165)	batch 0.3605 (0.3839)	loss 4.8463 (5.0884)	grad_norm 3.2973 (4.9930)	mem 22477MB
Train: [0/180][111/10009]	eta 1:03:16 lr 0.100000	data 0.0006 (0.0164)	batch 0.3517 (0.3836)	loss 4.4466 (5.0827)	grad_norm 3.1954 (4.9769)	mem 22477MB
Train: [0/180][112/10009]	eta 1:03:14 lr 0.100000	data 0.0005 (0.0163)	batch 0.3580 (0.3834)	loss 4.3727 (5.0764)	grad_norm 3.1386 (4.9607)	mem 22477MB
Train: [0/180][113/10009]	eta 1:03:10 lr 0.100000	data 0.0007 (0.0161)	batch 0.3483 (0.3830)	loss 4.4647 (5.0711)	grad_norm 3.2962 (4.9461)	mem 22477MB
Train: [0/180][114/10009]	eta 1:03:06 lr 0.100000	data 0.0005 (0.0160)	batch 0.3443 (0.3827)	loss 4.4461 (5.0656)	grad_norm 3.2005 (4.9309)	mem 22477MB
Train: [0/180][115/10009]	eta 1:03:04 lr 0.100000	data 0.0007 (0.0158)	batch 0.3546 (0.3825)	loss 4.3558 (5.0595)	grad_norm 3.2143 (4.9161)	mem 22477MB
Train: [0/180][116/10009]	eta 1:03:01 lr 0.100000	data 0.0006 (0.0157)	batch 0.3535 (0.3822)	loss 5.0438 (5.0594)	grad_norm 3.4299 (4.9034)	mem 22477MB
Train: [0/180][117/10009]	eta 1:02:59 lr 0.100000	data 0.0006 (0.0156)	batch 0.3648 (0.3821)	loss 4.7230 (5.0565)	grad_norm 3.2996 (4.8898)	mem 22477MB
Train: [0/180][118/10009]	eta 1:02:56 lr 0.100000	data 0.0008 (0.0155)	batch 0.3548 (0.3818)	loss 4.8441 (5.0547)	grad_norm 3.3036 (4.8765)	mem 22477MB
Train: [0/180][119/10009]	eta 1:02:56 lr 0.100000	data 0.0007 (0.0153)	batch 0.3788 (0.3818)	loss 4.5616 (5.0506)	grad_norm 3.2077 (4.8626)	mem 22477MB
Train: [0/180][120/10009]	eta 1:02:54 lr 0.100000	data 0.0006 (0.0152)	batch 0.3623 (0.3817)	loss 4.6917 (5.0477)	grad_norm 3.3246 (4.8499)	mem 22477MB
Train: [0/180][121/10009]	eta 1:02:51 lr 0.100000	data 0.0007 (0.0151)	batch 0.3590 (0.3815)	loss 4.8366 (5.0459)	grad_norm 3.4103 (4.8381)	mem 22477MB
Train: [0/180][122/10009]	eta 1:02:50 lr 0.100000	data 0.0006 (0.0150)	batch 0.3672 (0.3814)	loss 4.4823 (5.0414)	grad_norm 3.4109 (4.8265)	mem 22477MB
Train: [0/180][123/10009]	eta 1:02:48 lr 0.100000	data 0.0007 (0.0149)	batch 0.3561 (0.3811)	loss 4.3315 (5.0356)	grad_norm 3.2032 (4.8134)	mem 22477MB
Train: [0/180][124/10009]	eta 1:02:45 lr 0.100000	data 0.0006 (0.0148)	batch 0.3567 (0.3810)	loss 4.4075 (5.0306)	grad_norm 3.2972 (4.8012)	mem 22477MB
Train: [0/180][125/10009]	eta 1:02:43 lr 0.100000	data 0.0006 (0.0146)	batch 0.3636 (0.3808)	loss 4.3380 (5.0251)	grad_norm 3.2268 (4.7887)	mem 22477MB
Train: [0/180][126/10009]	eta 1:02:41 lr 0.100000	data 0.0007 (0.0145)	batch 0.3517 (0.3806)	loss 4.4139 (5.0203)	grad_norm 3.2369 (4.7765)	mem 22477MB
Train: [0/180][127/10009]	eta 1:02:39 lr 0.100000	data 0.0008 (0.0144)	batch 0.3675 (0.3805)	loss 4.7681 (5.0183)	grad_norm 3.2831 (4.7649)	mem 22477MB
Train: [0/180][128/10009]	eta 1:02:38 lr 0.100000	data 0.0007 (0.0143)	batch 0.3643 (0.3804)	loss 4.4119 (5.0136)	grad_norm 3.1783 (4.7526)	mem 22477MB
Train: [0/180][129/10009]	eta 1:02:36 lr 0.100000	data 0.0007 (0.0142)	batch 0.3565 (0.3802)	loss 4.8017 (5.0120)	grad_norm 3.3526 (4.7418)	mem 22477MB
Train: [0/180][130/10009]	eta 1:02:34 lr 0.100000	data 0.0007 (0.0141)	batch 0.3628 (0.3800)	loss 4.3845 (5.0072)	grad_norm 3.1908 (4.7299)	mem 22477MB
Train: [0/180][131/10009]	eta 1:02:32 lr 0.100000	data 0.0007 (0.0140)	batch 0.3583 (0.3799)	loss 4.6147 (5.0042)	grad_norm 3.1371 (4.7179)	mem 22477MB
Train: [0/180][132/10009]	eta 1:02:30 lr 0.100000	data 0.0009 (0.0139)	batch 0.3537 (0.3797)	loss 4.3182 (4.9991)	grad_norm 3.1136 (4.7058)	mem 22477MB
Train: [0/180][133/10009]	eta 1:02:27 lr 0.100000	data 0.0006 (0.0138)	batch 0.3521 (0.3795)	loss 4.2647 (4.9936)	grad_norm 3.1241 (4.6940)	mem 22477MB
Train: [0/180][134/10009]	eta 1:02:26 lr 0.100000	data 0.0007 (0.0137)	batch 0.3706 (0.3794)	loss 4.6604 (4.9911)	grad_norm 3.2445 (4.6833)	mem 22477MB
Train: [0/180][135/10009]	eta 1:02:24 lr 0.100000	data 0.0007 (0.0136)	batch 0.3542 (0.3792)	loss 4.5683 (4.9880)	grad_norm 3.2729 (4.6729)	mem 22477MB
Train: [0/180][136/10009]	eta 1:02:22 lr 0.100000	data 0.0008 (0.0135)	batch 0.3623 (0.3791)	loss 4.4579 (4.9841)	grad_norm 3.0799 (4.6613)	mem 22477MB
Train: [0/180][137/10009]	eta 1:02:20 lr 0.100000	data 0.0005 (0.0134)	batch 0.3570 (0.3789)	loss 4.3855 (4.9798)	grad_norm 3.1363 (4.6502)	mem 22477MB
Train: [0/180][138/10009]	eta 1:02:18 lr 0.100000	data 0.0006 (0.0133)	batch 0.3537 (0.3788)	loss 4.2872 (4.9748)	grad_norm 3.1423 (4.6394)	mem 22477MB
Train: [0/180][139/10009]	eta 1:02:16 lr 0.100000	data 0.0007 (0.0132)	batch 0.3539 (0.3786)	loss 4.7798 (4.9734)	grad_norm 3.2114 (4.6292)	mem 22477MB
Train: [0/180][140/10009]	eta 1:02:15 lr 0.100000	data 0.0006 (0.0132)	batch 0.3704 (0.3785)	loss 4.7071 (4.9715)	grad_norm 3.1055 (4.6184)	mem 22477MB
Train: [0/180][141/10009]	eta 1:02:12 lr 0.100000	data 0.0005 (0.0131)	batch 0.3435 (0.3783)	loss 4.7751 (4.9702)	grad_norm 3.1970 (4.6084)	mem 22477MB
Train: [0/180][142/10009]	eta 1:02:10 lr 0.100000	data 0.0006 (0.0130)	batch 0.3482 (0.3781)	loss 4.4290 (4.9664)	grad_norm 3.2950 (4.5992)	mem 22477MB
Train: [0/180][143/10009]	eta 1:02:08 lr 0.100000	data 0.0006 (0.0129)	batch 0.3557 (0.3779)	loss 4.5649 (4.9636)	grad_norm 3.2208 (4.5896)	mem 22477MB
Train: [0/180][144/10009]	eta 1:02:07 lr 0.100000	data 0.0007 (0.0128)	batch 0.3717 (0.3779)	loss 4.6653 (4.9615)	grad_norm 3.1276 (4.5795)	mem 22477MB
Train: [0/180][145/10009]	eta 1:02:06 lr 0.100000	data 0.0007 (0.0127)	batch 0.3602 (0.3777)	loss 4.5903 (4.9590)	grad_norm 3.0775 (4.5692)	mem 22477MB
Train: [0/180][146/10009]	eta 1:02:03 lr 0.100000	data 0.0007 (0.0126)	batch 0.3483 (0.3775)	loss 4.7837 (4.9578)	grad_norm 3.0871 (4.5591)	mem 22477MB
Train: [0/180][147/10009]	eta 1:02:02 lr 0.100000	data 0.0008 (0.0126)	batch 0.3694 (0.3775)	loss 4.4067 (4.9541)	grad_norm 3.0583 (4.5490)	mem 22477MB
Train: [0/180][148/10009]	eta 1:02:01 lr 0.100000	data 0.0006 (0.0125)	batch 0.3565 (0.3773)	loss 4.5486 (4.9514)	grad_norm 3.1692 (4.5397)	mem 22477MB
Train: [0/180][149/10009]	eta 1:01:59 lr 0.100000	data 0.0008 (0.0124)	batch 0.3565 (0.3772)	loss 4.3365 (4.9473)	grad_norm 3.1784 (4.5307)	mem 22477MB
Train: [0/180][150/10009]	eta 1:01:57 lr 0.100000	data 0.0010 (0.0123)	batch 0.3538 (0.3771)	loss 4.5513 (4.9446)	grad_norm 3.1981 (4.5218)	mem 22477MB
Train: [0/180][151/10009]	eta 1:01:55 lr 0.100000	data 0.0007 (0.0123)	batch 0.3606 (0.3769)	loss 4.4552 (4.9414)	grad_norm 3.2510 (4.5135)	mem 22477MB
Train: [0/180][152/10009]	eta 1:01:53 lr 0.100000	data 0.0007 (0.0122)	batch 0.3454 (0.3767)	loss 4.4533 (4.9382)	grad_norm 3.2065 (4.5049)	mem 22477MB
Train: [0/180][153/10009]	eta 1:01:51 lr 0.100000	data 0.0006 (0.0121)	batch 0.3498 (0.3766)	loss 4.3380 (4.9343)	grad_norm 3.2634 (4.4969)	mem 22477MB
Train: [0/180][154/10009]	eta 1:01:51 lr 0.100000	data 0.0007 (0.0120)	batch 0.3763 (0.3766)	loss 4.4759 (4.9314)	grad_norm 3.1567 (4.4882)	mem 22477MB
Train: [0/180][155/10009]	eta 1:01:49 lr 0.100000	data 0.0007 (0.0120)	batch 0.3648 (0.3765)	loss 4.0674 (4.9258)	grad_norm 3.0366 (4.4789)	mem 22477MB
Train: [0/180][156/10009]	eta 1:01:48 lr 0.100000	data 0.0007 (0.0119)	batch 0.3668 (0.3764)	loss 4.6661 (4.9242)	grad_norm 3.1331 (4.4704)	mem 22477MB
Train: [0/180][157/10009]	eta 1:01:47 lr 0.100000	data 0.0005 (0.0118)	batch 0.3527 (0.3763)	loss 4.3163 (4.9203)	grad_norm 3.3125 (4.4630)	mem 22477MB
Train: [0/180][158/10009]	eta 1:01:45 lr 0.100000	data 0.0006 (0.0117)	batch 0.3563 (0.3762)	loss 4.5020 (4.9177)	grad_norm 3.1042 (4.4545)	mem 22477MB
Train: [0/180][159/10009]	eta 1:01:43 lr 0.100000	data 0.0009 (0.0117)	batch 0.3558 (0.3760)	loss 4.5077 (4.9151)	grad_norm 3.1454 (4.4463)	mem 22477MB
Train: [0/180][160/10009]	eta 1:01:41 lr 0.100000	data 0.0007 (0.0116)	batch 0.3503 (0.3759)	loss 4.2632 (4.9111)	grad_norm 3.2124 (4.4386)	mem 22477MB
Train: [0/180][161/10009]	eta 1:01:39 lr 0.100000	data 0.0006 (0.0115)	batch 0.3420 (0.3757)	loss 4.3637 (4.9077)	grad_norm 3.1511 (4.4307)	mem 22477MB
Train: [0/180][162/10009]	eta 1:01:37 lr 0.100000	data 0.0006 (0.0115)	batch 0.3541 (0.3755)	loss 4.1631 (4.9031)	grad_norm 3.1346 (4.4227)	mem 22477MB
Train: [0/180][163/10009]	eta 1:01:36 lr 0.100000	data 0.0005 (0.0114)	batch 0.3663 (0.3755)	loss 4.4650 (4.9005)	grad_norm 3.2816 (4.4158)	mem 22477MB
Train: [0/180][164/10009]	eta 1:01:35 lr 0.100000	data 0.0006 (0.0113)	batch 0.3576 (0.3754)	loss 4.4191 (4.8975)	grad_norm 3.1999 (4.4084)	mem 22477MB
Train: [0/180][165/10009]	eta 1:01:34 lr 0.100000	data 0.0007 (0.0113)	batch 0.3624 (0.3753)	loss 4.3497 (4.8942)	grad_norm 2.9781 (4.3998)	mem 22477MB
Train: [0/180][166/10009]	eta 1:01:32 lr 0.100000	data 0.0007 (0.0112)	batch 0.3466 (0.3751)	loss 4.3883 (4.8912)	grad_norm 2.9950 (4.3914)	mem 22477MB
Train: [0/180][167/10009]	eta 1:01:32 lr 0.100000	data 0.0008 (0.0112)	batch 0.3911 (0.3752)	loss 4.6376 (4.8897)	grad_norm 3.2063 (4.3843)	mem 22477MB
Train: [0/180][168/10009]	eta 1:01:31 lr 0.100000	data 0.0007 (0.0111)	batch 0.3568 (0.3751)	loss 4.4931 (4.8874)	grad_norm 3.1131 (4.3768)	mem 22477MB
Train: [0/180][169/10009]	eta 1:01:30 lr 0.100000	data 0.0007 (0.0110)	batch 0.3620 (0.3750)	loss 4.5616 (4.8854)	grad_norm 3.1453 (4.3696)	mem 22477MB
Train: [0/180][170/10009]	eta 1:01:29 lr 0.100000	data 0.0006 (0.0110)	batch 0.3777 (0.3750)	loss 4.5394 (4.8834)	grad_norm 3.0760 (4.3620)	mem 22477MB
Train: [0/180][171/10009]	eta 1:01:28 lr 0.100000	data 0.0007 (0.0109)	batch 0.3584 (0.3749)	loss 4.5435 (4.8814)	grad_norm 3.1821 (4.3551)	mem 22477MB
Train: [0/180][172/10009]	eta 1:01:27 lr 0.100000	data 0.0011 (0.0109)	batch 0.3568 (0.3748)	loss 4.5971 (4.8798)	grad_norm 3.1207 (4.3480)	mem 22477MB
Train: [0/180][173/10009]	eta 1:01:26 lr 0.100000	data 0.0007 (0.0108)	batch 0.3602 (0.3747)	loss 4.5974 (4.8782)	grad_norm 3.0563 (4.3406)	mem 22477MB
Train: [0/180][174/10009]	eta 1:01:24 lr 0.100000	data 0.0007 (0.0107)	batch 0.3535 (0.3746)	loss 4.4024 (4.8755)	grad_norm 3.1315 (4.3337)	mem 22477MB
Train: [0/180][175/10009]	eta 1:01:22 lr 0.100000	data 0.0006 (0.0107)	batch 0.3511 (0.3745)	loss 4.3425 (4.8724)	grad_norm 3.1036 (4.3267)	mem 22477MB
Train: [0/180][176/10009]	eta 1:01:21 lr 0.100000	data 0.0007 (0.0106)	batch 0.3526 (0.3744)	loss 4.3671 (4.8696)	grad_norm 2.9539 (4.3189)	mem 22477MB
Train: [0/180][177/10009]	eta 1:01:19 lr 0.100000	data 0.0008 (0.0106)	batch 0.3566 (0.3743)	loss 4.3054 (4.8664)	grad_norm 3.0656 (4.3119)	mem 22477MB
Train: [0/180][178/10009]	eta 1:01:17 lr 0.100000	data 0.0006 (0.0105)	batch 0.3468 (0.3741)	loss 4.4142 (4.8639)	grad_norm 3.0582 (4.3049)	mem 22477MB
Train: [0/180][179/10009]	eta 1:01:16 lr 0.100000	data 0.0006 (0.0105)	batch 0.3611 (0.3740)	loss 4.0855 (4.8596)	grad_norm 2.9912 (4.2976)	mem 22477MB
Train: [0/180][180/10009]	eta 1:01:15 lr 0.100000	data 0.0007 (0.0104)	batch 0.3503 (0.3739)	loss 4.6458 (4.8584)	grad_norm 3.1247 (4.2911)	mem 22477MB
Train: [0/180][181/10009]	eta 1:01:14 lr 0.100000	data 0.0007 (0.0103)	batch 0.3663 (0.3739)	loss 4.2997 (4.8553)	grad_norm 3.1014 (4.2846)	mem 22477MB
Train: [0/180][182/10009]	eta 1:01:14 lr 0.100000	data 0.0008 (0.0103)	batch 0.3768 (0.3739)	loss 4.4010 (4.8528)	grad_norm 3.1015 (4.2781)	mem 22477MB
Train: [0/180][183/10009]	eta 1:01:12 lr 0.100000	data 0.0007 (0.0102)	batch 0.3468 (0.3737)	loss 4.6748 (4.8519)	grad_norm 3.0646 (4.2715)	mem 22477MB
Train: [0/180][184/10009]	eta 1:01:10 lr 0.100000	data 0.0008 (0.0102)	batch 0.3462 (0.3736)	loss 4.3569 (4.8492)	grad_norm 3.2952 (4.2662)	mem 22477MB
Train: [0/180][185/10009]	eta 1:01:10 lr 0.100000	data 0.0007 (0.0101)	batch 0.3716 (0.3736)	loss 4.4926 (4.8473)	grad_norm 3.0398 (4.2596)	mem 22477MB
Train: [0/180][186/10009]	eta 1:01:08 lr 0.100000	data 0.0006 (0.0101)	batch 0.3484 (0.3734)	loss 4.7038 (4.8465)	grad_norm 3.1577 (4.2537)	mem 22477MB
Train: [0/180][187/10009]	eta 1:01:06 lr 0.100000	data 0.0009 (0.0100)	batch 0.3532 (0.3733)	loss 4.3609 (4.8439)	grad_norm 3.0510 (4.2473)	mem 22477MB
Train: [0/180][188/10009]	eta 1:01:05 lr 0.100000	data 0.0007 (0.0100)	batch 0.3574 (0.3733)	loss 4.3371 (4.8412)	grad_norm 3.0753 (4.2411)	mem 22477MB
Train: [0/180][189/10009]	eta 1:01:04 lr 0.100000	data 0.0008 (0.0099)	batch 0.3644 (0.3732)	loss 4.3921 (4.8389)	grad_norm 3.0713 (4.2350)	mem 22477MB
Train: [0/180][190/10009]	eta 1:01:03 lr 0.100000	data 0.0007 (0.0099)	batch 0.3594 (0.3731)	loss 4.6086 (4.8377)	grad_norm 3.1411 (4.2293)	mem 22477MB
Train: [0/180][191/10009]	eta 1:01:03 lr 0.100000	data 0.0007 (0.0098)	batch 0.3797 (0.3732)	loss 4.3151 (4.8349)	grad_norm 3.0137 (4.2229)	mem 22477MB
Train: [0/180][192/10009]	eta 1:01:02 lr 0.100000	data 0.0007 (0.0098)	batch 0.3471 (0.3730)	loss 4.1079 (4.8312)	grad_norm 2.9820 (4.2165)	mem 22477MB
Train: [0/180][193/10009]	eta 1:01:01 lr 0.100000	data 0.0006 (0.0098)	batch 0.3662 (0.3730)	loss 4.1428 (4.8276)	grad_norm 2.9268 (4.2099)	mem 22477MB
Train: [0/180][194/10009]	eta 1:01:00 lr 0.100000	data 0.0007 (0.0097)	batch 0.3628 (0.3729)	loss 4.5040 (4.8260)	grad_norm 3.0192 (4.2037)	mem 22477MB
Train: [0/180][195/10009]	eta 1:00:59 lr 0.100000	data 0.0007 (0.0097)	batch 0.3553 (0.3729)	loss 4.4034 (4.8238)	grad_norm 2.9502 (4.1973)	mem 22477MB
Train: [0/180][196/10009]	eta 1:00:57 lr 0.100000	data 0.0007 (0.0096)	batch 0.3520 (0.3727)	loss 4.3756 (4.8215)	grad_norm 3.0893 (4.1917)	mem 22477MB
Train: [0/180][197/10009]	eta 1:00:56 lr 0.100000	data 0.0007 (0.0096)	batch 0.3589 (0.3727)	loss 4.2756 (4.8188)	grad_norm 3.2274 (4.1869)	mem 22477MB
Train: [0/180][198/10009]	eta 1:00:55 lr 0.100000	data 0.0007 (0.0095)	batch 0.3534 (0.3726)	loss 4.3769 (4.8166)	grad_norm 2.9839 (4.1808)	mem 22477MB
Train: [0/180][199/10009]	eta 1:00:54 lr 0.100000	data 0.0007 (0.0095)	batch 0.3582 (0.3725)	loss 4.7812 (4.8164)	grad_norm 3.1948 (4.1759)	mem 22477MB
Train: [0/180][200/10009]	eta 1:00:53 lr 0.100000	data 0.0007 (0.0094)	batch 0.3638 (0.3725)	loss 4.3454 (4.8140)	grad_norm 3.1907 (4.1710)	mem 22477MB
Train: [0/180][201/10009]	eta 1:00:52 lr 0.100000	data 0.0007 (0.0094)	batch 0.3488 (0.3723)	loss 4.6805 (4.8134)	grad_norm 3.0166 (4.1653)	mem 22477MB
Train: [0/180][202/10009]	eta 1:00:50 lr 0.100000	data 0.0007 (0.0094)	batch 0.3523 (0.3723)	loss 4.2350 (4.8105)	grad_norm 2.9324 (4.1592)	mem 22477MB
Train: [0/180][203/10009]	eta 1:00:50 lr 0.100000	data 0.0007 (0.0093)	batch 0.3755 (0.3723)	loss 4.2335 (4.8077)	grad_norm 2.9981 (4.1535)	mem 22477MB
Train: [0/180][204/10009]	eta 1:00:49 lr 0.100000	data 0.0007 (0.0093)	batch 0.3569 (0.3722)	loss 4.1387 (4.8044)	grad_norm 3.1033 (4.1484)	mem 22477MB
Train: [0/180][205/10009]	eta 1:00:48 lr 0.100000	data 0.0006 (0.0092)	batch 0.3555 (0.3721)	loss 4.5685 (4.8033)	grad_norm 3.0190 (4.1429)	mem 22477MB
Train: [0/180][206/10009]	eta 1:00:48 lr 0.100000	data 0.0007 (0.0092)	batch 0.3773 (0.3721)	loss 4.5750 (4.8022)	grad_norm 3.2222 (4.1384)	mem 22477MB
Train: [0/180][207/10009]	eta 1:00:46 lr 0.100000	data 0.0007 (0.0091)	batch 0.3498 (0.3720)	loss 4.2390 (4.7995)	grad_norm 2.9860 (4.1329)	mem 22477MB
Train: [0/180][208/10009]	eta 1:00:46 lr 0.100000	data 0.0008 (0.0091)	batch 0.3784 (0.3721)	loss 4.3718 (4.7974)	grad_norm 3.1548 (4.1282)	mem 22477MB
Train: [0/180][209/10009]	eta 1:00:45 lr 0.100000	data 0.0006 (0.0091)	batch 0.3598 (0.3720)	loss 4.4345 (4.7957)	grad_norm 3.0712 (4.1232)	mem 22477MB
Train: [0/180][210/10009]	eta 1:00:44 lr 0.100000	data 0.0006 (0.0090)	batch 0.3606 (0.3719)	loss 4.3669 (4.7937)	grad_norm 3.0347 (4.1180)	mem 22477MB
Train: [0/180][211/10009]	eta 1:00:44 lr 0.100000	data 0.0007 (0.0090)	batch 0.3652 (0.3719)	loss 4.2006 (4.7909)	grad_norm 3.0292 (4.1129)	mem 22477MB
Train: [0/180][212/10009]	eta 1:00:42 lr 0.100000	data 0.0007 (0.0089)	batch 0.3544 (0.3718)	loss 4.1918 (4.7881)	grad_norm 3.0002 (4.1077)	mem 22477MB
Train: [0/180][213/10009]	eta 1:00:42 lr 0.100000	data 0.0007 (0.0089)	batch 0.3675 (0.3718)	loss 4.2602 (4.7856)	grad_norm 3.0640 (4.1028)	mem 22477MB
Train: [0/180][214/10009]	eta 1:00:41 lr 0.100000	data 0.0007 (0.0089)	batch 0.3592 (0.3718)	loss 4.2388 (4.7830)	grad_norm 3.0509 (4.0979)	mem 22477MB
Train: [0/180][215/10009]	eta 1:00:40 lr 0.100000	data 0.0008 (0.0088)	batch 0.3634 (0.3717)	loss 4.3672 (4.7811)	grad_norm 3.1055 (4.0933)	mem 22477MB
Train: [0/180][216/10009]	eta 1:00:39 lr 0.100000	data 0.0007 (0.0088)	batch 0.3625 (0.3717)	loss 4.5306 (4.7800)	grad_norm 3.1117 (4.0888)	mem 22477MB
Train: [0/180][217/10009]	eta 1:00:38 lr 0.100000	data 0.0007 (0.0088)	batch 0.3522 (0.3716)	loss 4.1962 (4.7773)	grad_norm 3.0879 (4.0842)	mem 22477MB
Train: [0/180][218/10009]	eta 1:00:41 lr 0.100000	data 0.0007 (0.0087)	batch 0.4546 (0.3720)	loss 4.4146 (4.7756)	grad_norm 3.0758 (4.0796)	mem 22477MB
Train: [0/180][219/10009]	eta 1:00:41 lr 0.100000	data 0.0007 (0.0087)	batch 0.3628 (0.3719)	loss 4.4128 (4.7740)	grad_norm 3.0591 (4.0749)	mem 22477MB
Train: [0/180][220/10009]	eta 1:00:39 lr 0.100000	data 0.0007 (0.0086)	batch 0.3541 (0.3718)	loss 4.3096 (4.7719)	grad_norm 2.9705 (4.0700)	mem 22477MB
Train: [0/180][221/10009]	eta 1:00:38 lr 0.100000	data 0.0006 (0.0086)	batch 0.3550 (0.3718)	loss 4.4033 (4.7702)	grad_norm 3.0798 (4.0655)	mem 22477MB
Train: [0/180][222/10009]	eta 1:00:38 lr 0.100000	data 0.0009 (0.0086)	batch 0.3639 (0.3717)	loss 4.3639 (4.7684)	grad_norm 2.9467 (4.0605)	mem 22477MB
Train: [0/180][223/10009]	eta 1:00:36 lr 0.100000	data 0.0007 (0.0085)	batch 0.3539 (0.3716)	loss 4.3020 (4.7663)	grad_norm 3.0292 (4.0559)	mem 22477MB
Train: [0/180][224/10009]	eta 1:00:36 lr 0.100000	data 0.0007 (0.0085)	batch 0.3600 (0.3716)	loss 4.6350 (4.7657)	grad_norm 3.2202 (4.0522)	mem 22477MB
Train: [0/180][225/10009]	eta 1:00:34 lr 0.100000	data 0.0006 (0.0085)	batch 0.3529 (0.3715)	loss 4.2682 (4.7635)	grad_norm 2.9429 (4.0472)	mem 22477MB
Train: [0/180][226/10009]	eta 1:00:35 lr 0.100000	data 0.0007 (0.0084)	batch 0.3855 (0.3716)	loss 4.1281 (4.7607)	grad_norm 2.9282 (4.0423)	mem 22477MB
Train: [0/180][227/10009]	eta 1:00:34 lr 0.100000	data 0.0007 (0.0084)	batch 0.3551 (0.3715)	loss 4.6966 (4.7605)	grad_norm 3.1465 (4.0384)	mem 22477MB
Train: [0/180][228/10009]	eta 1:00:33 lr 0.100000	data 0.0006 (0.0084)	batch 0.3747 (0.3715)	loss 4.3647 (4.7587)	grad_norm 3.1693 (4.0346)	mem 22477MB
Train: [0/180][229/10009]	eta 1:00:32 lr 0.100000	data 0.0009 (0.0083)	batch 0.3564 (0.3715)	loss 4.3142 (4.7568)	grad_norm 3.0608 (4.0304)	mem 22477MB
Train: [0/180][230/10009]	eta 1:00:32 lr 0.100000	data 0.0007 (0.0083)	batch 0.3746 (0.3715)	loss 4.3422 (4.7550)	grad_norm 3.0267 (4.0260)	mem 22477MB
Train: [0/180][231/10009]	eta 1:00:31 lr 0.100000	data 0.0006 (0.0083)	batch 0.3470 (0.3714)	loss 4.5058 (4.7539)	grad_norm 3.2577 (4.0227)	mem 22477MB
Train: [0/180][232/10009]	eta 1:00:30 lr 0.100000	data 0.0006 (0.0082)	batch 0.3652 (0.3713)	loss 4.0761 (4.7510)	grad_norm 3.0024 (4.0183)	mem 22477MB
Train: [0/180][233/10009]	eta 1:00:29 lr 0.100000	data 0.0006 (0.0082)	batch 0.3517 (0.3713)	loss 4.2503 (4.7489)	grad_norm 3.0700 (4.0143)	mem 22477MB
Train: [0/180][234/10009]	eta 1:00:29 lr 0.100000	data 0.0007 (0.0082)	batch 0.3746 (0.3713)	loss 4.1677 (4.7464)	grad_norm 3.0906 (4.0103)	mem 22477MB
Train: [0/180][235/10009]	eta 1:00:28 lr 0.100000	data 0.0007 (0.0081)	batch 0.3560 (0.3712)	loss 4.3039 (4.7445)	grad_norm 2.9286 (4.0058)	mem 22477MB
Train: [0/180][236/10009]	eta 1:00:27 lr 0.100000	data 0.0007 (0.0081)	batch 0.3648 (0.3712)	loss 4.6021 (4.7439)	grad_norm 3.1149 (4.0020)	mem 22477MB
Train: [0/180][237/10009]	eta 1:00:27 lr 0.100000	data 0.0007 (0.0081)	batch 0.3904 (0.3713)	loss 4.0386 (4.7410)	grad_norm 2.8363 (3.9971)	mem 22477MB
Train: [0/180][238/10009]	eta 1:00:27 lr 0.100000	data 0.0006 (0.0080)	batch 0.3648 (0.3712)	loss 4.3603 (4.7394)	grad_norm 2.9141 (3.9926)	mem 22477MB
Train: [0/180][239/10009]	eta 1:00:26 lr 0.100000	data 0.0007 (0.0080)	batch 0.3562 (0.3712)	loss 4.0323 (4.7364)	grad_norm 2.9728 (3.9883)	mem 22477MB
Train: [0/180][240/10009]	eta 1:00:25 lr 0.100000	data 0.0008 (0.0080)	batch 0.3592 (0.3711)	loss 3.8681 (4.7328)	grad_norm 2.7833 (3.9833)	mem 22477MB
Train: [0/180][241/10009]	eta 1:00:24 lr 0.100000	data 0.0005 (0.0080)	batch 0.3478 (0.3710)	loss 4.2182 (4.7307)	grad_norm 2.9710 (3.9791)	mem 22477MB
Train: [0/180][242/10009]	eta 1:00:22 lr 0.100000	data 0.0006 (0.0079)	batch 0.3450 (0.3709)	loss 4.5035 (4.7298)	grad_norm 3.0034 (3.9751)	mem 22477MB
Train: [0/180][243/10009]	eta 1:00:22 lr 0.100000	data 0.0007 (0.0079)	batch 0.3688 (0.3709)	loss 4.6135 (4.7293)	grad_norm 3.1214 (3.9716)	mem 22477MB
Train: [0/180][244/10009]	eta 1:00:20 lr 0.100000	data 0.0005 (0.0079)	batch 0.3463 (0.3708)	loss 4.3929 (4.7279)	grad_norm 3.0356 (3.9678)	mem 22477MB
Train: [0/180][245/10009]	eta 1:00:19 lr 0.100000	data 0.0007 (0.0078)	batch 0.3537 (0.3707)	loss 4.3608 (4.7264)	grad_norm 2.8486 (3.9633)	mem 22477MB
Train: [0/180][246/10009]	eta 1:00:18 lr 0.100000	data 0.0007 (0.0078)	batch 0.3550 (0.3707)	loss 4.2039 (4.7243)	grad_norm 2.8883 (3.9589)	mem 22477MB
Train: [0/180][247/10009]	eta 1:00:17 lr 0.100000	data 0.0005 (0.0078)	batch 0.3520 (0.3706)	loss 4.4674 (4.7233)	grad_norm 2.9594 (3.9549)	mem 22477MB
Train: [0/180][248/10009]	eta 1:00:16 lr 0.100000	data 0.0006 (0.0077)	batch 0.3543 (0.3705)	loss 4.2573 (4.7214)	grad_norm 2.9704 (3.9509)	mem 22477MB
Train: [0/180][249/10009]	eta 1:00:15 lr 0.100000	data 0.0007 (0.0077)	batch 0.3564 (0.3705)	loss 4.0871 (4.7189)	grad_norm 2.9460 (3.9469)	mem 22477MB
Train: [0/180][250/10009]	eta 1:00:14 lr 0.100000	data 0.0007 (0.0077)	batch 0.3567 (0.3704)	loss 4.5254 (4.7181)	grad_norm 3.0703 (3.9434)	mem 22477MB
Train: [0/180][251/10009]	eta 1:00:13 lr 0.100000	data 0.0006 (0.0077)	batch 0.3526 (0.3703)	loss 4.5587 (4.7175)	grad_norm 3.0017 (3.9397)	mem 22477MB
Train: [0/180][252/10009]	eta 1:00:12 lr 0.100000	data 0.0007 (0.0076)	batch 0.3563 (0.3703)	loss 4.3500 (4.7160)	grad_norm 2.9654 (3.9358)	mem 22477MB
Train: [0/180][253/10009]	eta 1:00:11 lr 0.100000	data 0.0005 (0.0076)	batch 0.3473 (0.3702)	loss 3.9225 (4.7129)	grad_norm 2.7658 (3.9312)	mem 22477MB
Train: [0/180][254/10009]	eta 1:00:10 lr 0.100000	data 0.0006 (0.0076)	batch 0.3549 (0.3701)	loss 4.3928 (4.7116)	grad_norm 2.9066 (3.9272)	mem 22477MB
Train: [0/180][255/10009]	eta 1:00:10 lr 0.100000	data 0.0094 (0.0076)	batch 0.3670 (0.3701)	loss 3.9910 (4.7088)	grad_norm 2.7977 (3.9228)	mem 22477MB
Train: [0/180][256/10009]	eta 1:00:09 lr 0.100000	data 0.0006 (0.0076)	batch 0.3616 (0.3701)	loss 4.2242 (4.7069)	grad_norm 2.9475 (3.9190)	mem 22477MB
Train: [0/180][257/10009]	eta 1:00:08 lr 0.100000	data 0.0007 (0.0075)	batch 0.3536 (0.3700)	loss 4.3731 (4.7056)	grad_norm 3.0342 (3.9156)	mem 22477MB
Train: [0/180][258/10009]	eta 1:00:07 lr 0.100000	data 0.0008 (0.0075)	batch 0.3495 (0.3700)	loss 4.3130 (4.7041)	grad_norm 2.8314 (3.9114)	mem 22477MB
Train: [0/180][259/10009]	eta 1:00:06 lr 0.100000	data 0.0005 (0.0075)	batch 0.3517 (0.3699)	loss 4.6351 (4.7038)	grad_norm 2.9361 (3.9076)	mem 22477MB
Train: [0/180][260/10009]	eta 1:00:05 lr 0.100000	data 0.0006 (0.0075)	batch 0.3583 (0.3698)	loss 4.3187 (4.7024)	grad_norm 2.8453 (3.9036)	mem 22477MB
Train: [0/180][261/10009]	eta 1:00:04 lr 0.100000	data 0.0007 (0.0074)	batch 0.3594 (0.3698)	loss 4.4400 (4.7014)	grad_norm 2.9596 (3.8999)	mem 22477MB
Train: [0/180][262/10009]	eta 1:00:03 lr 0.100000	data 0.0007 (0.0074)	batch 0.3489 (0.3697)	loss 4.2806 (4.6998)	grad_norm 2.9048 (3.8962)	mem 22477MB
Train: [0/180][263/10009]	eta 1:00:02 lr 0.100000	data 0.0007 (0.0074)	batch 0.3621 (0.3697)	loss 4.4475 (4.6988)	grad_norm 3.0585 (3.8930)	mem 22477MB
Train: [0/180][264/10009]	eta 1:00:02 lr 0.100000	data 0.0007 (0.0074)	batch 0.3638 (0.3697)	loss 4.0432 (4.6963)	grad_norm 2.9716 (3.8895)	mem 22477MB
Train: [0/180][265/10009]	eta 1:00:01 lr 0.100000	data 0.0006 (0.0073)	batch 0.3441 (0.3696)	loss 4.5151 (4.6957)	grad_norm 3.0544 (3.8864)	mem 22477MB
Train: [0/180][266/10009]	eta 0:59:59 lr 0.100000	data 0.0006 (0.0073)	batch 0.3479 (0.3695)	loss 4.0619 (4.6933)	grad_norm 2.8703 (3.8826)	mem 22477MB
Train: [0/180][267/10009]	eta 0:59:59 lr 0.100000	data 0.0007 (0.0073)	batch 0.3566 (0.3694)	loss 4.4607 (4.6924)	grad_norm 2.8297 (3.8786)	mem 22477MB
Train: [0/180][268/10009]	eta 0:59:58 lr 0.100000	data 0.0006 (0.0073)	batch 0.3568 (0.3694)	loss 4.2728 (4.6909)	grad_norm 2.9368 (3.8751)	mem 22477MB
Train: [0/180][269/10009]	eta 0:59:57 lr 0.100000	data 0.0007 (0.0072)	batch 0.3551 (0.3693)	loss 4.1013 (4.6887)	grad_norm 2.8318 (3.8713)	mem 22477MB
Train: [0/180][270/10009]	eta 0:59:56 lr 0.100000	data 0.0007 (0.0072)	batch 0.3503 (0.3693)	loss 4.3356 (4.6874)	grad_norm 3.0238 (3.8681)	mem 22477MB
Train: [0/180][271/10009]	eta 0:59:55 lr 0.100000	data 0.0006 (0.0072)	batch 0.3588 (0.3692)	loss 4.1600 (4.6854)	grad_norm 2.9403 (3.8647)	mem 22477MB
Train: [0/180][272/10009]	eta 0:59:54 lr 0.100000	data 0.0006 (0.0072)	batch 0.3482 (0.3692)	loss 4.2891 (4.6840)	grad_norm 2.9585 (3.8614)	mem 22477MB
Train: [0/180][273/10009]	eta 0:59:53 lr 0.100000	data 0.0006 (0.0071)	batch 0.3584 (0.3691)	loss 4.1740 (4.6821)	grad_norm 2.8287 (3.8576)	mem 22477MB
Train: [0/180][274/10009]	eta 0:59:53 lr 0.100000	data 0.0006 (0.0071)	batch 0.3658 (0.3691)	loss 4.3215 (4.6808)	grad_norm 2.8627 (3.8540)	mem 22477MB
Train: [0/180][275/10009]	eta 0:59:52 lr 0.100000	data 0.0008 (0.0071)	batch 0.3657 (0.3691)	loss 4.2772 (4.6793)	grad_norm 2.8667 (3.8505)	mem 22477MB
Train: [0/180][276/10009]	eta 0:59:52 lr 0.100000	data 0.0008 (0.0071)	batch 0.3622 (0.3691)	loss 4.1939 (4.6776)	grad_norm 2.8513 (3.8468)	mem 22477MB
Train: [0/180][277/10009]	eta 0:59:51 lr 0.100000	data 0.0008 (0.0070)	batch 0.3627 (0.3690)	loss 4.1703 (4.6758)	grad_norm 2.9495 (3.8436)	mem 22477MB
Train: [0/180][278/10009]	eta 0:59:50 lr 0.100000	data 0.0006 (0.0070)	batch 0.3634 (0.3690)	loss 4.3409 (4.6746)	grad_norm 2.9930 (3.8406)	mem 22477MB
Train: [0/180][279/10009]	eta 0:59:50 lr 0.100000	data 0.0008 (0.0070)	batch 0.3635 (0.3690)	loss 4.1054 (4.6725)	grad_norm 2.9342 (3.8373)	mem 22477MB
Train: [0/180][280/10009]	eta 0:59:50 lr 0.100000	data 0.0007 (0.0070)	batch 0.3732 (0.3690)	loss 4.0516 (4.6703)	grad_norm 2.8038 (3.8337)	mem 22477MB
Train: [0/180][281/10009]	eta 0:59:49 lr 0.100000	data 0.0007 (0.0069)	batch 0.3517 (0.3690)	loss 4.2503 (4.6688)	grad_norm 2.9572 (3.8305)	mem 22477MB
Train: [0/180][282/10009]	eta 0:59:48 lr 0.100000	data 0.0007 (0.0069)	batch 0.3655 (0.3689)	loss 4.3531 (4.6677)	grad_norm 3.0020 (3.8276)	mem 22477MB
Train: [0/180][283/10009]	eta 0:59:47 lr 0.100000	data 0.0009 (0.0069)	batch 0.3529 (0.3689)	loss 4.3663 (4.6667)	grad_norm 3.0400 (3.8248)	mem 22477MB
Train: [0/180][284/10009]	eta 0:59:47 lr 0.100000	data 0.0005 (0.0069)	batch 0.3603 (0.3689)	loss 4.1125 (4.6647)	grad_norm 2.9224 (3.8217)	mem 22477MB
Train: [0/180][285/10009]	eta 0:59:46 lr 0.100000	data 0.0008 (0.0069)	batch 0.3589 (0.3688)	loss 4.1504 (4.6629)	grad_norm 3.0194 (3.8189)	mem 22477MB
Train: [0/180][286/10009]	eta 0:59:45 lr 0.100000	data 0.0007 (0.0068)	batch 0.3513 (0.3688)	loss 3.9592 (4.6605)	grad_norm 2.8999 (3.8157)	mem 22477MB
Train: [0/180][287/10009]	eta 0:59:45 lr 0.100000	data 0.0007 (0.0068)	batch 0.3768 (0.3688)	loss 4.3204 (4.6593)	grad_norm 2.9762 (3.8128)	mem 22477MB
Train: [0/180][288/10009]	eta 0:59:44 lr 0.100000	data 0.0007 (0.0068)	batch 0.3595 (0.3688)	loss 4.5280 (4.6588)	grad_norm 3.0183 (3.8100)	mem 22477MB
Train: [0/180][289/10009]	eta 0:59:43 lr 0.100000	data 0.0006 (0.0068)	batch 0.3542 (0.3687)	loss 4.1035 (4.6569)	grad_norm 2.9581 (3.8071)	mem 22477MB
Train: [0/180][290/10009]	eta 0:59:42 lr 0.100000	data 0.0006 (0.0068)	batch 0.3511 (0.3686)	loss 4.2581 (4.6555)	grad_norm 2.9981 (3.8043)	mem 22477MB
Train: [0/180][291/10009]	eta 0:59:41 lr 0.100000	data 0.0006 (0.0067)	batch 0.3507 (0.3686)	loss 4.6183 (4.6554)	grad_norm 3.1467 (3.8020)	mem 22477MB
Train: [0/180][292/10009]	eta 0:59:40 lr 0.100000	data 0.0006 (0.0067)	batch 0.3500 (0.3685)	loss 4.1154 (4.6536)	grad_norm 3.0035 (3.7993)	mem 22477MB
Train: [0/180][293/10009]	eta 0:59:40 lr 0.100000	data 0.0007 (0.0067)	batch 0.3581 (0.3685)	loss 4.1744 (4.6519)	grad_norm 3.0383 (3.7967)	mem 22477MB
Train: [0/180][294/10009]	eta 0:59:39 lr 0.100000	data 0.0007 (0.0067)	batch 0.3689 (0.3685)	loss 4.4628 (4.6513)	grad_norm 3.1331 (3.7945)	mem 22477MB
Train: [0/180][295/10009]	eta 0:59:39 lr 0.100000	data 0.0006 (0.0067)	batch 0.3563 (0.3684)	loss 3.9556 (4.6490)	grad_norm 2.8115 (3.7912)	mem 22477MB
Train: [0/180][296/10009]	eta 0:59:38 lr 0.100000	data 0.0005 (0.0066)	batch 0.3542 (0.3684)	loss 4.0743 (4.6470)	grad_norm 2.8304 (3.7879)	mem 22477MB
Train: [0/180][297/10009]	eta 0:59:37 lr 0.100000	data 0.0006 (0.0066)	batch 0.3526 (0.3683)	loss 4.4480 (4.6464)	grad_norm 2.9617 (3.7851)	mem 22477MB
Train: [0/180][298/10009]	eta 0:59:36 lr 0.100000	data 0.0007 (0.0066)	batch 0.3572 (0.3683)	loss 4.3039 (4.6452)	grad_norm 2.9697 (3.7824)	mem 22477MB
Train: [0/180][299/10009]	eta 0:59:35 lr 0.100000	data 0.0006 (0.0066)	batch 0.3561 (0.3683)	loss 4.2507 (4.6439)	grad_norm 3.0672 (3.7800)	mem 22477MB
Train: [0/180][300/10009]	eta 0:59:35 lr 0.100000	data 0.0008 (0.0066)	batch 0.3693 (0.3683)	loss 4.4575 (4.6433)	grad_norm 2.9571 (3.7773)	mem 22477MB
Train: [0/180][301/10009]	eta 0:59:35 lr 0.100000	data 0.0006 (0.0065)	batch 0.3654 (0.3683)	loss 4.3621 (4.6423)	grad_norm 3.0680 (3.7750)	mem 22477MB
Train: [0/180][302/10009]	eta 0:59:34 lr 0.100000	data 0.0008 (0.0065)	batch 0.3667 (0.3683)	loss 4.0646 (4.6404)	grad_norm 2.9630 (3.7723)	mem 22477MB
Train: [0/180][303/10009]	eta 0:59:34 lr 0.100000	data 0.0007 (0.0065)	batch 0.3623 (0.3682)	loss 4.1178 (4.6387)	grad_norm 2.9666 (3.7696)	mem 22477MB
Train: [0/180][304/10009]	eta 0:59:33 lr 0.100000	data 0.0007 (0.0065)	batch 0.3476 (0.3682)	loss 4.2174 (4.6373)	grad_norm 2.9646 (3.7670)	mem 22477MB
Train: [0/180][305/10009]	eta 0:59:32 lr 0.100000	data 0.0007 (0.0065)	batch 0.3626 (0.3682)	loss 4.2398 (4.6360)	grad_norm 3.0182 (3.7645)	mem 22477MB
Train: [0/180][306/10009]	eta 0:59:32 lr 0.100000	data 0.0006 (0.0064)	batch 0.3684 (0.3682)	loss 4.0932 (4.6343)	grad_norm 2.8885 (3.7617)	mem 22477MB
Train: [0/180][307/10009]	eta 0:59:31 lr 0.100000	data 0.0007 (0.0064)	batch 0.3729 (0.3682)	loss 4.5148 (4.6339)	grad_norm 2.9784 (3.7591)	mem 22477MB
Train: [0/180][308/10009]	eta 0:59:31 lr 0.100000	data 0.0007 (0.0064)	batch 0.3608 (0.3681)	loss 4.1526 (4.6323)	grad_norm 3.1682 (3.7572)	mem 22477MB
Train: [0/180][309/10009]	eta 0:59:30 lr 0.100000	data 0.0008 (0.0064)	batch 0.3620 (0.3681)	loss 4.4165 (4.6316)	grad_norm 3.0137 (3.7548)	mem 22477MB
Train: [0/180][310/10009]	eta 0:59:29 lr 0.100000	data 0.0006 (0.0064)	batch 0.3540 (0.3681)	loss 4.0511 (4.6298)	grad_norm 2.9588 (3.7523)	mem 22477MB
Train: [0/180][311/10009]	eta 0:59:29 lr 0.100000	data 0.0007 (0.0063)	batch 0.3509 (0.3680)	loss 4.4010 (4.6290)	grad_norm 3.0714 (3.7501)	mem 22477MB
Train: [0/180][312/10009]	eta 0:59:28 lr 0.100000	data 0.0006 (0.0063)	batch 0.3720 (0.3680)	loss 4.2911 (4.6279)	grad_norm 3.0792 (3.7479)	mem 22477MB
Train: [0/180][313/10009]	eta 0:59:28 lr 0.100000	data 0.0007 (0.0063)	batch 0.3642 (0.3680)	loss 4.6043 (4.6279)	grad_norm 2.9176 (3.7453)	mem 22477MB
Train: [0/180][314/10009]	eta 0:59:27 lr 0.100000	data 0.0005 (0.0063)	batch 0.3483 (0.3680)	loss 4.4000 (4.6271)	grad_norm 3.0148 (3.7430)	mem 22477MB
Train: [0/180][315/10009]	eta 0:59:26 lr 0.100000	data 0.0006 (0.0063)	batch 0.3600 (0.3679)	loss 4.4340 (4.6265)	grad_norm 2.9842 (3.7406)	mem 22477MB
Train: [0/180][316/10009]	eta 0:59:26 lr 0.100000	data 0.0006 (0.0063)	batch 0.3572 (0.3679)	loss 4.2197 (4.6253)	grad_norm 2.8881 (3.7379)	mem 22477MB
Train: [0/180][317/10009]	eta 0:59:25 lr 0.100000	data 0.0007 (0.0062)	batch 0.3568 (0.3679)	loss 4.3210 (4.6243)	grad_norm 2.9479 (3.7354)	mem 22477MB
Train: [0/180][318/10009]	eta 0:59:24 lr 0.100000	data 0.0006 (0.0062)	batch 0.3574 (0.3678)	loss 4.4110 (4.6236)	grad_norm 2.8320 (3.7326)	mem 22477MB
Train: [0/180][319/10009]	eta 0:59:24 lr 0.100000	data 0.0006 (0.0062)	batch 0.3694 (0.3678)	loss 4.4081 (4.6230)	grad_norm 2.9509 (3.7301)	mem 22477MB
Train: [0/180][320/10009]	eta 0:59:23 lr 0.100000	data 0.0008 (0.0062)	batch 0.3579 (0.3678)	loss 4.1517 (4.6215)	grad_norm 2.9182 (3.7276)	mem 22477MB
Train: [0/180][321/10009]	eta 0:59:22 lr 0.100000	data 0.0006 (0.0062)	batch 0.3513 (0.3678)	loss 4.5785 (4.6214)	grad_norm 2.9651 (3.7252)	mem 22477MB
Train: [0/180][322/10009]	eta 0:59:21 lr 0.100000	data 0.0005 (0.0062)	batch 0.3431 (0.3677)	loss 4.2895 (4.6203)	grad_norm 2.8393 (3.7225)	mem 22477MB
Train: [0/180][323/10009]	eta 0:59:20 lr 0.100000	data 0.0007 (0.0061)	batch 0.3483 (0.3676)	loss 4.5656 (4.6202)	grad_norm 2.9645 (3.7202)	mem 22477MB
Train: [0/180][324/10009]	eta 0:59:20 lr 0.100000	data 0.0006 (0.0061)	batch 0.3745 (0.3676)	loss 3.9112 (4.6180)	grad_norm 2.9402 (3.7178)	mem 22477MB
Train: [0/180][325/10009]	eta 0:59:19 lr 0.100000	data 0.0007 (0.0061)	batch 0.3543 (0.3676)	loss 4.3930 (4.6173)	grad_norm 2.9752 (3.7155)	mem 22477MB
Train: [0/180][326/10009]	eta 0:59:19 lr 0.100000	data 0.0006 (0.0061)	batch 0.3629 (0.3676)	loss 4.2081 (4.6160)	grad_norm 2.9174 (3.7130)	mem 22477MB
Train: [0/180][327/10009]	eta 0:59:19 lr 0.100000	data 0.0007 (0.0061)	batch 0.3708 (0.3676)	loss 4.1453 (4.6146)	grad_norm 2.8589 (3.7104)	mem 22477MB
Train: [0/180][328/10009]	eta 0:59:18 lr 0.100000	data 0.0006 (0.0061)	batch 0.3591 (0.3676)	loss 4.2672 (4.6135)	grad_norm 2.9560 (3.7081)	mem 22477MB
Train: [0/180][329/10009]	eta 0:59:18 lr 0.100000	data 0.0006 (0.0060)	batch 0.3729 (0.3676)	loss 4.2043 (4.6123)	grad_norm 2.9906 (3.7060)	mem 22477MB
Train: [0/180][330/10009]	eta 0:59:17 lr 0.100000	data 0.0006 (0.0060)	batch 0.3495 (0.3675)	loss 4.1717 (4.6110)	grad_norm 2.8757 (3.7035)	mem 22477MB
Train: [0/180][331/10009]	eta 0:59:16 lr 0.100000	data 0.0007 (0.0060)	batch 0.3583 (0.3675)	loss 4.2265 (4.6098)	grad_norm 2.9280 (3.7011)	mem 22477MB
Train: [0/180][332/10009]	eta 0:59:18 lr 0.100000	data 0.0006 (0.0060)	batch 0.4411 (0.3677)	loss 4.0838 (4.6082)	grad_norm 2.7790 (3.6983)	mem 22477MB
Train: [0/180][333/10009]	eta 0:59:18 lr 0.100000	data 0.0008 (0.0060)	batch 0.3729 (0.3677)	loss 4.5400 (4.6080)	grad_norm 2.9518 (3.6961)	mem 22477MB
Train: [0/180][334/10009]	eta 0:59:17 lr 0.100000	data 0.0006 (0.0060)	batch 0.3572 (0.3677)	loss 4.1865 (4.6068)	grad_norm 2.8832 (3.6937)	mem 22477MB
Train: [0/180][335/10009]	eta 0:59:16 lr 0.100000	data 0.0006 (0.0059)	batch 0.3456 (0.3676)	loss 4.3481 (4.6060)	grad_norm 2.8037 (3.6910)	mem 22477MB
Train: [0/180][336/10009]	eta 0:59:15 lr 0.100000	data 0.0005 (0.0059)	batch 0.3533 (0.3676)	loss 4.3223 (4.6052)	grad_norm 2.8308 (3.6885)	mem 22477MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::leaky_relu encountered 35 time(s)
Unsupported operator aten::sub encountered 35 time(s)
Unsupported operator aten::mul encountered 35 time(s)
Unsupported operator aten::add encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 5:26:34 lr 0.100000	data 3.1322 (3.1322)	batch 3.9157 (3.9157)	loss 1.8005 (1.8005)	grad_norm 1.0420 (1.0420)	mem 36228MB
Train: [0/180][1/5004]	eta 3:28:41 lr 0.100000	data 0.0008 (1.5665)	batch 1.0897 (2.5027)	loss 1.7120 (1.7563)	grad_norm 0.9332 (0.9876)	mem 36269MB
Train: [0/180][2/5004]	eta 2:34:07 lr 0.100000	data 0.0008 (1.0446)	batch 0.5406 (1.8487)	loss 1.8545 (1.7890)	grad_norm 0.9907 (0.9886)	mem 36269MB
Train: [0/180][3/5004]	eta 2:06:51 lr 0.100000	data 0.0006 (0.7836)	batch 0.5418 (1.5220)	loss 1.7349 (1.7755)	grad_norm 0.9986 (0.9911)	mem 36269MB
Train: [0/180][4/5004]	eta 1:50:47 lr 0.100000	data 0.0006 (0.6270)	batch 0.5593 (1.3294)	loss 1.7994 (1.7803)	grad_norm 1.0893 (1.0108)	mem 36269MB
Train: [0/180][5/5004]	eta 1:39:55 lr 0.100000	data 0.0006 (0.5226)	batch 0.5486 (1.1993)	loss 1.8605 (1.7936)	grad_norm 1.1394 (1.0322)	mem 36269MB
Train: [0/180][6/5004]	eta 1:32:17 lr 0.100000	data 0.0007 (0.4480)	batch 0.5604 (1.1080)	loss 1.9108 (1.8104)	grad_norm 1.2453 (1.0626)	mem 36269MB
Train: [0/180][7/5004]	eta 1:26:32 lr 0.100000	data 0.0006 (0.3921)	batch 0.5576 (1.0392)	loss 1.8914 (1.8205)	grad_norm 1.2780 (1.0896)	mem 36269MB
Train: [0/180][8/5004]	eta 1:22:05 lr 0.100000	data 0.0005 (0.3486)	batch 0.5585 (0.9858)	loss 1.8798 (1.8271)	grad_norm 1.0872 (1.0893)	mem 36269MB
Train: [0/180][9/5004]	eta 1:18:31 lr 0.100000	data 0.0005 (0.3138)	batch 0.5605 (0.9433)	loss 1.8248 (1.8269)	grad_norm 1.2172 (1.1021)	mem 36269MB
Train: [0/180][10/5004]	eta 1:15:39 lr 0.100000	data 0.0006 (0.2853)	batch 0.5671 (0.9091)	loss 1.8153 (1.8258)	grad_norm 1.2449 (1.1151)	mem 36269MB
Train: [0/180][11/5004]	eta 1:13:15 lr 0.100000	data 0.0007 (0.2616)	batch 0.5649 (0.8804)	loss 1.6657 (1.8125)	grad_norm 1.1628 (1.1190)	mem 36269MB
Train: [0/180][12/5004]	eta 1:11:10 lr 0.100000	data 0.0005 (0.2415)	batch 0.5566 (0.8555)	loss 1.7492 (1.8076)	grad_norm 1.1172 (1.1189)	mem 36269MB
Train: [0/180][13/5004]	eta 1:09:24 lr 0.100000	data 0.0005 (0.2243)	batch 0.5602 (0.8344)	loss 1.8687 (1.8120)	grad_norm 1.2372 (1.1274)	mem 36269MB
Train: [0/180][14/5004]	eta 1:07:54 lr 0.100000	data 0.0007 (0.2094)	batch 0.5664 (0.8165)	loss 1.9028 (1.8180)	grad_norm 1.2108 (1.1329)	mem 36269MB
Train: [0/180][15/5004]	eta 1:06:30 lr 0.100000	data 0.0007 (0.1963)	batch 0.5494 (0.7998)	loss 1.9859 (1.8285)	grad_norm 1.1743 (1.1355)	mem 36269MB
Train: [0/180][16/5004]	eta 1:05:19 lr 0.100000	data 0.0006 (0.1848)	batch 0.5601 (0.7857)	loss 1.7764 (1.8255)	grad_norm 1.2854 (1.1443)	mem 36269MB
Train: [0/180][17/5004]	eta 1:04:10 lr 0.100000	data 0.0006 (0.1746)	batch 0.5404 (0.7721)	loss 1.7908 (1.8235)	grad_norm 1.1182 (1.1429)	mem 36269MB
Train: [0/180][18/5004]	eta 1:03:15 lr 0.100000	data 0.0006 (0.1654)	batch 0.5641 (0.7611)	loss 2.1282 (1.8396)	grad_norm 1.3754 (1.1551)	mem 36269MB
Train: [0/180][19/5004]	eta 1:02:23 lr 0.100000	data 0.0008 (0.1572)	batch 0.5553 (0.7509)	loss 1.8394 (1.8396)	grad_norm 1.1724 (1.1560)	mem 36269MB
Train: [0/180][20/5004]	eta 1:01:40 lr 0.100000	data 0.0007 (0.1497)	batch 0.5729 (0.7424)	loss 1.9795 (1.8462)	grad_norm 1.3759 (1.1664)	mem 36269MB
Train: [0/180][21/5004]	eta 1:00:58 lr 0.100000	data 0.0007 (0.1430)	batch 0.5642 (0.7343)	loss 1.7407 (1.8414)	grad_norm 1.3263 (1.1737)	mem 36269MB
Train: [0/180][22/5004]	eta 1:00:21 lr 0.100000	data 0.0006 (0.1368)	batch 0.5632 (0.7268)	loss 1.7163 (1.8360)	grad_norm 1.2970 (1.1791)	mem 36269MB
Train: [0/180][23/5004]	eta 0:59:47 lr 0.100000	data 0.0007 (0.1311)	batch 0.5698 (0.7203)	loss 1.9183 (1.8394)	grad_norm 1.3630 (1.1867)	mem 36269MB
Train: [0/180][24/5004]	eta 0:59:13 lr 0.100000	data 0.0007 (0.1259)	batch 0.5503 (0.7135)	loss 2.0365 (1.8473)	grad_norm 1.3800 (1.1945)	mem 36269MB
Train: [0/180][25/5004]	eta 0:58:42 lr 0.100000	data 0.0008 (0.1211)	batch 0.5545 (0.7074)	loss 1.8868 (1.8488)	grad_norm 1.2564 (1.1968)	mem 36269MB
Train: [0/180][26/5004]	eta 0:58:15 lr 0.100000	data 0.0006 (0.1166)	batch 0.5654 (0.7021)	loss 1.7714 (1.8460)	grad_norm 1.3475 (1.2024)	mem 36269MB
Train: [0/180][27/5004]	eta 0:57:50 lr 0.100000	data 0.0007 (0.1125)	batch 0.5684 (0.6973)	loss 1.7773 (1.8435)	grad_norm 1.3996 (1.2095)	mem 36269MB
Train: [0/180][28/5004]	eta 0:57:27 lr 0.100000	data 0.0010 (0.1086)	batch 0.5655 (0.6928)	loss 1.9265 (1.8464)	grad_norm 1.5679 (1.2218)	mem 36269MB
Train: [0/180][29/5004]	eta 0:57:02 lr 0.100000	data 0.0007 (0.1050)	batch 0.5488 (0.6880)	loss 1.8157 (1.8453)	grad_norm 1.4024 (1.2278)	mem 36269MB
Train: [0/180][30/5004]	eta 0:56:42 lr 0.100000	data 0.0006 (0.1017)	batch 0.5679 (0.6841)	loss 2.0428 (1.8517)	grad_norm 1.5359 (1.2378)	mem 36269MB
Train: [0/180][31/5004]	eta 0:56:23 lr 0.100000	data 0.0008 (0.0985)	batch 0.5655 (0.6804)	loss 2.0129 (1.8567)	grad_norm 1.4092 (1.2431)	mem 36269MB
Train: [0/180][32/5004]	eta 0:56:05 lr 0.100000	data 0.0006 (0.0956)	batch 0.5631 (0.6769)	loss 1.9189 (1.8586)	grad_norm 1.4699 (1.2500)	mem 36269MB
Train: [0/180][33/5004]	eta 0:55:47 lr 0.100000	data 0.0006 (0.0928)	batch 0.5617 (0.6735)	loss 1.8896 (1.8595)	grad_norm 1.4532 (1.2560)	mem 36269MB
Train: [0/180][34/5004]	eta 0:55:32 lr 0.100000	data 0.0006 (0.0901)	batch 0.5693 (0.6705)	loss 2.0827 (1.8659)	grad_norm 1.5721 (1.2650)	mem 36269MB
Train: [0/180][35/5004]	eta 0:55:16 lr 0.100000	data 0.0007 (0.0876)	batch 0.5607 (0.6674)	loss 2.0386 (1.8707)	grad_norm 1.5747 (1.2736)	mem 36269MB
Train: [0/180][36/5004]	eta 0:55:02 lr 0.100000	data 0.0007 (0.0853)	batch 0.5651 (0.6647)	loss 2.0178 (1.8747)	grad_norm 1.4877 (1.2794)	mem 36269MB
Train: [0/180][37/5004]	eta 0:54:47 lr 0.100000	data 0.0006 (0.0831)	batch 0.5586 (0.6619)	loss 1.8129 (1.8731)	grad_norm 1.4082 (1.2828)	mem 36269MB
Train: [0/180][38/5004]	eta 0:54:32 lr 0.100000	data 0.0006 (0.0809)	batch 0.5464 (0.6589)	loss 1.9009 (1.8738)	grad_norm 1.3432 (1.2843)	mem 36269MB
Train: [0/180][39/5004]	eta 0:54:19 lr 0.100000	data 0.0007 (0.0789)	batch 0.5613 (0.6565)	loss 1.6802 (1.8689)	grad_norm 1.4664 (1.2889)	mem 36269MB
Train: [0/180][40/5004]	eta 0:54:08 lr 0.100000	data 0.0005 (0.0770)	batch 0.5674 (0.6543)	loss 2.1247 (1.8752)	grad_norm 1.5222 (1.2946)	mem 36269MB
Train: [0/180][41/5004]	eta 0:53:57 lr 0.100000	data 0.0008 (0.0752)	batch 0.5704 (0.6523)	loss 1.7914 (1.8732)	grad_norm 1.4273 (1.2977)	mem 36269MB
Train: [0/180][42/5004]	eta 0:53:45 lr 0.100000	data 0.0007 (0.0735)	batch 0.5533 (0.6500)	loss 1.8701 (1.8731)	grad_norm 1.4881 (1.3022)	mem 36269MB
Train: [0/180][43/5004]	eta 0:53:33 lr 0.100000	data 0.0007 (0.0718)	batch 0.5511 (0.6478)	loss 1.9921 (1.8758)	grad_norm 1.4826 (1.3063)	mem 36269MB
Train: [0/180][44/5004]	eta 0:53:23 lr 0.100000	data 0.0008 (0.0702)	batch 0.5587 (0.6458)	loss 1.8440 (1.8751)	grad_norm 1.7026 (1.3151)	mem 36269MB
Train: [0/180][45/5004]	eta 0:53:14 lr 0.100000	data 0.0007 (0.0687)	batch 0.5676 (0.6441)	loss 2.0993 (1.8800)	grad_norm 1.4626 (1.3183)	mem 36269MB
Train: [0/180][46/5004]	eta 0:53:04 lr 0.100000	data 0.0006 (0.0673)	batch 0.5581 (0.6423)	loss 1.7243 (1.8767)	grad_norm 1.5662 (1.3236)	mem 36269MB
Train: [0/180][47/5004]	eta 0:52:53 lr 0.100000	data 0.0006 (0.0659)	batch 0.5477 (0.6403)	loss 2.0611 (1.8805)	grad_norm 1.4933 (1.3271)	mem 36269MB
Train: [0/180][48/5004]	eta 0:52:45 lr 0.100000	data 0.0007 (0.0646)	batch 0.5663 (0.6388)	loss 1.9928 (1.8828)	grad_norm 1.4857 (1.3303)	mem 36269MB
Train: [0/180][49/5004]	eta 0:52:35 lr 0.100000	data 0.0007 (0.0633)	batch 0.5442 (0.6369)	loss 2.0332 (1.8858)	grad_norm 1.5775 (1.3353)	mem 36269MB
Train: [0/180][50/5004]	eta 0:52:28 lr 0.100000	data 0.0006 (0.0621)	batch 0.5633 (0.6354)	loss 1.9716 (1.8875)	grad_norm 1.5074 (1.3387)	mem 36269MB
Train: [0/180][51/5004]	eta 0:52:20 lr 0.100000	data 0.0006 (0.0609)	batch 0.5592 (0.6340)	loss 2.0165 (1.8900)	grad_norm 1.6125 (1.3439)	mem 36269MB
Train: [0/180][52/5004]	eta 0:52:11 lr 0.100000	data 0.0008 (0.0597)	batch 0.5530 (0.6325)	loss 1.8212 (1.8887)	grad_norm 1.6045 (1.3488)	mem 36269MB
Train: [0/180][53/5004]	eta 0:52:04 lr 0.100000	data 0.0006 (0.0586)	batch 0.5536 (0.6310)	loss 1.8517 (1.8880)	grad_norm 1.4041 (1.3499)	mem 36269MB
Train: [0/180][54/5004]	eta 0:51:57 lr 0.100000	data 0.0005 (0.0576)	batch 0.5659 (0.6298)	loss 2.1471 (1.8927)	grad_norm 1.5350 (1.3532)	mem 36269MB
Train: [0/180][55/5004]	eta 0:51:51 lr 0.100000	data 0.0007 (0.0566)	batch 0.5653 (0.6287)	loss 1.9531 (1.8938)	grad_norm 1.4758 (1.3554)	mem 36269MB
Train: [0/180][56/5004]	eta 0:51:43 lr 0.100000	data 0.0008 (0.0556)	batch 0.5421 (0.6271)	loss 1.9875 (1.8954)	grad_norm 1.6973 (1.3614)	mem 36269MB
Train: [0/180][57/5004]	eta 0:51:35 lr 0.100000	data 0.0006 (0.0546)	batch 0.5514 (0.6258)	loss 1.9954 (1.8971)	grad_norm 1.5334 (1.3644)	mem 36269MB
Train: [0/180][58/5004]	eta 0:51:29 lr 0.100000	data 0.0007 (0.0537)	batch 0.5591 (0.6247)	loss 2.0624 (1.8999)	grad_norm 1.6388 (1.3690)	mem 36269MB
Train: [0/180][59/5004]	eta 0:51:22 lr 0.100000	data 0.0006 (0.0528)	batch 0.5490 (0.6234)	loss 1.9201 (1.9003)	grad_norm 1.5608 (1.3722)	mem 36269MB
Train: [0/180][60/5004]	eta 0:51:17 lr 0.100000	data 0.0007 (0.0520)	batch 0.5606 (0.6224)	loss 2.1305 (1.9041)	grad_norm 1.4892 (1.3741)	mem 36269MB
Train: [0/180][61/5004]	eta 0:51:11 lr 0.100000	data 0.0006 (0.0512)	batch 0.5641 (0.6215)	loss 2.0441 (1.9063)	grad_norm 1.5722 (1.3773)	mem 36269MB
Train: [0/180][62/5004]	eta 0:51:05 lr 0.100000	data 0.0006 (0.0504)	batch 0.5449 (0.6203)	loss 1.9235 (1.9066)	grad_norm 1.4306 (1.3782)	mem 36269MB
Train: [0/180][63/5004]	eta 0:51:00 lr 0.100000	data 0.0005 (0.0496)	batch 0.5657 (0.6194)	loss 2.1993 (1.9112)	grad_norm 1.6381 (1.3822)	mem 36269MB
Train: [0/180][64/5004]	eta 0:50:54 lr 0.100000	data 0.0007 (0.0488)	batch 0.5496 (0.6183)	loss 1.8801 (1.9107)	grad_norm 1.6100 (1.3858)	mem 36269MB
Train: [0/180][65/5004]	eta 0:50:49 lr 0.100000	data 0.0007 (0.0481)	batch 0.5564 (0.6174)	loss 1.9821 (1.9118)	grad_norm 1.6767 (1.3902)	mem 36269MB
Train: [0/180][66/5004]	eta 0:50:43 lr 0.100000	data 0.0006 (0.0474)	batch 0.5433 (0.6163)	loss 1.9291 (1.9120)	grad_norm 1.6619 (1.3942)	mem 36269MB
Train: [0/180][67/5004]	eta 0:50:39 lr 0.100000	data 0.0006 (0.0467)	batch 0.5670 (0.6156)	loss 1.8109 (1.9105)	grad_norm 1.6985 (1.3987)	mem 36269MB
Train: [0/180][68/5004]	eta 0:50:34 lr 0.100000	data 0.0007 (0.0460)	batch 0.5614 (0.6148)	loss 1.8279 (1.9093)	grad_norm 1.5196 (1.4004)	mem 36269MB
Train: [0/180][69/5004]	eta 0:50:30 lr 0.100000	data 0.0006 (0.0454)	batch 0.5597 (0.6140)	loss 2.0772 (1.9117)	grad_norm 1.6220 (1.4036)	mem 36269MB
Train: [0/180][70/5004]	eta 0:50:25 lr 0.100000	data 0.0006 (0.0448)	batch 0.5586 (0.6132)	loss 1.9099 (1.9117)	grad_norm 1.6137 (1.4066)	mem 36269MB
Train: [0/180][71/5004]	eta 0:50:20 lr 0.100000	data 0.0006 (0.0441)	batch 0.5488 (0.6123)	loss 1.9173 (1.9118)	grad_norm 1.4718 (1.4075)	mem 36269MB
Train: [0/180][72/5004]	eta 0:50:16 lr 0.100000	data 0.0007 (0.0435)	batch 0.5686 (0.6117)	loss 2.0598 (1.9138)	grad_norm 1.7456 (1.4121)	mem 36269MB
Train: [0/180][73/5004]	eta 0:50:13 lr 0.100000	data 0.0006 (0.0430)	batch 0.5656 (0.6111)	loss 1.7998 (1.9123)	grad_norm 1.7083 (1.4161)	mem 36269MB
Train: [0/180][74/5004]	eta 0:50:09 lr 0.100000	data 0.0005 (0.0424)	batch 0.5648 (0.6105)	loss 2.1356 (1.9153)	grad_norm 1.6099 (1.4187)	mem 36269MB
Train: [0/180][75/5004]	eta 0:50:06 lr 0.100000	data 0.0006 (0.0419)	batch 0.5669 (0.6099)	loss 1.9100 (1.9152)	grad_norm 1.3999 (1.4184)	mem 36269MB
Train: [0/180][76/5004]	eta 0:50:02 lr 0.100000	data 0.0006 (0.0413)	batch 0.5556 (0.6092)	loss 2.0073 (1.9164)	grad_norm 1.5968 (1.4208)	mem 36269MB
Train: [0/180][77/5004]	eta 0:49:58 lr 0.100000	data 0.0007 (0.0408)	batch 0.5624 (0.6086)	loss 1.9062 (1.9162)	grad_norm 1.5857 (1.4229)	mem 36269MB
Train: [0/180][78/5004]	eta 0:49:54 lr 0.100000	data 0.0007 (0.0403)	batch 0.5614 (0.6080)	loss 1.9440 (1.9166)	grad_norm 1.5321 (1.4243)	mem 36269MB
Train: [0/180][79/5004]	eta 0:49:51 lr 0.100000	data 0.0007 (0.0398)	batch 0.5643 (0.6075)	loss 1.9658 (1.9172)	grad_norm 1.4040 (1.4240)	mem 36269MB
Train: [0/180][80/5004]	eta 0:49:47 lr 0.100000	data 0.0007 (0.0393)	batch 0.5432 (0.6067)	loss 1.9712 (1.9179)	grad_norm 1.4562 (1.4244)	mem 36269MB
Train: [0/180][81/5004]	eta 0:49:43 lr 0.100000	data 0.0006 (0.0388)	batch 0.5575 (0.6061)	loss 1.9981 (1.9189)	grad_norm 1.6547 (1.4272)	mem 36269MB
Train: [0/180][82/5004]	eta 0:49:39 lr 0.100000	data 0.0006 (0.0384)	batch 0.5492 (0.6054)	loss 1.9256 (1.9189)	grad_norm 1.5677 (1.4289)	mem 36269MB
Train: [0/180][83/5004]	eta 0:49:36 lr 0.100000	data 0.0006 (0.0379)	batch 0.5610 (0.6048)	loss 2.0278 (1.9202)	grad_norm 1.7094 (1.4322)	mem 36269MB
Train: [0/180][84/5004]	eta 0:49:33 lr 0.100000	data 0.0008 (0.0375)	batch 0.5719 (0.6045)	loss 2.1261 (1.9227)	grad_norm 1.6926 (1.4353)	mem 36269MB
Train: [0/180][85/5004]	eta 0:49:30 lr 0.100000	data 0.0009 (0.0371)	batch 0.5607 (0.6039)	loss 1.9137 (1.9226)	grad_norm 1.5801 (1.4370)	mem 36269MB
Train: [0/180][86/5004]	eta 0:49:27 lr 0.100000	data 0.0006 (0.0366)	batch 0.5626 (0.6035)	loss 2.0273 (1.9238)	grad_norm 1.5922 (1.4388)	mem 36269MB
Train: [0/180][87/5004]	eta 0:49:24 lr 0.100000	data 0.0009 (0.0362)	batch 0.5548 (0.6029)	loss 2.0193 (1.9248)	grad_norm 1.6071 (1.4407)	mem 36269MB
Train: [0/180][88/5004]	eta 0:49:21 lr 0.100000	data 0.0008 (0.0358)	batch 0.5605 (0.6024)	loss 1.9550 (1.9252)	grad_norm 1.5696 (1.4421)	mem 36269MB
Train: [0/180][89/5004]	eta 0:49:18 lr 0.100000	data 0.0008 (0.0355)	batch 0.5628 (0.6020)	loss 2.1375 (1.9275)	grad_norm 1.7962 (1.4461)	mem 36269MB
Train: [0/180][90/5004]	eta 0:49:16 lr 0.100000	data 0.0007 (0.0351)	batch 0.5635 (0.6016)	loss 2.1681 (1.9302)	grad_norm 1.6342 (1.4481)	mem 36269MB
Train: [0/180][91/5004]	eta 0:49:12 lr 0.100000	data 0.0008 (0.0347)	batch 0.5502 (0.6010)	loss 2.0603 (1.9316)	grad_norm 1.5828 (1.4496)	mem 36269MB
Train: [0/180][92/5004]	eta 0:49:10 lr 0.100000	data 0.0007 (0.0343)	batch 0.5635 (0.6006)	loss 2.1070 (1.9335)	grad_norm 1.5862 (1.4511)	mem 36269MB
Train: [0/180][93/5004]	eta 0:49:07 lr 0.100000	data 0.0007 (0.0340)	batch 0.5639 (0.6002)	loss 2.0155 (1.9344)	grad_norm 1.6970 (1.4537)	mem 36269MB
Train: [0/180][94/5004]	eta 0:49:05 lr 0.100000	data 0.0006 (0.0336)	batch 0.5670 (0.5999)	loss 1.9329 (1.9343)	grad_norm 1.8152 (1.4575)	mem 36269MB
Train: [0/180][95/5004]	eta 0:49:02 lr 0.100000	data 0.0007 (0.0333)	batch 0.5641 (0.5995)	loss 2.0383 (1.9354)	grad_norm 1.6939 (1.4599)	mem 36269MB
Train: [0/180][96/5004]	eta 0:49:00 lr 0.100000	data 0.0006 (0.0329)	batch 0.5637 (0.5991)	loss 2.0537 (1.9366)	grad_norm 1.5291 (1.4607)	mem 36269MB
Train: [0/180][97/5004]	eta 0:48:57 lr 0.100000	data 0.0006 (0.0326)	batch 0.5589 (0.5987)	loss 2.0760 (1.9381)	grad_norm 1.6401 (1.4625)	mem 36269MB
Train: [0/180][98/5004]	eta 0:48:54 lr 0.100000	data 0.0005 (0.0323)	batch 0.5410 (0.5981)	loss 2.2766 (1.9415)	grad_norm 1.6460 (1.4643)	mem 36269MB
Train: [0/180][99/5004]	eta 0:48:52 lr 0.100000	data 0.0006 (0.0320)	batch 0.5596 (0.5978)	loss 2.0406 (1.9425)	grad_norm 1.7166 (1.4669)	mem 36269MB
Train: [0/180][100/5004]	eta 0:48:48 lr 0.100000	data 0.0006 (0.0317)	batch 0.5476 (0.5973)	loss 2.0582 (1.9436)	grad_norm 1.5690 (1.4679)	mem 36269MB
Train: [0/180][101/5004]	eta 0:48:46 lr 0.100000	data 0.0007 (0.0314)	batch 0.5496 (0.5968)	loss 2.0398 (1.9446)	grad_norm 1.6232 (1.4694)	mem 36269MB
Train: [0/180][102/5004]	eta 0:48:43 lr 0.100000	data 0.0007 (0.0311)	batch 0.5553 (0.5964)	loss 2.0472 (1.9456)	grad_norm 1.6246 (1.4709)	mem 36269MB
Train: [0/180][103/5004]	eta 0:48:41 lr 0.100000	data 0.0006 (0.0308)	batch 0.5688 (0.5961)	loss 1.9740 (1.9458)	grad_norm 1.4988 (1.4712)	mem 36269MB
Train: [0/180][104/5004]	eta 0:48:39 lr 0.100000	data 0.0006 (0.0305)	batch 0.5612 (0.5958)	loss 2.1022 (1.9473)	grad_norm 1.7952 (1.4743)	mem 36269MB
Train: [0/180][105/5004]	eta 0:48:36 lr 0.100000	data 0.0006 (0.0302)	batch 0.5542 (0.5954)	loss 2.0955 (1.9487)	grad_norm 1.8231 (1.4776)	mem 36269MB
Train: [0/180][106/5004]	eta 0:48:34 lr 0.100000	data 0.0006 (0.0299)	batch 0.5555 (0.5950)	loss 2.1523 (1.9506)	grad_norm 1.6156 (1.4788)	mem 36269MB
Train: [0/180][107/5004]	eta 0:48:32 lr 0.100000	data 0.0007 (0.0297)	batch 0.5589 (0.5947)	loss 2.1505 (1.9525)	grad_norm 1.6359 (1.4803)	mem 36269MB
Train: [0/180][108/5004]	eta 0:48:30 lr 0.100000	data 0.0006 (0.0294)	batch 0.5629 (0.5944)	loss 2.2270 (1.9550)	grad_norm 1.7686 (1.4829)	mem 36269MB
Train: [0/180][109/5004]	eta 0:48:28 lr 0.100000	data 0.0007 (0.0291)	batch 0.5634 (0.5941)	loss 2.0898 (1.9562)	grad_norm 1.7274 (1.4852)	mem 36269MB
Train: [0/180][110/5004]	eta 0:48:25 lr 0.100000	data 0.0006 (0.0289)	batch 0.5407 (0.5936)	loss 2.3077 (1.9594)	grad_norm 1.8674 (1.4886)	mem 36269MB
Train: [0/180][111/5004]	eta 0:48:23 lr 0.100000	data 0.0006 (0.0286)	batch 0.5569 (0.5933)	loss 2.0759 (1.9604)	grad_norm 1.6263 (1.4898)	mem 36269MB
Train: [0/180][112/5004]	eta 0:48:21 lr 0.100000	data 0.0006 (0.0284)	batch 0.5702 (0.5931)	loss 2.3383 (1.9638)	grad_norm 1.6111 (1.4909)	mem 36269MB
Train: [0/180][113/5004]	eta 0:48:19 lr 0.100000	data 0.0006 (0.0281)	batch 0.5586 (0.5928)	loss 2.2174 (1.9660)	grad_norm 1.7440 (1.4931)	mem 36269MB
Train: [0/180][114/5004]	eta 0:48:17 lr 0.100000	data 0.0006 (0.0279)	batch 0.5608 (0.5925)	loss 2.1712 (1.9678)	grad_norm 1.6573 (1.4946)	mem 36269MB
Train: [0/180][115/5004]	eta 0:48:15 lr 0.100000	data 0.0007 (0.0276)	batch 0.5523 (0.5922)	loss 2.2039 (1.9698)	grad_norm 1.7353 (1.4966)	mem 36269MB
Train: [0/180][116/5004]	eta 0:48:12 lr 0.100000	data 0.0006 (0.0274)	batch 0.5479 (0.5918)	loss 2.1206 (1.9711)	grad_norm 1.7450 (1.4988)	mem 36269MB
Train: [0/180][117/5004]	eta 0:48:10 lr 0.100000	data 0.0006 (0.0272)	batch 0.5521 (0.5915)	loss 2.1165 (1.9723)	grad_norm 1.7221 (1.5007)	mem 36269MB
Train: [0/180][118/5004]	eta 0:48:08 lr 0.100000	data 0.0006 (0.0270)	batch 0.5604 (0.5912)	loss 2.0873 (1.9733)	grad_norm 1.6255 (1.5017)	mem 36269MB
Train: [0/180][119/5004]	eta 0:48:06 lr 0.100000	data 0.0007 (0.0268)	batch 0.5641 (0.5910)	loss 2.1583 (1.9748)	grad_norm 1.7924 (1.5041)	mem 36269MB
Train: [0/180][120/5004]	eta 0:48:04 lr 0.100000	data 0.0008 (0.0265)	batch 0.5468 (0.5906)	loss 2.1607 (1.9764)	grad_norm 1.7439 (1.5061)	mem 36269MB
Train: [0/180][121/5004]	eta 0:48:02 lr 0.100000	data 0.0007 (0.0263)	batch 0.5432 (0.5902)	loss 2.1123 (1.9775)	grad_norm 1.7394 (1.5080)	mem 36269MB
Train: [0/180][122/5004]	eta 0:47:59 lr 0.100000	data 0.0006 (0.0261)	batch 0.5481 (0.5899)	loss 1.8610 (1.9765)	grad_norm 1.5359 (1.5082)	mem 36269MB
Train: [0/180][123/5004]	eta 0:47:58 lr 0.100000	data 0.0006 (0.0259)	batch 0.5653 (0.5897)	loss 2.0929 (1.9775)	grad_norm 1.6019 (1.5090)	mem 36269MB
Train: [0/180][124/5004]	eta 0:47:56 lr 0.100000	data 0.0006 (0.0257)	batch 0.5602 (0.5894)	loss 2.1864 (1.9792)	grad_norm 1.6457 (1.5101)	mem 36269MB
Train: [0/180][125/5004]	eta 0:47:54 lr 0.100000	data 0.0007 (0.0255)	batch 0.5440 (0.5891)	loss 2.2345 (1.9812)	grad_norm 1.8070 (1.5125)	mem 36269MB
Train: [0/180][126/5004]	eta 0:47:52 lr 0.100000	data 0.0006 (0.0253)	batch 0.5619 (0.5889)	loss 2.1072 (1.9822)	grad_norm 1.7809 (1.5146)	mem 36269MB
Train: [0/180][127/5004]	eta 0:47:50 lr 0.100000	data 0.0005 (0.0251)	batch 0.5599 (0.5886)	loss 2.3248 (1.9849)	grad_norm 1.6087 (1.5153)	mem 36269MB
Train: [0/180][128/5004]	eta 0:47:48 lr 0.100000	data 0.0006 (0.0249)	batch 0.5486 (0.5883)	loss 2.1378 (1.9860)	grad_norm 1.8959 (1.5182)	mem 36269MB
Train: [0/180][129/5004]	eta 0:47:47 lr 0.100000	data 0.0006 (0.0247)	batch 0.5609 (0.5881)	loss 1.8564 (1.9850)	grad_norm 1.7244 (1.5198)	mem 36269MB
Train: [0/180][130/5004]	eta 0:47:46 lr 0.100000	data 0.0006 (0.0246)	batch 0.5812 (0.5881)	loss 2.0532 (1.9856)	grad_norm 1.6029 (1.5205)	mem 36269MB
Train: [0/180][131/5004]	eta 0:47:44 lr 0.100000	data 0.0007 (0.0244)	batch 0.5549 (0.5878)	loss 1.9638 (1.9854)	grad_norm 1.6935 (1.5218)	mem 36269MB
Train: [0/180][132/5004]	eta 0:47:42 lr 0.100000	data 0.0006 (0.0242)	batch 0.5481 (0.5875)	loss 2.0497 (1.9859)	grad_norm 1.6783 (1.5230)	mem 36269MB
Train: [0/180][133/5004]	eta 0:47:40 lr 0.100000	data 0.0007 (0.0240)	batch 0.5488 (0.5872)	loss 2.1877 (1.9874)	grad_norm 1.7195 (1.5244)	mem 36269MB
Train: [0/180][134/5004]	eta 0:47:38 lr 0.100000	data 0.0006 (0.0239)	batch 0.5553 (0.5870)	loss 2.1287 (1.9884)	grad_norm 1.6438 (1.5253)	mem 36269MB
Train: [0/180][135/5004]	eta 0:47:36 lr 0.100000	data 0.0007 (0.0237)	batch 0.5450 (0.5867)	loss 2.1063 (1.9893)	grad_norm 1.7386 (1.5269)	mem 36269MB
Train: [0/180][136/5004]	eta 0:47:35 lr 0.100000	data 0.0007 (0.0235)	batch 0.5599 (0.5865)	loss 2.1986 (1.9908)	grad_norm 1.6357 (1.5277)	mem 36269MB
Train: [0/180][137/5004]	eta 0:47:33 lr 0.100000	data 0.0007 (0.0233)	batch 0.5679 (0.5864)	loss 2.0923 (1.9916)	grad_norm 1.7954 (1.5296)	mem 36269MB
Train: [0/180][138/5004]	eta 0:47:32 lr 0.100000	data 0.0006 (0.0232)	batch 0.5637 (0.5862)	loss 2.0683 (1.9921)	grad_norm 1.7363 (1.5311)	mem 36269MB
Train: [0/180][139/5004]	eta 0:47:30 lr 0.100000	data 0.0007 (0.0230)	batch 0.5484 (0.5859)	loss 1.9949 (1.9921)	grad_norm 1.6494 (1.5319)	mem 36269MB
Train: [0/180][140/5004]	eta 0:47:29 lr 0.100000	data 0.0005 (0.0229)	batch 0.5627 (0.5858)	loss 2.2140 (1.9937)	grad_norm 1.8113 (1.5339)	mem 36269MB
Train: [0/180][141/5004]	eta 0:47:27 lr 0.100000	data 0.0006 (0.0227)	batch 0.5522 (0.5855)	loss 2.2085 (1.9952)	grad_norm 1.6332 (1.5346)	mem 36269MB
Train: [0/180][142/5004]	eta 0:47:26 lr 0.100000	data 0.0008 (0.0226)	batch 0.5653 (0.5854)	loss 2.0307 (1.9955)	grad_norm 1.5826 (1.5350)	mem 36269MB
Train: [0/180][143/5004]	eta 0:47:24 lr 0.100000	data 0.0007 (0.0224)	batch 0.5661 (0.5852)	loss 2.0328 (1.9957)	grad_norm 1.6416 (1.5357)	mem 36269MB
Train: [0/180][144/5004]	eta 0:47:23 lr 0.100000	data 0.0005 (0.0222)	batch 0.5692 (0.5851)	loss 2.3211 (1.9980)	grad_norm 1.7205 (1.5370)	mem 36269MB
Train: [0/180][145/5004]	eta 0:47:22 lr 0.100000	data 0.0008 (0.0221)	batch 0.5674 (0.5850)	loss 1.9029 (1.9973)	grad_norm 1.7457 (1.5384)	mem 36269MB
Train: [0/180][146/5004]	eta 0:47:21 lr 0.100000	data 0.0006 (0.0220)	batch 0.5580 (0.5848)	loss 2.1715 (1.9985)	grad_norm 1.7776 (1.5400)	mem 36269MB
Train: [0/180][147/5004]	eta 0:47:19 lr 0.100000	data 0.0006 (0.0218)	batch 0.5572 (0.5846)	loss 2.3514 (2.0009)	grad_norm 1.7155 (1.5412)	mem 36269MB
Train: [0/180][148/5004]	eta 0:47:18 lr 0.100000	data 0.0006 (0.0217)	batch 0.5623 (0.5845)	loss 2.1703 (2.0020)	grad_norm 1.7724 (1.5428)	mem 36269MB
Train: [0/180][149/5004]	eta 0:47:16 lr 0.100000	data 0.0007 (0.0215)	batch 0.5582 (0.5843)	loss 2.2257 (2.0035)	grad_norm 1.7979 (1.5445)	mem 36269MB
Train: [0/180][150/5004]	eta 0:47:15 lr 0.100000	data 0.0006 (0.0214)	batch 0.5643 (0.5842)	loss 2.1574 (2.0045)	grad_norm 1.7417 (1.5458)	mem 36269MB
Train: [0/180][151/5004]	eta 0:47:14 lr 0.100000	data 0.0006 (0.0213)	batch 0.5535 (0.5840)	loss 2.2708 (2.0063)	grad_norm 1.6470 (1.5464)	mem 36269MB
Train: [0/180][152/5004]	eta 0:47:12 lr 0.100000	data 0.0006 (0.0211)	batch 0.5498 (0.5838)	loss 2.1628 (2.0073)	grad_norm 1.8407 (1.5484)	mem 36269MB
Train: [0/180][153/5004]	eta 0:47:10 lr 0.100000	data 0.0006 (0.0210)	batch 0.5498 (0.5835)	loss 1.9522 (2.0070)	grad_norm 1.6179 (1.5488)	mem 36269MB
Train: [0/180][154/5004]	eta 0:47:09 lr 0.100000	data 0.0006 (0.0209)	batch 0.5474 (0.5833)	loss 1.9334 (2.0065)	grad_norm 1.5480 (1.5488)	mem 36269MB
Train: [0/180][155/5004]	eta 0:47:07 lr 0.100000	data 0.0007 (0.0207)	batch 0.5621 (0.5832)	loss 2.2597 (2.0081)	grad_norm 1.6566 (1.5495)	mem 36269MB
Train: [0/180][156/5004]	eta 0:47:06 lr 0.100000	data 0.0006 (0.0206)	batch 0.5641 (0.5830)	loss 2.1522 (2.0090)	grad_norm 1.8058 (1.5511)	mem 36269MB
Train: [0/180][157/5004]	eta 0:47:04 lr 0.100000	data 0.0007 (0.0205)	batch 0.5464 (0.5828)	loss 2.0598 (2.0093)	grad_norm 1.5984 (1.5514)	mem 36269MB
Train: [0/180][158/5004]	eta 0:47:03 lr 0.100000	data 0.0006 (0.0203)	batch 0.5643 (0.5827)	loss 2.3254 (2.0113)	grad_norm 1.8679 (1.5534)	mem 36269MB
Train: [0/180][159/5004]	eta 0:47:02 lr 0.100000	data 0.0006 (0.0202)	batch 0.5645 (0.5826)	loss 2.0814 (2.0118)	grad_norm 1.7018 (1.5544)	mem 36269MB
Train: [0/180][160/5004]	eta 0:47:01 lr 0.100000	data 0.0008 (0.0201)	batch 0.5608 (0.5825)	loss 1.9765 (2.0115)	grad_norm 1.6171 (1.5547)	mem 36269MB
Train: [0/180][161/5004]	eta 0:46:59 lr 0.100000	data 0.0006 (0.0200)	batch 0.5416 (0.5822)	loss 2.1569 (2.0124)	grad_norm 1.6809 (1.5555)	mem 36269MB
Train: [0/180][162/5004]	eta 0:46:58 lr 0.100000	data 0.0006 (0.0199)	batch 0.5606 (0.5821)	loss 2.0517 (2.0127)	grad_norm 1.6126 (1.5559)	mem 36269MB
Train: [0/180][163/5004]	eta 0:46:57 lr 0.100000	data 0.0007 (0.0197)	batch 0.5679 (0.5820)	loss 2.1849 (2.0137)	grad_norm 1.6560 (1.5565)	mem 36269MB
Train: [0/180][164/5004]	eta 0:46:56 lr 0.100000	data 0.0007 (0.0196)	batch 0.5591 (0.5818)	loss 2.0771 (2.0141)	grad_norm 1.7574 (1.5577)	mem 36269MB
Train: [0/180][165/5004]	eta 0:46:54 lr 0.100000	data 0.0007 (0.0195)	batch 0.5617 (0.5817)	loss 2.1838 (2.0151)	grad_norm 1.7361 (1.5588)	mem 36269MB
Train: [0/180][166/5004]	eta 0:46:53 lr 0.100000	data 0.0007 (0.0194)	batch 0.5627 (0.5816)	loss 2.1762 (2.0161)	grad_norm 1.8242 (1.5604)	mem 36269MB
Train: [0/180][167/5004]	eta 0:46:52 lr 0.100000	data 0.0008 (0.0193)	batch 0.5442 (0.5814)	loss 2.1849 (2.0171)	grad_norm 1.7972 (1.5618)	mem 36269MB
Train: [0/180][168/5004]	eta 0:46:51 lr 0.100000	data 0.0006 (0.0192)	batch 0.5651 (0.5813)	loss 2.2572 (2.0185)	grad_norm 1.7376 (1.5628)	mem 36269MB
Train: [0/180][169/5004]	eta 0:46:50 lr 0.100000	data 0.0007 (0.0191)	batch 0.5663 (0.5812)	loss 2.2258 (2.0198)	grad_norm 1.6779 (1.5635)	mem 36269MB
Train: [0/180][170/5004]	eta 0:46:48 lr 0.100000	data 0.0006 (0.0190)	batch 0.5433 (0.5810)	loss 2.0041 (2.0197)	grad_norm 1.5540 (1.5634)	mem 36269MB
Train: [0/180][171/5004]	eta 0:46:47 lr 0.100000	data 0.0007 (0.0189)	batch 0.5672 (0.5809)	loss 1.9428 (2.0192)	grad_norm 1.7144 (1.5643)	mem 36269MB
Train: [0/180][172/5004]	eta 0:46:45 lr 0.100000	data 0.0010 (0.0188)	batch 0.5470 (0.5807)	loss 2.1387 (2.0199)	grad_norm 1.7536 (1.5654)	mem 36269MB
Train: [0/180][173/5004]	eta 0:46:44 lr 0.100000	data 0.0007 (0.0187)	batch 0.5653 (0.5806)	loss 2.2814 (2.0214)	grad_norm 1.7347 (1.5664)	mem 36269MB
Train: [0/180][174/5004]	eta 0:46:43 lr 0.100000	data 0.0009 (0.0186)	batch 0.5643 (0.5805)	loss 2.2711 (2.0228)	grad_norm 1.6950 (1.5671)	mem 36269MB
Train: [0/180][175/5004]	eta 0:46:43 lr 0.100000	data 0.0008 (0.0185)	batch 0.5701 (0.5805)	loss 2.2090 (2.0239)	grad_norm 1.7669 (1.5682)	mem 36269MB
Train: [0/180][176/5004]	eta 0:46:42 lr 0.100000	data 0.0006 (0.0184)	batch 0.5640 (0.5804)	loss 2.3655 (2.0258)	grad_norm 1.8002 (1.5696)	mem 36269MB
Train: [0/180][177/5004]	eta 0:46:40 lr 0.100000	data 0.0007 (0.0183)	batch 0.5605 (0.5803)	loss 2.2424 (2.0270)	grad_norm 1.8437 (1.5711)	mem 36269MB
Train: [0/180][178/5004]	eta 0:46:39 lr 0.100000	data 0.0007 (0.0182)	batch 0.5655 (0.5802)	loss 2.3206 (2.0287)	grad_norm 1.6929 (1.5718)	mem 36269MB
Train: [0/180][179/5004]	eta 0:46:38 lr 0.100000	data 0.0007 (0.0181)	batch 0.5629 (0.5801)	loss 2.1510 (2.0294)	grad_norm 1.6938 (1.5725)	mem 36269MB
Train: [0/180][180/5004]	eta 0:46:37 lr 0.100000	data 0.0007 (0.0180)	batch 0.5570 (0.5799)	loss 2.1383 (2.0300)	grad_norm 1.5855 (1.5725)	mem 36269MB
Train: [0/180][181/5004]	eta 0:46:36 lr 0.100000	data 0.0007 (0.0179)	batch 0.5541 (0.5798)	loss 2.2532 (2.0312)	grad_norm 1.8677 (1.5741)	mem 36269MB
Train: [0/180][182/5004]	eta 0:46:35 lr 0.100000	data 0.0007 (0.0178)	batch 0.5641 (0.5797)	loss 2.1866 (2.0320)	grad_norm 1.8128 (1.5755)	mem 36269MB
Train: [0/180][183/5004]	eta 0:46:34 lr 0.100000	data 0.0008 (0.0177)	batch 0.5537 (0.5796)	loss 2.1500 (2.0327)	grad_norm 1.5302 (1.5752)	mem 36269MB
Train: [0/180][184/5004]	eta 0:46:33 lr 0.100000	data 0.0006 (0.0176)	batch 0.5734 (0.5795)	loss 2.0719 (2.0329)	grad_norm 1.9585 (1.5773)	mem 36269MB
Train: [0/180][185/5004]	eta 0:46:32 lr 0.100000	data 0.0007 (0.0175)	batch 0.5474 (0.5794)	loss 2.0347 (2.0329)	grad_norm 1.8977 (1.5790)	mem 36269MB
Train: [0/180][186/5004]	eta 0:46:30 lr 0.100000	data 0.0007 (0.0174)	batch 0.5616 (0.5793)	loss 2.2731 (2.0342)	grad_norm 1.6557 (1.5794)	mem 36269MB
Train: [0/180][187/5004]	eta 0:46:29 lr 0.100000	data 0.0007 (0.0173)	batch 0.5498 (0.5791)	loss 2.2140 (2.0351)	grad_norm 1.7216 (1.5802)	mem 36269MB
Train: [0/180][188/5004]	eta 0:46:28 lr 0.100000	data 0.0006 (0.0172)	batch 0.5642 (0.5790)	loss 2.2835 (2.0365)	grad_norm 1.6952 (1.5808)	mem 36269MB
Train: [0/180][189/5004]	eta 0:46:27 lr 0.100000	data 0.0006 (0.0171)	batch 0.5521 (0.5789)	loss 2.0050 (2.0363)	grad_norm 1.5325 (1.5805)	mem 36269MB
Train: [0/180][190/5004]	eta 0:46:26 lr 0.100000	data 0.0006 (0.0171)	batch 0.5553 (0.5788)	loss 2.1534 (2.0369)	grad_norm 1.6809 (1.5810)	mem 36269MB
Train: [0/180][191/5004]	eta 0:46:25 lr 0.100000	data 0.0007 (0.0170)	batch 0.5634 (0.5787)	loss 2.0478 (2.0370)	grad_norm 1.8624 (1.5825)	mem 36269MB
Train: [0/180][192/5004]	eta 0:46:24 lr 0.100000	data 0.0006 (0.0169)	batch 0.5592 (0.5786)	loss 2.2136 (2.0379)	grad_norm 1.7756 (1.5835)	mem 36269MB
Train: [0/180][193/5004]	eta 0:46:23 lr 0.100000	data 0.0006 (0.0168)	batch 0.5660 (0.5785)	loss 2.1764 (2.0386)	grad_norm 1.7486 (1.5844)	mem 36269MB
Train: [0/180][194/5004]	eta 0:46:22 lr 0.100000	data 0.0006 (0.0167)	batch 0.5639 (0.5785)	loss 2.2798 (2.0398)	grad_norm 1.7575 (1.5853)	mem 36269MB
Train: [0/180][195/5004]	eta 0:46:21 lr 0.100000	data 0.0006 (0.0166)	batch 0.5684 (0.5784)	loss 2.0966 (2.0401)	grad_norm 1.7261 (1.5860)	mem 36269MB
Train: [0/180][196/5004]	eta 0:46:20 lr 0.100000	data 0.0007 (0.0166)	batch 0.5639 (0.5783)	loss 2.1878 (2.0409)	grad_norm 1.6750 (1.5864)	mem 36269MB
Train: [0/180][197/5004]	eta 0:46:19 lr 0.100000	data 0.0006 (0.0165)	batch 0.5603 (0.5782)	loss 2.1391 (2.0414)	grad_norm 1.6586 (1.5868)	mem 36269MB
Train: [0/180][198/5004]	eta 0:46:18 lr 0.100000	data 0.0007 (0.0164)	batch 0.5653 (0.5782)	loss 2.2516 (2.0424)	grad_norm 1.7903 (1.5878)	mem 36269MB
Train: [0/180][199/5004]	eta 0:46:17 lr 0.100000	data 0.0007 (0.0163)	batch 0.5483 (0.5780)	loss 2.4302 (2.0444)	grad_norm 1.7103 (1.5884)	mem 36269MB
Train: [0/180][200/5004]	eta 0:46:16 lr 0.100000	data 0.0007 (0.0162)	batch 0.5706 (0.5780)	loss 2.2175 (2.0452)	grad_norm 1.7256 (1.5891)	mem 36269MB
Train: [0/180][201/5004]	eta 0:46:15 lr 0.100000	data 0.0007 (0.0162)	batch 0.5596 (0.5779)	loss 2.2257 (2.0461)	grad_norm 1.6439 (1.5894)	mem 36269MB
Train: [0/180][202/5004]	eta 0:46:14 lr 0.100000	data 0.0006 (0.0161)	batch 0.5470 (0.5777)	loss 2.1584 (2.0467)	grad_norm 1.7950 (1.5904)	mem 36269MB
Train: [0/180][203/5004]	eta 0:46:13 lr 0.100000	data 0.0006 (0.0160)	batch 0.5708 (0.5777)	loss 2.3085 (2.0479)	grad_norm 1.7619 (1.5912)	mem 36269MB
Train: [0/180][204/5004]	eta 0:46:12 lr 0.100000	data 0.0007 (0.0159)	batch 0.5462 (0.5776)	loss 2.1401 (2.0484)	grad_norm 1.8154 (1.5923)	mem 36269MB
Train: [0/180][205/5004]	eta 0:46:11 lr 0.100000	data 0.0007 (0.0159)	batch 0.5614 (0.5775)	loss 2.0743 (2.0485)	grad_norm 1.8062 (1.5934)	mem 36269MB
Train: [0/180][206/5004]	eta 0:46:10 lr 0.100000	data 0.0007 (0.0158)	batch 0.5745 (0.5775)	loss 2.1960 (2.0492)	grad_norm 1.5949 (1.5934)	mem 36269MB
Train: [0/180][207/5004]	eta 0:46:09 lr 0.100000	data 0.0007 (0.0157)	batch 0.5658 (0.5774)	loss 2.3029 (2.0505)	grad_norm 1.7916 (1.5943)	mem 36269MB
Train: [0/180][208/5004]	eta 0:46:08 lr 0.100000	data 0.0006 (0.0156)	batch 0.5629 (0.5773)	loss 2.1616 (2.0510)	grad_norm 1.9396 (1.5960)	mem 36269MB
Train: [0/180][209/5004]	eta 0:46:08 lr 0.100000	data 0.0006 (0.0156)	batch 0.5637 (0.5773)	loss 2.1800 (2.0516)	grad_norm 1.7023 (1.5965)	mem 36269MB
Train: [0/180][210/5004]	eta 0:46:06 lr 0.100000	data 0.0007 (0.0155)	batch 0.5461 (0.5771)	loss 2.2059 (2.0523)	grad_norm 1.6347 (1.5967)	mem 36269MB
Train: [0/180][211/5004]	eta 0:46:05 lr 0.100000	data 0.0006 (0.0154)	batch 0.5679 (0.5771)	loss 2.1229 (2.0527)	grad_norm 1.7169 (1.5972)	mem 36269MB
Train: [0/180][212/5004]	eta 0:46:04 lr 0.100000	data 0.0006 (0.0154)	batch 0.5424 (0.5769)	loss 2.0178 (2.0525)	grad_norm 1.6162 (1.5973)	mem 36269MB
Train: [0/180][213/5004]	eta 0:46:03 lr 0.100000	data 0.0007 (0.0153)	batch 0.5648 (0.5769)	loss 2.2247 (2.0533)	grad_norm 1.7964 (1.5982)	mem 36269MB
Train: [0/180][214/5004]	eta 0:46:04 lr 0.100000	data 0.0006 (0.0152)	batch 0.6429 (0.5772)	loss 2.1233 (2.0536)	grad_norm 1.9302 (1.5998)	mem 36269MB
Train: [0/180][215/5004]	eta 0:46:03 lr 0.100000	data 0.0007 (0.0152)	batch 0.5674 (0.5771)	loss 2.1109 (2.0539)	grad_norm 1.7460 (1.6005)	mem 36269MB
Train: [0/180][216/5004]	eta 0:46:02 lr 0.100000	data 0.0006 (0.0151)	batch 0.5483 (0.5770)	loss 2.0897 (2.0541)	grad_norm 1.4684 (1.5999)	mem 36269MB
Train: [0/180][217/5004]	eta 0:46:01 lr 0.100000	data 0.0006 (0.0150)	batch 0.5503 (0.5769)	loss 2.0738 (2.0542)	grad_norm 1.6883 (1.6003)	mem 36269MB
Train: [0/180][218/5004]	eta 0:46:00 lr 0.100000	data 0.0007 (0.0150)	batch 0.5645 (0.5768)	loss 2.1936 (2.0548)	grad_norm 1.6485 (1.6005)	mem 36269MB
Train: [0/180][219/5004]	eta 0:45:59 lr 0.100000	data 0.0006 (0.0149)	batch 0.5625 (0.5768)	loss 2.2280 (2.0556)	grad_norm 1.5506 (1.6003)	mem 36269MB
Train: [0/180][220/5004]	eta 0:45:58 lr 0.100000	data 0.0006 (0.0148)	batch 0.5497 (0.5766)	loss 2.0553 (2.0556)	grad_norm 1.7309 (1.6009)	mem 36269MB
Train: [0/180][221/5004]	eta 0:45:57 lr 0.100000	data 0.0007 (0.0148)	batch 0.5593 (0.5765)	loss 2.1773 (2.0561)	grad_norm 1.8822 (1.6021)	mem 36269MB
Train: [0/180][222/5004]	eta 0:45:56 lr 0.100000	data 0.0005 (0.0147)	batch 0.5602 (0.5765)	loss 2.1858 (2.0567)	grad_norm 1.7187 (1.6026)	mem 36269MB
Train: [0/180][223/5004]	eta 0:45:55 lr 0.100000	data 0.0008 (0.0146)	batch 0.5600 (0.5764)	loss 2.0682 (2.0568)	grad_norm 1.6200 (1.6027)	mem 36269MB
Train: [0/180][224/5004]	eta 0:45:55 lr 0.100000	data 0.0006 (0.0146)	batch 0.5707 (0.5764)	loss 1.9568 (2.0563)	grad_norm 1.5719 (1.6026)	mem 36269MB
Train: [0/180][225/5004]	eta 0:45:54 lr 0.100000	data 0.0008 (0.0145)	batch 0.5591 (0.5763)	loss 2.0798 (2.0564)	grad_norm 1.7979 (1.6034)	mem 36269MB
Train: [0/180][226/5004]	eta 0:45:53 lr 0.100000	data 0.0006 (0.0145)	batch 0.5642 (0.5762)	loss 2.2313 (2.0572)	grad_norm 1.6338 (1.6036)	mem 36269MB
Train: [0/180][227/5004]	eta 0:45:52 lr 0.100000	data 0.0007 (0.0144)	batch 0.5529 (0.5761)	loss 2.0439 (2.0571)	grad_norm 1.6339 (1.6037)	mem 36269MB
Train: [0/180][228/5004]	eta 0:45:51 lr 0.100000	data 0.0006 (0.0143)	batch 0.5464 (0.5760)	loss 2.1069 (2.0573)	grad_norm 1.6871 (1.6041)	mem 36269MB
Train: [0/180][229/5004]	eta 0:45:50 lr 0.100000	data 0.0006 (0.0143)	batch 0.5618 (0.5760)	loss 2.1098 (2.0576)	grad_norm 1.7344 (1.6046)	mem 36269MB
Train: [0/180][230/5004]	eta 0:45:49 lr 0.100000	data 0.0007 (0.0142)	batch 0.5608 (0.5759)	loss 2.1059 (2.0578)	grad_norm 1.6821 (1.6050)	mem 36269MB
Train: [0/180][231/5004]	eta 0:45:47 lr 0.100000	data 0.0006 (0.0142)	batch 0.5411 (0.5757)	loss 2.1730 (2.0583)	grad_norm 1.5779 (1.6049)	mem 36269MB
Train: [0/180][232/5004]	eta 0:45:46 lr 0.100000	data 0.0008 (0.0141)	batch 0.5476 (0.5756)	loss 2.0761 (2.0584)	grad_norm 1.6433 (1.6050)	mem 36269MB
Train: [0/180][233/5004]	eta 0:45:46 lr 0.100000	data 0.0008 (0.0140)	batch 0.5648 (0.5756)	loss 1.9905 (2.0581)	grad_norm 1.5573 (1.6048)	mem 36269MB
Train: [0/180][234/5004]	eta 0:45:45 lr 0.100000	data 0.0008 (0.0140)	batch 0.5667 (0.5755)	loss 2.1390 (2.0584)	grad_norm 1.7910 (1.6056)	mem 36269MB
Train: [0/180][235/5004]	eta 0:45:44 lr 0.100000	data 0.0007 (0.0139)	batch 0.5670 (0.5755)	loss 2.3525 (2.0597)	grad_norm 1.8185 (1.6065)	mem 36269MB
Train: [0/180][236/5004]	eta 0:45:43 lr 0.100000	data 0.0007 (0.0139)	batch 0.5637 (0.5754)	loss 2.1080 (2.0599)	grad_norm 1.8670 (1.6076)	mem 36269MB
Train: [0/180][237/5004]	eta 0:45:42 lr 0.100000	data 0.0006 (0.0138)	batch 0.5472 (0.5753)	loss 2.1200 (2.0601)	grad_norm 1.6981 (1.6080)	mem 36269MB
Train: [0/180][238/5004]	eta 0:45:41 lr 0.100000	data 0.0007 (0.0138)	batch 0.5666 (0.5753)	loss 2.2046 (2.0607)	grad_norm 1.8602 (1.6091)	mem 36269MB
Train: [0/180][239/5004]	eta 0:45:40 lr 0.100000	data 0.0006 (0.0137)	batch 0.5569 (0.5752)	loss 2.1566 (2.0611)	grad_norm 1.6570 (1.6093)	mem 36269MB
Train: [0/180][240/5004]	eta 0:45:40 lr 0.100000	data 0.0007 (0.0137)	batch 0.5587 (0.5751)	loss 2.1205 (2.0614)	grad_norm 1.7497 (1.6098)	mem 36269MB
Train: [0/180][241/5004]	eta 0:45:39 lr 0.100000	data 0.0007 (0.0136)	batch 0.5582 (0.5751)	loss 2.1223 (2.0616)	grad_norm 1.6738 (1.6101)	mem 36269MB
Train: [0/180][242/5004]	eta 0:45:38 lr 0.100000	data 0.0006 (0.0135)	batch 0.5525 (0.5750)	loss 2.0776 (2.0617)	grad_norm 1.6552 (1.6103)	mem 36269MB
Train: [0/180][243/5004]	eta 0:45:37 lr 0.100000	data 0.0008 (0.0135)	batch 0.5622 (0.5749)	loss 2.2002 (2.0623)	grad_norm 1.8408 (1.6112)	mem 36269MB
Train: [0/180][244/5004]	eta 0:45:36 lr 0.100000	data 0.0006 (0.0134)	batch 0.5578 (0.5749)	loss 2.2287 (2.0629)	grad_norm 1.7304 (1.6117)	mem 36269MB
Train: [0/180][245/5004]	eta 0:45:35 lr 0.100000	data 0.0007 (0.0134)	batch 0.5716 (0.5748)	loss 2.2907 (2.0639)	grad_norm 1.7901 (1.6124)	mem 36269MB
Train: [0/180][246/5004]	eta 0:45:34 lr 0.100000	data 0.0008 (0.0133)	batch 0.5415 (0.5747)	loss 1.9848 (2.0635)	grad_norm 1.5914 (1.6124)	mem 36269MB
Train: [0/180][247/5004]	eta 0:45:33 lr 0.100000	data 0.0008 (0.0133)	batch 0.5659 (0.5747)	loss 2.1706 (2.0640)	grad_norm 1.7894 (1.6131)	mem 36269MB
Train: [0/180][248/5004]	eta 0:45:32 lr 0.100000	data 0.0007 (0.0132)	batch 0.5488 (0.5746)	loss 2.2534 (2.0647)	grad_norm 1.7845 (1.6138)	mem 36269MB
Train: [0/180][249/5004]	eta 0:45:31 lr 0.100000	data 0.0008 (0.0132)	batch 0.5674 (0.5745)	loss 1.8576 (2.0639)	grad_norm 1.5142 (1.6134)	mem 36269MB
Train: [0/180][250/5004]	eta 0:45:30 lr 0.100000	data 0.0007 (0.0131)	batch 0.5474 (0.5744)	loss 2.1954 (2.0644)	grad_norm 1.6489 (1.6135)	mem 36269MB
Train: [0/180][251/5004]	eta 0:45:30 lr 0.100000	data 0.0005 (0.0131)	batch 0.5646 (0.5744)	loss 2.2009 (2.0650)	grad_norm 1.6964 (1.6138)	mem 36269MB
Train: [0/180][252/5004]	eta 0:45:28 lr 0.100000	data 0.0007 (0.0130)	batch 0.5417 (0.5743)	loss 2.2091 (2.0655)	grad_norm 1.9217 (1.6150)	mem 36269MB
Train: [0/180][253/5004]	eta 0:45:28 lr 0.100000	data 0.0008 (0.0130)	batch 0.5664 (0.5742)	loss 2.2919 (2.0664)	grad_norm 1.8001 (1.6158)	mem 36269MB
Train: [0/180][254/5004]	eta 0:45:27 lr 0.100000	data 0.0007 (0.0129)	batch 0.5480 (0.5741)	loss 2.2028 (2.0670)	grad_norm 1.7510 (1.6163)	mem 36269MB
Train: [0/180][255/5004]	eta 0:45:26 lr 0.100000	data 0.0008 (0.0129)	batch 0.5446 (0.5740)	loss 2.0505 (2.0669)	grad_norm 1.6248 (1.6163)	mem 36269MB
Train: [0/180][256/5004]	eta 0:45:24 lr 0.100000	data 0.0008 (0.0128)	batch 0.5466 (0.5739)	loss 2.1784 (2.0673)	grad_norm 1.7442 (1.6168)	mem 36269MB
Train: [0/180][257/5004]	eta 0:45:24 lr 0.100000	data 0.0007 (0.0128)	batch 0.5687 (0.5739)	loss 2.3282 (2.0683)	grad_norm 1.8828 (1.6179)	mem 36269MB
Train: [0/180][258/5004]	eta 0:45:23 lr 0.100000	data 0.0007 (0.0128)	batch 0.5478 (0.5738)	loss 2.1649 (2.0687)	grad_norm 1.8912 (1.6189)	mem 36269MB
Train: [0/180][259/5004]	eta 0:45:22 lr 0.100000	data 0.0006 (0.0127)	batch 0.5495 (0.5737)	loss 2.0936 (2.0688)	grad_norm 1.6194 (1.6189)	mem 36269MB
Train: [0/180][260/5004]	eta 0:45:21 lr 0.100000	data 0.0008 (0.0127)	batch 0.5628 (0.5737)	loss 2.2282 (2.0694)	grad_norm 1.8248 (1.6197)	mem 36269MB
Train: [0/180][261/5004]	eta 0:45:20 lr 0.100000	data 0.0005 (0.0126)	batch 0.5713 (0.5736)	loss 2.2327 (2.0700)	grad_norm 1.8323 (1.6205)	mem 36269MB
Train: [0/180][262/5004]	eta 0:45:19 lr 0.100000	data 0.0007 (0.0126)	batch 0.5600 (0.5736)	loss 2.1934 (2.0705)	grad_norm 1.8229 (1.6213)	mem 36269MB
Train: [0/180][263/5004]	eta 0:45:18 lr 0.100000	data 0.0007 (0.0125)	batch 0.5482 (0.5735)	loss 2.1462 (2.0708)	grad_norm 1.6617 (1.6214)	mem 36269MB
Train: [0/180][264/5004]	eta 0:45:18 lr 0.100000	data 0.0007 (0.0125)	batch 0.5576 (0.5734)	loss 2.2605 (2.0715)	grad_norm 1.6125 (1.6214)	mem 36269MB
Train: [0/180][265/5004]	eta 0:45:17 lr 0.100000	data 0.0009 (0.0124)	batch 0.5628 (0.5734)	loss 2.1041 (2.0716)	grad_norm 1.7250 (1.6218)	mem 36269MB
Train: [0/180][266/5004]	eta 0:45:16 lr 0.100000	data 0.0006 (0.0124)	batch 0.5566 (0.5733)	loss 2.2045 (2.0721)	grad_norm 1.5925 (1.6217)	mem 36269MB
Train: [0/180][267/5004]	eta 0:45:15 lr 0.100000	data 0.0007 (0.0123)	batch 0.5647 (0.5733)	loss 2.1943 (2.0726)	grad_norm 1.7651 (1.6222)	mem 36269MB
Train: [0/180][268/5004]	eta 0:45:14 lr 0.100000	data 0.0008 (0.0123)	batch 0.5551 (0.5732)	loss 2.1308 (2.0728)	grad_norm 1.6424 (1.6223)	mem 36269MB
Train: [0/180][269/5004]	eta 0:45:14 lr 0.100000	data 0.0007 (0.0123)	batch 0.5628 (0.5732)	loss 2.3207 (2.0737)	grad_norm 1.7561 (1.6228)	mem 36269MB
Train: [0/180][270/5004]	eta 0:45:13 lr 0.100000	data 0.0007 (0.0122)	batch 0.5567 (0.5731)	loss 2.2493 (2.0744)	grad_norm 1.7989 (1.6234)	mem 36269MB
Train: [0/180][271/5004]	eta 0:45:12 lr 0.100000	data 0.0006 (0.0122)	batch 0.5554 (0.5731)	loss 2.0134 (2.0742)	grad_norm 1.5912 (1.6233)	mem 36269MB
Train: [0/180][272/5004]	eta 0:45:11 lr 0.100000	data 0.0007 (0.0121)	batch 0.5622 (0.5730)	loss 2.0747 (2.0742)	grad_norm 1.6803 (1.6235)	mem 36269MB
Train: [0/180][273/5004]	eta 0:45:10 lr 0.100000	data 0.0006 (0.0121)	batch 0.5504 (0.5730)	loss 2.0687 (2.0741)	grad_norm 1.6452 (1.6236)	mem 36269MB
Train: [0/180][274/5004]	eta 0:45:09 lr 0.100000	data 0.0007 (0.0121)	batch 0.5498 (0.5729)	loss 2.0065 (2.0739)	grad_norm 1.8540 (1.6245)	mem 36269MB
Train: [0/180][275/5004]	eta 0:45:08 lr 0.100000	data 0.0007 (0.0120)	batch 0.5598 (0.5728)	loss 2.1597 (2.0742)	grad_norm 1.6901 (1.6247)	mem 36269MB
Train: [0/180][276/5004]	eta 0:45:08 lr 0.100000	data 0.0006 (0.0120)	batch 0.5670 (0.5728)	loss 1.9308 (2.0737)	grad_norm 1.7129 (1.6250)	mem 36269MB
Train: [0/180][277/5004]	eta 0:45:07 lr 0.100000	data 0.0007 (0.0119)	batch 0.5516 (0.5727)	loss 2.2095 (2.0742)	grad_norm 1.7861 (1.6256)	mem 36269MB
Train: [0/180][278/5004]	eta 0:45:06 lr 0.100000	data 0.0008 (0.0119)	batch 0.5591 (0.5727)	loss 2.2696 (2.0749)	grad_norm 1.8395 (1.6264)	mem 36269MB
Train: [0/180][279/5004]	eta 0:45:05 lr 0.100000	data 0.0007 (0.0118)	batch 0.5609 (0.5726)	loss 2.1789 (2.0752)	grad_norm 1.7074 (1.6266)	mem 36269MB
Train: [0/180][280/5004]	eta 0:45:04 lr 0.100000	data 0.0007 (0.0118)	batch 0.5637 (0.5726)	loss 1.9929 (2.0749)	grad_norm 1.7060 (1.6269)	mem 36269MB
Train: [0/180][281/5004]	eta 0:45:04 lr 0.100000	data 0.0006 (0.0118)	batch 0.5674 (0.5726)	loss 2.1927 (2.0754)	grad_norm 1.7863 (1.6275)	mem 36269MB
Train: [0/180][282/5004]	eta 0:45:03 lr 0.100000	data 0.0007 (0.0117)	batch 0.5651 (0.5726)	loss 2.2042 (2.0758)	grad_norm 1.8112 (1.6281)	mem 36269MB
Train: [0/180][283/5004]	eta 0:45:02 lr 0.100000	data 0.0006 (0.0117)	batch 0.5554 (0.5725)	loss 2.2784 (2.0765)	grad_norm 1.7584 (1.6286)	mem 36269MB
Train: [0/180][284/5004]	eta 0:45:02 lr 0.100000	data 0.0007 (0.0117)	batch 0.5649 (0.5725)	loss 2.1046 (2.0766)	grad_norm 1.5237 (1.6282)	mem 36269MB
Train: [0/180][285/5004]	eta 0:45:01 lr 0.100000	data 0.0007 (0.0116)	batch 0.5465 (0.5724)	loss 2.1749 (2.0770)	grad_norm 1.6260 (1.6282)	mem 36269MB
Train: [0/180][286/5004]	eta 0:44:59 lr 0.100000	data 0.0006 (0.0116)	batch 0.5421 (0.5723)	loss 2.3219 (2.0778)	grad_norm 1.7866 (1.6288)	mem 36269MB
Train: [0/180][287/5004]	eta 0:44:59 lr 0.100000	data 0.0007 (0.0115)	batch 0.5647 (0.5722)	loss 2.1741 (2.0782)	grad_norm 1.8742 (1.6296)	mem 36269MB
Train: [0/180][288/5004]	eta 0:44:58 lr 0.100000	data 0.0008 (0.0115)	batch 0.5576 (0.5722)	loss 2.2820 (2.0789)	grad_norm 1.8377 (1.6304)	mem 36269MB
Train: [0/180][289/5004]	eta 0:44:57 lr 0.100000	data 0.0007 (0.0115)	batch 0.5660 (0.5722)	loss 2.2603 (2.0795)	grad_norm 1.8877 (1.6312)	mem 36269MB
Train: [0/180][290/5004]	eta 0:44:56 lr 0.100000	data 0.0008 (0.0114)	batch 0.5442 (0.5721)	loss 2.0757 (2.0795)	grad_norm 1.5780 (1.6311)	mem 36269MB
Train: [0/180][291/5004]	eta 0:44:55 lr 0.100000	data 0.0006 (0.0114)	batch 0.5523 (0.5720)	loss 2.2065 (2.0799)	grad_norm 1.5762 (1.6309)	mem 36269MB
Train: [0/180][292/5004]	eta 0:44:54 lr 0.100000	data 0.0008 (0.0114)	batch 0.5420 (0.5719)	loss 2.1603 (2.0802)	grad_norm 1.6866 (1.6311)	mem 36269MB
Train: [0/180][293/5004]	eta 0:44:54 lr 0.100000	data 0.0006 (0.0113)	batch 0.5680 (0.5719)	loss 2.0750 (2.0802)	grad_norm 1.6933 (1.6313)	mem 36269MB
Train: [0/180][294/5004]	eta 0:44:53 lr 0.100000	data 0.0007 (0.0113)	batch 0.5637 (0.5719)	loss 2.2564 (2.0808)	grad_norm 1.6412 (1.6313)	mem 36269MB
Train: [0/180][295/5004]	eta 0:44:52 lr 0.100000	data 0.0007 (0.0112)	batch 0.5589 (0.5718)	loss 2.2157 (2.0812)	grad_norm 1.7124 (1.6316)	mem 36269MB
Train: [0/180][296/5004]	eta 0:44:51 lr 0.100000	data 0.0006 (0.0112)	batch 0.5482 (0.5717)	loss 2.0376 (2.0811)	grad_norm 1.7099 (1.6318)	mem 36269MB
Train: [0/180][297/5004]	eta 0:44:51 lr 0.100000	data 0.0008 (0.0112)	batch 0.5652 (0.5717)	loss 2.1952 (2.0815)	grad_norm 1.6667 (1.6320)	mem 36269MB
Train: [0/180][298/5004]	eta 0:44:50 lr 0.100000	data 0.0008 (0.0111)	batch 0.5602 (0.5717)	loss 2.1197 (2.0816)	grad_norm 1.6568 (1.6320)	mem 36269MB
Train: [0/180][299/5004]	eta 0:44:49 lr 0.100000	data 0.0007 (0.0111)	batch 0.5680 (0.5717)	loss 2.1972 (2.0820)	grad_norm 1.7221 (1.6323)	mem 36269MB
Train: [0/180][300/5004]	eta 0:44:49 lr 0.100000	data 0.0006 (0.0111)	batch 0.5677 (0.5717)	loss 2.5973 (2.0837)	grad_norm 1.8786 (1.6332)	mem 36269MB
Train: [0/180][301/5004]	eta 0:44:48 lr 0.100000	data 0.0007 (0.0110)	batch 0.5600 (0.5716)	loss 2.0941 (2.0837)	grad_norm 1.6313 (1.6332)	mem 36269MB
Train: [0/180][302/5004]	eta 0:44:47 lr 0.100000	data 0.0007 (0.0110)	batch 0.5605 (0.5716)	loss 2.2883 (2.0844)	grad_norm 1.7091 (1.6334)	mem 36269MB
Train: [0/180][303/5004]	eta 0:44:46 lr 0.100000	data 0.0007 (0.0110)	batch 0.5441 (0.5715)	loss 2.4306 (2.0855)	grad_norm 1.6513 (1.6335)	mem 36269MB
Train: [0/180][304/5004]	eta 0:44:45 lr 0.100000	data 0.0007 (0.0109)	batch 0.5557 (0.5714)	loss 2.2536 (2.0861)	grad_norm 1.7866 (1.6340)	mem 36269MB
Train: [0/180][305/5004]	eta 0:44:44 lr 0.100000	data 0.0006 (0.0109)	batch 0.5578 (0.5714)	loss 2.1185 (2.0862)	grad_norm 1.5491 (1.6337)	mem 36269MB
Train: [0/180][306/5004]	eta 0:44:44 lr 0.100000	data 0.0007 (0.0109)	batch 0.5532 (0.5713)	loss 2.1406 (2.0864)	grad_norm 1.5989 (1.6336)	mem 36269MB
Train: [0/180][307/5004]	eta 0:44:43 lr 0.100000	data 0.0007 (0.0108)	batch 0.5584 (0.5713)	loss 2.2489 (2.0869)	grad_norm 1.7938 (1.6341)	mem 36269MB
Train: [0/180][308/5004]	eta 0:44:42 lr 0.100000	data 0.0007 (0.0108)	batch 0.5652 (0.5713)	loss 2.2692 (2.0875)	grad_norm 1.6892 (1.6343)	mem 36269MB
Train: [0/180][309/5004]	eta 0:44:41 lr 0.100000	data 0.0007 (0.0108)	batch 0.5431 (0.5712)	loss 2.2341 (2.0880)	grad_norm 1.7072 (1.6345)	mem 36269MB
Train: [0/180][310/5004]	eta 0:44:40 lr 0.100000	data 0.0005 (0.0107)	batch 0.5547 (0.5711)	loss 2.2017 (2.0883)	grad_norm 1.6615 (1.6346)	mem 36269MB
Train: [0/180][311/5004]	eta 0:44:40 lr 0.100000	data 0.0008 (0.0107)	batch 0.5678 (0.5711)	loss 2.1010 (2.0884)	grad_norm 1.6787 (1.6347)	mem 36269MB
Train: [0/180][312/5004]	eta 0:44:39 lr 0.100000	data 0.0006 (0.0107)	batch 0.5724 (0.5711)	loss 2.3926 (2.0893)	grad_norm 1.7255 (1.6350)	mem 36269MB
Train: [0/180][313/5004]	eta 0:44:39 lr 0.100000	data 0.0007 (0.0106)	batch 0.5621 (0.5711)	loss 2.1995 (2.0897)	grad_norm 1.7277 (1.6353)	mem 36269MB
Train: [0/180][314/5004]	eta 0:44:38 lr 0.100000	data 0.0006 (0.0106)	batch 0.5595 (0.5711)	loss 2.0945 (2.0897)	grad_norm 1.6108 (1.6352)	mem 36269MB
Train: [0/180][315/5004]	eta 0:44:37 lr 0.100000	data 0.0007 (0.0106)	batch 0.5413 (0.5710)	loss 2.1404 (2.0899)	grad_norm 1.5370 (1.6349)	mem 36269MB
Train: [0/180][316/5004]	eta 0:44:36 lr 0.100000	data 0.0006 (0.0105)	batch 0.5415 (0.5709)	loss 2.2134 (2.0903)	grad_norm 1.7173 (1.6352)	mem 36269MB
Train: [0/180][317/5004]	eta 0:44:35 lr 0.100000	data 0.0006 (0.0105)	batch 0.5577 (0.5708)	loss 2.1943 (2.0906)	grad_norm 1.6609 (1.6353)	mem 36269MB
Train: [0/180][318/5004]	eta 0:44:34 lr 0.100000	data 0.0006 (0.0105)	batch 0.5423 (0.5707)	loss 2.1192 (2.0907)	grad_norm 1.7436 (1.6356)	mem 36269MB
Train: [0/180][319/5004]	eta 0:44:33 lr 0.100000	data 0.0007 (0.0105)	batch 0.5615 (0.5707)	loss 2.3597 (2.0915)	grad_norm 1.8248 (1.6362)	mem 36269MB
Train: [0/180][320/5004]	eta 0:44:33 lr 0.100000	data 0.0007 (0.0104)	batch 0.5633 (0.5707)	loss 2.2198 (2.0919)	grad_norm 1.6639 (1.6363)	mem 36269MB
Train: [0/180][321/5004]	eta 0:44:32 lr 0.100000	data 0.0007 (0.0104)	batch 0.5609 (0.5707)	loss 2.1325 (2.0920)	grad_norm 1.7947 (1.6368)	mem 36269MB
Train: [0/180][322/5004]	eta 0:44:31 lr 0.100000	data 0.0005 (0.0104)	batch 0.5475 (0.5706)	loss 2.2836 (2.0926)	grad_norm 1.7434 (1.6371)	mem 36269MB
Train: [0/180][323/5004]	eta 0:44:30 lr 0.100000	data 0.0006 (0.0103)	batch 0.5650 (0.5706)	loss 2.3189 (2.0933)	grad_norm 1.7159 (1.6374)	mem 36269MB
Train: [0/180][324/5004]	eta 0:44:29 lr 0.100000	data 0.0006 (0.0103)	batch 0.5426 (0.5705)	loss 2.2055 (2.0937)	grad_norm 1.5840 (1.6372)	mem 36269MB
Train: [0/180][325/5004]	eta 0:44:29 lr 0.100000	data 0.0007 (0.0103)	batch 0.5616 (0.5705)	loss 2.3285 (2.0944)	grad_norm 1.6825 (1.6373)	mem 36269MB
Train: [0/180][326/5004]	eta 0:44:28 lr 0.100000	data 0.0007 (0.0102)	batch 0.5548 (0.5704)	loss 2.0670 (2.0943)	grad_norm 1.5575 (1.6371)	mem 36269MB
Train: [0/180][327/5004]	eta 0:44:27 lr 0.100000	data 0.0007 (0.0102)	batch 0.5588 (0.5704)	loss 2.1119 (2.0944)	grad_norm 1.5609 (1.6369)	mem 36269MB
Train: [0/180][328/5004]	eta 0:44:26 lr 0.100000	data 0.0006 (0.0102)	batch 0.5583 (0.5703)	loss 2.0484 (2.0942)	grad_norm 1.5210 (1.6365)	mem 36269MB
Train: [0/180][329/5004]	eta 0:44:26 lr 0.100000	data 0.0006 (0.0102)	batch 0.5555 (0.5703)	loss 2.1447 (2.0944)	grad_norm 1.6403 (1.6365)	mem 36269MB
Train: [0/180][330/5004]	eta 0:44:25 lr 0.100000	data 0.0006 (0.0101)	batch 0.5654 (0.5703)	loss 2.4375 (2.0954)	grad_norm 1.7996 (1.6370)	mem 36269MB
Train: [0/180][331/5004]	eta 0:44:24 lr 0.100000	data 0.0007 (0.0101)	batch 0.5431 (0.5702)	loss 2.3463 (2.0962)	grad_norm 1.6847 (1.6371)	mem 36269MB
Train: [0/180][332/5004]	eta 0:44:23 lr 0.100000	data 0.0007 (0.0101)	batch 0.5700 (0.5702)	loss 2.2866 (2.0967)	grad_norm 1.6795 (1.6373)	mem 36269MB
Train: [0/180][333/5004]	eta 0:44:23 lr 0.100000	data 0.0008 (0.0100)	batch 0.5660 (0.5702)	loss 2.1975 (2.0970)	grad_norm 1.5102 (1.6369)	mem 36269MB
Train: [0/180][334/5004]	eta 0:44:22 lr 0.100000	data 0.0008 (0.0100)	batch 0.5599 (0.5701)	loss 2.2159 (2.0974)	grad_norm 1.6430 (1.6369)	mem 36269MB
Train: [0/180][335/5004]	eta 0:44:21 lr 0.100000	data 0.0006 (0.0100)	batch 0.5446 (0.5701)	loss 2.2190 (2.0978)	grad_norm 1.5514 (1.6367)	mem 36269MB
Train: [0/180][336/5004]	eta 0:44:20 lr 0.100000	data 0.0007 (0.0100)	batch 0.5497 (0.5700)	loss 2.1879 (2.0980)	grad_norm 1.5938 (1.6365)	mem 36269MB
Train: [0/180][337/5004]	eta 0:44:19 lr 0.100000	data 0.0007 (0.0099)	batch 0.5469 (0.5699)	loss 2.1632 (2.0982)	grad_norm 1.7480 (1.6369)	mem 36269MB
Train: [0/180][338/5004]	eta 0:44:19 lr 0.100000	data 0.0006 (0.0099)	batch 0.5672 (0.5699)	loss 2.0101 (2.0980)	grad_norm 1.7299 (1.6371)	mem 36269MB
Train: [0/180][339/5004]	eta 0:44:18 lr 0.100000	data 0.0007 (0.0099)	batch 0.5503 (0.5699)	loss 2.2505 (2.0984)	grad_norm 1.8057 (1.6376)	mem 36269MB
Train: [0/180][340/5004]	eta 0:44:17 lr 0.100000	data 0.0007 (0.0098)	batch 0.5652 (0.5699)	loss 2.0052 (2.0981)	grad_norm 1.6510 (1.6377)	mem 36269MB
Train: [0/180][341/5004]	eta 0:44:16 lr 0.100000	data 0.0007 (0.0098)	batch 0.5470 (0.5698)	loss 2.2078 (2.0985)	grad_norm 1.8598 (1.6383)	mem 36269MB
Train: [0/180][342/5004]	eta 0:44:16 lr 0.100000	data 0.0005 (0.0098)	batch 0.5429 (0.5697)	loss 2.2479 (2.0989)	grad_norm 1.6511 (1.6384)	mem 36269MB
Train: [0/180][343/5004]	eta 0:44:15 lr 0.100000	data 0.0006 (0.0098)	batch 0.5572 (0.5697)	loss 2.2840 (2.0994)	grad_norm 1.7114 (1.6386)	mem 36269MB
Train: [0/180][344/5004]	eta 0:44:14 lr 0.100000	data 0.0005 (0.0097)	batch 0.5639 (0.5697)	loss 2.1712 (2.0996)	grad_norm 1.6882 (1.6387)	mem 36269MB
Train: [0/180][345/5004]	eta 0:44:14 lr 0.100000	data 0.0007 (0.0097)	batch 0.5644 (0.5697)	loss 2.1534 (2.0998)	grad_norm 1.7298 (1.6390)	mem 36269MB
Train: [0/180][346/5004]	eta 0:44:13 lr 0.100000	data 0.0007 (0.0097)	batch 0.5646 (0.5696)	loss 2.2282 (2.1002)	grad_norm 1.6825 (1.6391)	mem 36269MB
Train: [0/180][347/5004]	eta 0:44:12 lr 0.100000	data 0.0006 (0.0097)	batch 0.5647 (0.5696)	loss 2.3135 (2.1008)	grad_norm 1.5973 (1.6390)	mem 36269MB
Train: [0/180][348/5004]	eta 0:44:11 lr 0.100000	data 0.0006 (0.0096)	batch 0.5470 (0.5696)	loss 2.3417 (2.1015)	grad_norm 1.6987 (1.6392)	mem 36269MB
Train: [0/180][349/5004]	eta 0:44:11 lr 0.100000	data 0.0007 (0.0096)	batch 0.5684 (0.5696)	loss 1.9576 (2.1011)	grad_norm 1.6704 (1.6392)	mem 36269MB
Train: [0/180][350/5004]	eta 0:44:10 lr 0.100000	data 0.0007 (0.0096)	batch 0.5479 (0.5695)	loss 2.4214 (2.1020)	grad_norm 1.8742 (1.6399)	mem 36269MB
Train: [0/180][351/5004]	eta 0:44:09 lr 0.100000	data 0.0007 (0.0096)	batch 0.5524 (0.5694)	loss 2.3217 (2.1026)	grad_norm 1.8912 (1.6406)	mem 36269MB
Train: [0/180][352/5004]	eta 0:44:09 lr 0.100000	data 0.0007 (0.0095)	batch 0.5735 (0.5695)	loss 2.1613 (2.1028)	grad_norm 1.6984 (1.6408)	mem 36269MB
Train: [0/180][353/5004]	eta 0:44:08 lr 0.100000	data 0.0007 (0.0095)	batch 0.5420 (0.5694)	loss 2.3082 (2.1033)	grad_norm 1.6597 (1.6408)	mem 36269MB
Train: [0/180][354/5004]	eta 0:44:07 lr 0.100000	data 0.0006 (0.0095)	batch 0.5577 (0.5693)	loss 2.0907 (2.1033)	grad_norm 1.7132 (1.6410)	mem 36269MB
Train: [0/180][355/5004]	eta 0:44:06 lr 0.100000	data 0.0007 (0.0095)	batch 0.5554 (0.5693)	loss 2.3318 (2.1039)	grad_norm 1.6711 (1.6411)	mem 36269MB
Train: [0/180][356/5004]	eta 0:44:06 lr 0.100000	data 0.0007 (0.0094)	batch 0.5664 (0.5693)	loss 2.4266 (2.1049)	grad_norm 1.7582 (1.6415)	mem 36269MB
Train: [0/180][357/5004]	eta 0:44:05 lr 0.100000	data 0.0007 (0.0094)	batch 0.5526 (0.5693)	loss 2.1756 (2.1051)	grad_norm 1.7133 (1.6417)	mem 36269MB
Train: [0/180][358/5004]	eta 0:44:04 lr 0.100000	data 0.0010 (0.0094)	batch 0.5646 (0.5692)	loss 2.3042 (2.1056)	grad_norm 1.7419 (1.6419)	mem 36269MB
Train: [0/180][359/5004]	eta 0:44:03 lr 0.100000	data 0.0007 (0.0094)	batch 0.5589 (0.5692)	loss 2.2478 (2.1060)	grad_norm 1.5882 (1.6418)	mem 36269MB
Train: [0/180][360/5004]	eta 0:44:03 lr 0.100000	data 0.0007 (0.0093)	batch 0.5605 (0.5692)	loss 2.1160 (2.1060)	grad_norm 1.6845 (1.6419)	mem 36269MB
Train: [0/180][361/5004]	eta 0:44:02 lr 0.100000	data 0.0007 (0.0093)	batch 0.5499 (0.5691)	loss 2.2672 (2.1065)	grad_norm 1.6981 (1.6421)	mem 36269MB
Train: [0/180][362/5004]	eta 0:44:01 lr 0.100000	data 0.0008 (0.0093)	batch 0.5584 (0.5691)	loss 2.1046 (2.1065)	grad_norm 1.6899 (1.6422)	mem 36269MB
Train: [0/180][363/5004]	eta 0:44:01 lr 0.100000	data 0.0006 (0.0093)	batch 0.5594 (0.5691)	loss 2.1394 (2.1066)	grad_norm 1.5533 (1.6420)	mem 36269MB
Train: [0/180][364/5004]	eta 0:44:00 lr 0.100000	data 0.0006 (0.0092)	batch 0.5668 (0.5691)	loss 2.2290 (2.1069)	grad_norm 1.6978 (1.6421)	mem 36269MB
Train: [0/180][365/5004]	eta 0:43:59 lr 0.100000	data 0.0006 (0.0092)	batch 0.5644 (0.5691)	loss 2.0624 (2.1068)	grad_norm 1.5134 (1.6418)	mem 36269MB
Train: [0/180][366/5004]	eta 0:43:59 lr 0.100000	data 0.0006 (0.0092)	batch 0.5528 (0.5690)	loss 2.2371 (2.1071)	grad_norm 1.7667 (1.6421)	mem 36269MB
Train: [0/180][367/5004]	eta 0:43:58 lr 0.100000	data 0.0006 (0.0092)	batch 0.5654 (0.5690)	loss 2.0584 (2.1070)	grad_norm 1.4307 (1.6415)	mem 36269MB
Train: [0/180][368/5004]	eta 0:43:57 lr 0.100000	data 0.0005 (0.0092)	batch 0.5626 (0.5690)	loss 2.2201 (2.1073)	grad_norm 1.6148 (1.6414)	mem 36269MB
Train: [0/180][369/5004]	eta 0:43:57 lr 0.100000	data 0.0006 (0.0091)	batch 0.5698 (0.5690)	loss 2.2332 (2.1076)	grad_norm 1.7655 (1.6418)	mem 36269MB
Train: [0/180][370/5004]	eta 0:43:56 lr 0.100000	data 0.0007 (0.0091)	batch 0.5663 (0.5690)	loss 2.0642 (2.1075)	grad_norm 1.6147 (1.6417)	mem 36269MB
Train: [0/180][371/5004]	eta 0:43:55 lr 0.100000	data 0.0007 (0.0091)	batch 0.5583 (0.5690)	loss 2.1120 (2.1075)	grad_norm 1.6265 (1.6417)	mem 36269MB
Train: [0/180][372/5004]	eta 0:43:55 lr 0.100000	data 0.0101 (0.0091)	batch 0.5688 (0.5690)	loss 2.2038 (2.1078)	grad_norm 1.7643 (1.6420)	mem 36269MB
Train: [0/180][373/5004]	eta 0:43:54 lr 0.100000	data 0.0006 (0.0091)	batch 0.5451 (0.5689)	loss 2.3755 (2.1085)	grad_norm 1.7774 (1.6424)	mem 36269MB
Train: [0/180][374/5004]	eta 0:43:53 lr 0.100000	data 0.0005 (0.0090)	batch 0.5634 (0.5689)	loss 2.2849 (2.1090)	grad_norm 1.5457 (1.6421)	mem 36269MB
Train: [0/180][375/5004]	eta 0:43:53 lr 0.100000	data 0.0007 (0.0090)	batch 0.5687 (0.5689)	loss 2.2579 (2.1094)	grad_norm 1.6641 (1.6422)	mem 36269MB
Train: [0/180][376/5004]	eta 0:43:52 lr 0.100000	data 0.0008 (0.0090)	batch 0.5558 (0.5688)	loss 2.1825 (2.1096)	grad_norm 1.6773 (1.6423)	mem 36269MB
Train: [0/180][377/5004]	eta 0:43:51 lr 0.100000	data 0.0007 (0.0090)	batch 0.5574 (0.5688)	loss 2.1383 (2.1096)	grad_norm 1.7249 (1.6425)	mem 36269MB
Train: [0/180][378/5004]	eta 0:43:51 lr 0.100000	data 0.0006 (0.0090)	batch 0.5673 (0.5688)	loss 2.1496 (2.1098)	grad_norm 1.5812 (1.6423)	mem 36269MB
Train: [0/180][379/5004]	eta 0:43:50 lr 0.100000	data 0.0007 (0.0089)	batch 0.5655 (0.5688)	loss 2.1341 (2.1098)	grad_norm 1.7917 (1.6427)	mem 36269MB
Train: [0/180][380/5004]	eta 0:43:50 lr 0.100000	data 0.0008 (0.0089)	batch 0.5637 (0.5688)	loss 2.0691 (2.1097)	grad_norm 1.5939 (1.6426)	mem 36269MB
Train: [0/180][381/5004]	eta 0:43:49 lr 0.100000	data 0.0007 (0.0089)	batch 0.5436 (0.5687)	loss 2.1054 (2.1097)	grad_norm 1.5604 (1.6424)	mem 36269MB
Train: [0/180][382/5004]	eta 0:43:48 lr 0.100000	data 0.0007 (0.0089)	batch 0.5718 (0.5687)	loss 2.2130 (2.1100)	grad_norm 1.6209 (1.6423)	mem 36269MB
Train: [0/180][383/5004]	eta 0:43:47 lr 0.100000	data 0.0007 (0.0088)	batch 0.5613 (0.5687)	loss 2.1134 (2.1100)	grad_norm 1.7776 (1.6427)	mem 36269MB
Train: [0/180][384/5004]	eta 0:43:47 lr 0.100000	data 0.0007 (0.0088)	batch 0.5569 (0.5687)	loss 2.1347 (2.1100)	grad_norm 1.4485 (1.6421)	mem 36269MB
Train: [0/180][385/5004]	eta 0:43:46 lr 0.100000	data 0.0007 (0.0088)	batch 0.5640 (0.5687)	loss 2.4016 (2.1108)	grad_norm 1.7714 (1.6425)	mem 36269MB
Train: [0/180][386/5004]	eta 0:43:45 lr 0.100000	data 0.0006 (0.0088)	batch 0.5610 (0.5686)	loss 2.0651 (2.1107)	grad_norm 1.6007 (1.6424)	mem 36269MB
Train: [0/180][387/5004]	eta 0:43:45 lr 0.100000	data 0.0007 (0.0088)	batch 0.5468 (0.5686)	loss 2.2004 (2.1109)	grad_norm 1.6067 (1.6423)	mem 36269MB
Train: [0/180][388/5004]	eta 0:43:44 lr 0.100000	data 0.0007 (0.0087)	batch 0.5650 (0.5686)	loss 2.3022 (2.1114)	grad_norm 1.8544 (1.6428)	mem 36269MB
Train: [0/180][389/5004]	eta 0:43:43 lr 0.100000	data 0.0007 (0.0087)	batch 0.5469 (0.5685)	loss 2.2182 (2.1117)	grad_norm 1.6701 (1.6429)	mem 36269MB
Train: [0/180][390/5004]	eta 0:43:43 lr 0.100000	data 0.0008 (0.0087)	batch 0.5667 (0.5685)	loss 2.2704 (2.1121)	grad_norm 1.7279 (1.6431)	mem 36269MB
Train: [0/180][391/5004]	eta 0:43:42 lr 0.100000	data 0.0008 (0.0087)	batch 0.5684 (0.5685)	loss 2.2059 (2.1123)	grad_norm 1.7046 (1.6433)	mem 36269MB
Train: [0/180][392/5004]	eta 0:43:41 lr 0.100000	data 0.0008 (0.0087)	batch 0.5646 (0.5685)	loss 2.2912 (2.1128)	grad_norm 1.7502 (1.6435)	mem 36269MB
Train: [0/180][393/5004]	eta 0:43:41 lr 0.100000	data 0.0007 (0.0086)	batch 0.5668 (0.5685)	loss 2.0401 (2.1126)	grad_norm 1.6561 (1.6436)	mem 36269MB
Train: [0/180][394/5004]	eta 0:43:40 lr 0.100000	data 0.0007 (0.0086)	batch 0.5566 (0.5685)	loss 2.2074 (2.1128)	grad_norm 1.6589 (1.6436)	mem 36269MB
Train: [0/180][395/5004]	eta 0:43:39 lr 0.100000	data 0.0008 (0.0086)	batch 0.5518 (0.5684)	loss 2.1128 (2.1128)	grad_norm 1.6254 (1.6436)	mem 36269MB
Train: [0/180][396/5004]	eta 0:43:39 lr 0.100000	data 0.0006 (0.0086)	batch 0.5546 (0.5684)	loss 2.2422 (2.1132)	grad_norm 1.8317 (1.6440)	mem 36269MB
Train: [0/180][397/5004]	eta 0:43:38 lr 0.100000	data 0.0006 (0.0086)	batch 0.5474 (0.5683)	loss 2.0306 (2.1129)	grad_norm 1.6016 (1.6439)	mem 36269MB
Train: [0/180][398/5004]	eta 0:43:37 lr 0.100000	data 0.0006 (0.0085)	batch 0.5493 (0.5683)	loss 2.1614 (2.1131)	grad_norm 1.8573 (1.6445)	mem 36269MB
Train: [0/180][399/5004]	eta 0:43:36 lr 0.100000	data 0.0007 (0.0085)	batch 0.5670 (0.5683)	loss 2.2237 (2.1133)	grad_norm 1.6699 (1.6445)	mem 36269MB
Train: [0/180][400/5004]	eta 0:43:36 lr 0.100000	data 0.0007 (0.0085)	batch 0.5617 (0.5683)	loss 2.1240 (2.1134)	grad_norm 1.5785 (1.6444)	mem 36269MB
Train: [0/180][401/5004]	eta 0:43:35 lr 0.100000	data 0.0006 (0.0085)	batch 0.5626 (0.5683)	loss 2.1041 (2.1134)	grad_norm 1.5204 (1.6441)	mem 36269MB
Train: [0/180][402/5004]	eta 0:43:35 lr 0.100000	data 0.0006 (0.0085)	batch 0.5636 (0.5682)	loss 2.2035 (2.1136)	grad_norm 1.7355 (1.6443)	mem 36269MB
Train: [0/180][403/5004]	eta 0:43:34 lr 0.100000	data 0.0005 (0.0084)	batch 0.5473 (0.5682)	loss 2.0609 (2.1134)	grad_norm 1.7174 (1.6445)	mem 36269MB
Train: [0/180][404/5004]	eta 0:43:33 lr 0.100000	data 0.0007 (0.0084)	batch 0.5492 (0.5681)	loss 2.1889 (2.1136)	grad_norm 1.6612 (1.6445)	mem 36269MB
Train: [0/180][405/5004]	eta 0:43:32 lr 0.100000	data 0.0007 (0.0084)	batch 0.5593 (0.5681)	loss 1.9521 (2.1132)	grad_norm 1.5549 (1.6443)	mem 36269MB
Train: [0/180][406/5004]	eta 0:43:32 lr 0.100000	data 0.0005 (0.0084)	batch 0.5604 (0.5681)	loss 2.2239 (2.1135)	grad_norm 1.7465 (1.6445)	mem 36269MB
Train: [0/180][407/5004]	eta 0:43:31 lr 0.100000	data 0.0006 (0.0084)	batch 0.5468 (0.5681)	loss 2.3519 (2.1141)	grad_norm 1.5953 (1.6444)	mem 36269MB
Train: [0/180][408/5004]	eta 0:43:30 lr 0.100000	data 0.0006 (0.0083)	batch 0.5431 (0.5680)	loss 2.2417 (2.1144)	grad_norm 1.6946 (1.6445)	mem 36269MB
Train: [0/180][409/5004]	eta 0:43:29 lr 0.100000	data 0.0006 (0.0083)	batch 0.5667 (0.5680)	loss 2.2903 (2.1148)	grad_norm 1.9012 (1.6452)	mem 36269MB
Train: [0/180][410/5004]	eta 0:43:29 lr 0.100000	data 0.0007 (0.0083)	batch 0.5641 (0.5680)	loss 2.1503 (2.1149)	grad_norm 1.6049 (1.6451)	mem 36269MB
Train: [0/180][411/5004]	eta 0:43:28 lr 0.100000	data 0.0006 (0.0083)	batch 0.5652 (0.5680)	loss 2.1428 (2.1150)	grad_norm 1.8072 (1.6455)	mem 36269MB
Train: [0/180][412/5004]	eta 0:43:28 lr 0.100000	data 0.0006 (0.0083)	batch 0.5660 (0.5680)	loss 2.2618 (2.1153)	grad_norm 1.6469 (1.6455)	mem 36269MB
Train: [0/180][413/5004]	eta 0:43:27 lr 0.100000	data 0.0006 (0.0083)	batch 0.5662 (0.5680)	loss 2.2498 (2.1157)	grad_norm 1.7081 (1.6456)	mem 36269MB
Train: [0/180][414/5004]	eta 0:43:26 lr 0.100000	data 0.0007 (0.0082)	batch 0.5661 (0.5680)	loss 2.1005 (2.1156)	grad_norm 1.7729 (1.6459)	mem 36269MB
Train: [0/180][415/5004]	eta 0:43:26 lr 0.100000	data 0.0007 (0.0082)	batch 0.5558 (0.5679)	loss 2.2884 (2.1160)	grad_norm 1.7790 (1.6462)	mem 36269MB
Train: [0/180][416/5004]	eta 0:43:25 lr 0.100000	data 0.0007 (0.0082)	batch 0.5477 (0.5679)	loss 2.3092 (2.1165)	grad_norm 1.6495 (1.6463)	mem 36269MB
Train: [0/180][417/5004]	eta 0:43:24 lr 0.100000	data 0.0007 (0.0082)	batch 0.5476 (0.5678)	loss 2.1408 (2.1166)	grad_norm 1.5111 (1.6459)	mem 36269MB
Train: [0/180][418/5004]	eta 0:43:24 lr 0.100000	data 0.0006 (0.0082)	batch 0.5683 (0.5678)	loss 2.3400 (2.1171)	grad_norm 1.7842 (1.6463)	mem 36269MB
Train: [0/180][419/5004]	eta 0:43:23 lr 0.100000	data 0.0006 (0.0081)	batch 0.5699 (0.5678)	loss 2.2551 (2.1174)	grad_norm 1.6567 (1.6463)	mem 36269MB
Train: [0/180][420/5004]	eta 0:43:22 lr 0.100000	data 0.0007 (0.0081)	batch 0.5668 (0.5678)	loss 1.9657 (2.1171)	grad_norm 1.5851 (1.6461)	mem 36269MB
Train: [0/180][421/5004]	eta 0:43:22 lr 0.100000	data 0.0006 (0.0081)	batch 0.5582 (0.5678)	loss 2.0630 (2.1169)	grad_norm 1.6099 (1.6461)	mem 36269MB
Train: [0/180][422/5004]	eta 0:43:21 lr 0.100000	data 0.0007 (0.0081)	batch 0.5436 (0.5678)	loss 2.2247 (2.1172)	grad_norm 1.5887 (1.6459)	mem 36269MB
Train: [0/180][423/5004]	eta 0:43:20 lr 0.100000	data 0.0006 (0.0081)	batch 0.5604 (0.5677)	loss 2.3219 (2.1177)	grad_norm 1.8509 (1.6464)	mem 36269MB
Train: [0/180][424/5004]	eta 0:43:20 lr 0.100000	data 0.0007 (0.0081)	batch 0.5648 (0.5677)	loss 2.2888 (2.1181)	grad_norm 1.6758 (1.6465)	mem 36269MB
Train: [0/180][425/5004]	eta 0:43:19 lr 0.100000	data 0.0007 (0.0080)	batch 0.5656 (0.5677)	loss 2.1001 (2.1180)	grad_norm 1.6209 (1.6464)	mem 36269MB
Train: [0/180][426/5004]	eta 0:43:19 lr 0.100000	data 0.0006 (0.0080)	batch 0.5662 (0.5677)	loss 2.1608 (2.1181)	grad_norm 1.6819 (1.6465)	mem 36269MB
Train: [0/180][427/5004]	eta 0:43:18 lr 0.100000	data 0.0006 (0.0080)	batch 0.5461 (0.5677)	loss 2.2818 (2.1185)	grad_norm 1.6904 (1.6466)	mem 36269MB
Train: [0/180][428/5004]	eta 0:43:17 lr 0.100000	data 0.0007 (0.0080)	batch 0.5516 (0.5676)	loss 2.1485 (2.1186)	grad_norm 1.6296 (1.6466)	mem 36269MB
Train: [0/180][429/5004]	eta 0:43:16 lr 0.100000	data 0.0008 (0.0080)	batch 0.5612 (0.5676)	loss 2.2078 (2.1188)	grad_norm 1.5217 (1.6463)	mem 36269MB
Train: [0/180][430/5004]	eta 0:43:16 lr 0.100000	data 0.0006 (0.0080)	batch 0.5660 (0.5676)	loss 2.1345 (2.1188)	grad_norm 1.6055 (1.6462)	mem 36269MB
Train: [0/180][431/5004]	eta 0:43:15 lr 0.100000	data 0.0007 (0.0079)	batch 0.5425 (0.5676)	loss 2.0143 (2.1186)	grad_norm 1.7299 (1.6464)	mem 36269MB
Train: [0/180][432/5004]	eta 0:43:14 lr 0.100000	data 0.0006 (0.0079)	batch 0.5411 (0.5675)	loss 2.1799 (2.1187)	grad_norm 1.6226 (1.6463)	mem 36269MB
Train: [0/180][433/5004]	eta 0:43:13 lr 0.100000	data 0.0006 (0.0079)	batch 0.5615 (0.5675)	loss 2.0223 (2.1185)	grad_norm 1.6159 (1.6462)	mem 36269MB
Train: [0/180][434/5004]	eta 0:43:13 lr 0.100000	data 0.0005 (0.0079)	batch 0.5604 (0.5675)	loss 2.3221 (2.1190)	grad_norm 1.7648 (1.6465)	mem 36269MB
Train: [0/180][435/5004]	eta 0:43:12 lr 0.100000	data 0.0006 (0.0079)	batch 0.5567 (0.5674)	loss 2.1235 (2.1190)	grad_norm 1.7080 (1.6467)	mem 36269MB
Train: [0/180][436/5004]	eta 0:43:11 lr 0.100000	data 0.0006 (0.0079)	batch 0.5586 (0.5674)	loss 2.3380 (2.1195)	grad_norm 1.6764 (1.6467)	mem 36269MB
Train: [0/180][437/5004]	eta 0:43:11 lr 0.100000	data 0.0005 (0.0078)	batch 0.5481 (0.5674)	loss 2.0493 (2.1193)	grad_norm 1.6110 (1.6466)	mem 36269MB
Train: [0/180][438/5004]	eta 0:43:10 lr 0.100000	data 0.0005 (0.0078)	batch 0.5706 (0.5674)	loss 2.2016 (2.1195)	grad_norm 1.5720 (1.6465)	mem 36269MB
Train: [0/180][439/5004]	eta 0:43:10 lr 0.100000	data 0.0007 (0.0078)	batch 0.5564 (0.5674)	loss 2.2717 (2.1199)	grad_norm 1.7390 (1.6467)	mem 36269MB
Train: [0/180][440/5004]	eta 0:43:09 lr 0.100000	data 0.0007 (0.0078)	batch 0.5622 (0.5674)	loss 2.2314 (2.1201)	grad_norm 1.7457 (1.6469)	mem 36269MB
Train: [0/180][441/5004]	eta 0:43:08 lr 0.100000	data 0.0006 (0.0078)	batch 0.5579 (0.5673)	loss 2.1891 (2.1203)	grad_norm 1.7048 (1.6470)	mem 36269MB
Train: [0/180][442/5004]	eta 0:43:08 lr 0.100000	data 0.0007 (0.0078)	batch 0.5649 (0.5673)	loss 2.0848 (2.1202)	grad_norm 1.6986 (1.6472)	mem 36269MB
Train: [0/180][443/5004]	eta 0:43:07 lr 0.100000	data 0.0007 (0.0077)	batch 0.5409 (0.5673)	loss 1.9464 (2.1198)	grad_norm 1.6087 (1.6471)	mem 36269MB
Train: [0/180][444/5004]	eta 0:43:06 lr 0.100000	data 0.0007 (0.0077)	batch 0.5665 (0.5673)	loss 2.3940 (2.1204)	grad_norm 1.7412 (1.6473)	mem 36269MB
Train: [0/180][445/5004]	eta 0:43:06 lr 0.100000	data 0.0007 (0.0077)	batch 0.5679 (0.5673)	loss 2.0973 (2.1204)	grad_norm 1.8379 (1.6477)	mem 36269MB
Train: [0/180][446/5004]	eta 0:43:05 lr 0.100000	data 0.0006 (0.0077)	batch 0.5575 (0.5672)	loss 2.1539 (2.1204)	grad_norm 1.5908 (1.6476)	mem 36269MB
Train: [0/180][447/5004]	eta 0:43:04 lr 0.100000	data 0.0006 (0.0077)	batch 0.5485 (0.5672)	loss 2.3203 (2.1209)	grad_norm 1.6800 (1.6477)	mem 36269MB
Train: [0/180][448/5004]	eta 0:43:04 lr 0.100000	data 0.0006 (0.0077)	batch 0.5576 (0.5672)	loss 2.2227 (2.1211)	grad_norm 1.6375 (1.6476)	mem 36269MB
Train: [0/180][449/5004]	eta 0:43:03 lr 0.100000	data 0.0008 (0.0076)	batch 0.5617 (0.5672)	loss 2.3107 (2.1215)	grad_norm 1.8577 (1.6481)	mem 36269MB
Train: [0/180][450/5004]	eta 0:43:02 lr 0.100000	data 0.0007 (0.0076)	batch 0.5655 (0.5672)	loss 2.2634 (2.1218)	grad_norm 1.7871 (1.6484)	mem 36269MB
Train: [0/180][451/5004]	eta 0:43:02 lr 0.100000	data 0.0006 (0.0076)	batch 0.5733 (0.5672)	loss 2.2413 (2.1221)	grad_norm 1.7801 (1.6487)	mem 36269MB
Train: [0/180][452/5004]	eta 0:43:01 lr 0.100000	data 0.0006 (0.0076)	batch 0.5618 (0.5672)	loss 2.3240 (2.1226)	grad_norm 1.6885 (1.6488)	mem 36269MB
Train: [0/180][453/5004]	eta 0:43:01 lr 0.100000	data 0.0006 (0.0076)	batch 0.5676 (0.5672)	loss 2.1737 (2.1227)	grad_norm 1.6936 (1.6489)	mem 36269MB
Train: [0/180][454/5004]	eta 0:43:00 lr 0.100000	data 0.0007 (0.0076)	batch 0.5422 (0.5671)	loss 2.0834 (2.1226)	grad_norm 1.6644 (1.6489)	mem 36269MB
Train: [0/180][455/5004]	eta 0:42:59 lr 0.100000	data 0.0007 (0.0076)	batch 0.5423 (0.5671)	loss 2.1686 (2.1227)	grad_norm 1.6047 (1.6488)	mem 36269MB
Train: [0/180][456/5004]	eta 0:42:59 lr 0.100000	data 0.0008 (0.0075)	batch 0.5697 (0.5671)	loss 2.1681 (2.1228)	grad_norm 1.7517 (1.6490)	mem 36269MB
Train: [0/180][457/5004]	eta 0:42:58 lr 0.100000	data 0.0007 (0.0075)	batch 0.5593 (0.5670)	loss 2.4145 (2.1234)	grad_norm 1.6791 (1.6491)	mem 36269MB
Train: [0/180][458/5004]	eta 0:42:57 lr 0.100000	data 0.0006 (0.0075)	batch 0.5568 (0.5670)	loss 2.3364 (2.1239)	grad_norm 1.7371 (1.6493)	mem 36269MB
Train: [0/180][459/5004]	eta 0:42:57 lr 0.100000	data 0.0007 (0.0075)	batch 0.5667 (0.5670)	loss 2.3020 (2.1243)	grad_norm 1.6000 (1.6492)	mem 36269MB
Train: [0/180][460/5004]	eta 0:42:56 lr 0.100000	data 0.0005 (0.0075)	batch 0.5492 (0.5670)	loss 2.1848 (2.1244)	grad_norm 1.6639 (1.6492)	mem 36269MB
Train: [0/180][461/5004]	eta 0:42:55 lr 0.100000	data 0.0005 (0.0075)	batch 0.5620 (0.5670)	loss 2.2583 (2.1247)	grad_norm 1.7013 (1.6493)	mem 36269MB
Train: [0/180][462/5004]	eta 0:42:55 lr 0.100000	data 0.0009 (0.0074)	batch 0.5518 (0.5669)	loss 2.2344 (2.1249)	grad_norm 1.7727 (1.6496)	mem 36269MB
Train: [0/180][463/5004]	eta 0:42:54 lr 0.100000	data 0.0007 (0.0074)	batch 0.5728 (0.5670)	loss 2.2745 (2.1253)	grad_norm 1.5275 (1.6493)	mem 36269MB
Train: [0/180][464/5004]	eta 0:42:53 lr 0.100000	data 0.0006 (0.0074)	batch 0.5645 (0.5669)	loss 2.2253 (2.1255)	grad_norm 1.7421 (1.6495)	mem 36269MB
Train: [0/180][465/5004]	eta 0:42:53 lr 0.100000	data 0.0006 (0.0074)	batch 0.5529 (0.5669)	loss 2.2240 (2.1257)	grad_norm 1.6949 (1.6496)	mem 36269MB
Train: [0/180][466/5004]	eta 0:42:52 lr 0.100000	data 0.0006 (0.0074)	batch 0.5675 (0.5669)	loss 2.3931 (2.1263)	grad_norm 1.7218 (1.6498)	mem 36269MB
Train: [0/180][467/5004]	eta 0:42:52 lr 0.100000	data 0.0008 (0.0074)	batch 0.5673 (0.5669)	loss 2.3138 (2.1267)	grad_norm 1.6461 (1.6498)	mem 36269MB
Train: [0/180][468/5004]	eta 0:42:51 lr 0.100000	data 0.0007 (0.0074)	batch 0.5468 (0.5669)	loss 2.2778 (2.1270)	grad_norm 1.5572 (1.6496)	mem 36269MB
Train: [0/180][469/5004]	eta 0:42:50 lr 0.100000	data 0.0006 (0.0073)	batch 0.5505 (0.5668)	loss 2.2832 (2.1273)	grad_norm 1.6720 (1.6496)	mem 36269MB
Train: [0/180][470/5004]	eta 0:42:49 lr 0.100000	data 0.0008 (0.0073)	batch 0.5598 (0.5668)	loss 2.1703 (2.1274)	grad_norm 1.4529 (1.6492)	mem 36269MB
Train: [0/180][471/5004]	eta 0:42:49 lr 0.100000	data 0.0007 (0.0073)	batch 0.5630 (0.5668)	loss 2.0855 (2.1273)	grad_norm 1.7757 (1.6495)	mem 36269MB
Train: [0/180][472/5004]	eta 0:42:48 lr 0.100000	data 0.0005 (0.0073)	batch 0.5618 (0.5668)	loss 2.2493 (2.1276)	grad_norm 1.7503 (1.6497)	mem 36269MB
Train: [0/180][473/5004]	eta 0:42:48 lr 0.100000	data 0.0006 (0.0073)	batch 0.5636 (0.5668)	loss 2.1722 (2.1277)	grad_norm 1.6969 (1.6498)	mem 36269MB
Train: [0/180][474/5004]	eta 0:42:47 lr 0.100000	data 0.0007 (0.0073)	batch 0.5603 (0.5668)	loss 2.2610 (2.1279)	grad_norm 1.6591 (1.6498)	mem 36269MB
Train: [0/180][475/5004]	eta 0:42:46 lr 0.100000	data 0.0005 (0.0073)	batch 0.5594 (0.5668)	loss 2.3521 (2.1284)	grad_norm 1.6073 (1.6497)	mem 36269MB
Train: [0/180][476/5004]	eta 0:42:46 lr 0.100000	data 0.0007 (0.0072)	batch 0.5489 (0.5667)	loss 2.3895 (2.1290)	grad_norm 1.8242 (1.6501)	mem 36269MB
Train: [0/180][477/5004]	eta 0:42:45 lr 0.100000	data 0.0007 (0.0072)	batch 0.5621 (0.5667)	loss 2.3493 (2.1294)	grad_norm 1.6702 (1.6501)	mem 36269MB
Train: [0/180][478/5004]	eta 0:42:44 lr 0.100000	data 0.0006 (0.0072)	batch 0.5447 (0.5667)	loss 2.3345 (2.1298)	grad_norm 1.7636 (1.6504)	mem 36269MB
Train: [0/180][479/5004]	eta 0:42:44 lr 0.100000	data 0.0007 (0.0072)	batch 0.5630 (0.5667)	loss 2.3074 (2.1302)	grad_norm 1.7535 (1.6506)	mem 36269MB
Train: [0/180][480/5004]	eta 0:42:43 lr 0.100000	data 0.0006 (0.0072)	batch 0.5612 (0.5667)	loss 2.3247 (2.1306)	grad_norm 1.8018 (1.6509)	mem 36269MB
Train: [0/180][481/5004]	eta 0:42:42 lr 0.100000	data 0.0006 (0.0072)	batch 0.5623 (0.5667)	loss 2.3083 (2.1310)	grad_norm 1.7232 (1.6511)	mem 36269MB
Train: [0/180][482/5004]	eta 0:42:42 lr 0.100000	data 0.0007 (0.0072)	batch 0.5655 (0.5667)	loss 2.2608 (2.1313)	grad_norm 1.7649 (1.6513)	mem 36269MB
Train: [0/180][483/5004]	eta 0:42:41 lr 0.100000	data 0.0006 (0.0072)	batch 0.5495 (0.5666)	loss 2.1576 (2.1313)	grad_norm 1.8339 (1.6517)	mem 36269MB
Train: [0/180][484/5004]	eta 0:42:40 lr 0.100000	data 0.0006 (0.0071)	batch 0.5490 (0.5666)	loss 2.2315 (2.1315)	grad_norm 1.6077 (1.6516)	mem 36269MB
Train: [0/180][485/5004]	eta 0:42:40 lr 0.100000	data 0.0008 (0.0071)	batch 0.5704 (0.5666)	loss 1.9305 (2.1311)	grad_norm 1.6231 (1.6515)	mem 36269MB
Train: [0/180][486/5004]	eta 0:42:39 lr 0.100000	data 0.0006 (0.0071)	batch 0.5592 (0.5666)	loss 2.1103 (2.1311)	grad_norm 1.6256 (1.6515)	mem 36269MB
Train: [0/180][487/5004]	eta 0:42:39 lr 0.100000	data 0.0005 (0.0071)	batch 0.5613 (0.5666)	loss 2.4384 (2.1317)	grad_norm 1.5996 (1.6514)	mem 36269MB
Train: [0/180][488/5004]	eta 0:42:38 lr 0.100000	data 0.0007 (0.0071)	batch 0.5606 (0.5665)	loss 2.1890 (2.1318)	grad_norm 1.6566 (1.6514)	mem 36269MB
Train: [0/180][489/5004]	eta 0:42:37 lr 0.100000	data 0.0007 (0.0071)	batch 0.5663 (0.5665)	loss 2.3676 (2.1323)	grad_norm 1.7011 (1.6515)	mem 36269MB
Train: [0/180][490/5004]	eta 0:42:37 lr 0.100000	data 0.0006 (0.0071)	batch 0.5475 (0.5665)	loss 2.1722 (2.1324)	grad_norm 1.7418 (1.6517)	mem 36269MB
Train: [0/180][491/5004]	eta 0:42:36 lr 0.100000	data 0.0007 (0.0070)	batch 0.5624 (0.5665)	loss 2.2596 (2.1326)	grad_norm 1.8426 (1.6520)	mem 36269MB
Train: [0/180][492/5004]	eta 0:42:36 lr 0.100000	data 0.0007 (0.0070)	batch 0.5657 (0.5665)	loss 2.1545 (2.1327)	grad_norm 1.6472 (1.6520)	mem 36269MB
Train: [0/180][493/5004]	eta 0:42:35 lr 0.100000	data 0.0006 (0.0070)	batch 0.5554 (0.5665)	loss 2.2367 (2.1329)	grad_norm 1.5710 (1.6519)	mem 36269MB
Train: [0/180][494/5004]	eta 0:42:34 lr 0.100000	data 0.0007 (0.0070)	batch 0.5625 (0.5665)	loss 2.0604 (2.1327)	grad_norm 1.5910 (1.6517)	mem 36269MB
Train: [0/180][495/5004]	eta 0:42:34 lr 0.100000	data 0.0008 (0.0070)	batch 0.5487 (0.5664)	loss 2.1563 (2.1328)	grad_norm 1.5592 (1.6516)	mem 36269MB
Train: [0/180][496/5004]	eta 0:42:33 lr 0.100000	data 0.0007 (0.0070)	batch 0.5652 (0.5664)	loss 2.0020 (2.1325)	grad_norm 1.6427 (1.6515)	mem 36269MB
Train: [0/180][497/5004]	eta 0:42:32 lr 0.100000	data 0.0006 (0.0070)	batch 0.5471 (0.5664)	loss 2.2986 (2.1329)	grad_norm 1.7094 (1.6517)	mem 36269MB
Train: [0/180][498/5004]	eta 0:42:32 lr 0.100000	data 0.0007 (0.0070)	batch 0.5490 (0.5664)	loss 2.1794 (2.1330)	grad_norm 1.7057 (1.6518)	mem 36269MB
Train: [0/180][499/5004]	eta 0:42:31 lr 0.100000	data 0.0008 (0.0069)	batch 0.5599 (0.5663)	loss 2.2435 (2.1332)	grad_norm 1.7207 (1.6519)	mem 36269MB
Train: [0/180][500/5004]	eta 0:42:30 lr 0.100000	data 0.0006 (0.0069)	batch 0.5479 (0.5663)	loss 2.2537 (2.1334)	grad_norm 1.6569 (1.6519)	mem 36269MB
Train: [0/180][501/5004]	eta 0:42:29 lr 0.100000	data 0.0006 (0.0069)	batch 0.5565 (0.5663)	loss 2.2635 (2.1337)	grad_norm 1.6255 (1.6519)	mem 36269MB
Train: [0/180][502/5004]	eta 0:42:29 lr 0.100000	data 0.0006 (0.0069)	batch 0.5570 (0.5663)	loss 2.3450 (2.1341)	grad_norm 1.6457 (1.6518)	mem 36269MB
Train: [0/180][503/5004]	eta 0:42:28 lr 0.100000	data 0.0006 (0.0069)	batch 0.5766 (0.5663)	loss 2.2024 (2.1342)	grad_norm 1.6503 (1.6518)	mem 36269MB
Train: [0/180][504/5004]	eta 0:42:28 lr 0.100000	data 0.0007 (0.0069)	batch 0.5690 (0.5663)	loss 2.0994 (2.1342)	grad_norm 1.6161 (1.6518)	mem 36269MB
Train: [0/180][505/5004]	eta 0:42:27 lr 0.100000	data 0.0006 (0.0069)	batch 0.5608 (0.5663)	loss 2.1613 (2.1342)	grad_norm 1.5669 (1.6516)	mem 36269MB
Train: [0/180][506/5004]	eta 0:42:26 lr 0.100000	data 0.0006 (0.0069)	batch 0.5436 (0.5662)	loss 2.1325 (2.1342)	grad_norm 1.6287 (1.6516)	mem 36269MB
Train: [0/180][507/5004]	eta 0:42:26 lr 0.100000	data 0.0006 (0.0068)	batch 0.5695 (0.5662)	loss 2.2071 (2.1344)	grad_norm 1.6875 (1.6516)	mem 36269MB
Train: [0/180][508/5004]	eta 0:42:25 lr 0.100000	data 0.0007 (0.0068)	batch 0.5628 (0.5662)	loss 2.4185 (2.1349)	grad_norm 1.7191 (1.6518)	mem 36269MB
Train: [0/180][509/5004]	eta 0:42:25 lr 0.100000	data 0.0007 (0.0068)	batch 0.5683 (0.5662)	loss 2.1796 (2.1350)	grad_norm 1.7551 (1.6520)	mem 36269MB
Train: [0/180][510/5004]	eta 0:42:24 lr 0.100000	data 0.0007 (0.0068)	batch 0.5644 (0.5662)	loss 2.3737 (2.1355)	grad_norm 1.6953 (1.6521)	mem 36269MB
Train: [0/180][511/5004]	eta 0:42:24 lr 0.100000	data 0.0007 (0.0068)	batch 0.5709 (0.5662)	loss 2.2332 (2.1357)	grad_norm 1.7536 (1.6523)	mem 36269MB
Train: [0/180][512/5004]	eta 0:42:23 lr 0.100000	data 0.0006 (0.0068)	batch 0.5702 (0.5663)	loss 1.9713 (2.1353)	grad_norm 1.6770 (1.6523)	mem 36269MB
Train: [0/180][513/5004]	eta 0:42:22 lr 0.100000	data 0.0007 (0.0068)	batch 0.5558 (0.5662)	loss 2.0025 (2.1351)	grad_norm 1.6112 (1.6522)	mem 36269MB
Train: [0/180][514/5004]	eta 0:42:22 lr 0.100000	data 0.0007 (0.0068)	batch 0.5604 (0.5662)	loss 2.1793 (2.1352)	grad_norm 1.7533 (1.6524)	mem 36269MB
Train: [0/180][515/5004]	eta 0:42:21 lr 0.100000	data 0.0006 (0.0068)	batch 0.5674 (0.5662)	loss 2.2544 (2.1354)	grad_norm 1.7409 (1.6526)	mem 36269MB
Train: [0/180][516/5004]	eta 0:42:21 lr 0.100000	data 0.0010 (0.0067)	batch 0.5704 (0.5662)	loss 2.2254 (2.1356)	grad_norm 1.6762 (1.6526)	mem 36269MB
Train: [0/180][517/5004]	eta 0:42:20 lr 0.100000	data 0.0007 (0.0067)	batch 0.5524 (0.5662)	loss 2.2034 (2.1357)	grad_norm 1.6634 (1.6527)	mem 36269MB
Train: [0/180][518/5004]	eta 0:42:19 lr 0.100000	data 0.0007 (0.0067)	batch 0.5526 (0.5662)	loss 2.2751 (2.1360)	grad_norm 1.6279 (1.6526)	mem 36269MB
Train: [0/180][519/5004]	eta 0:42:19 lr 0.100000	data 0.0006 (0.0067)	batch 0.5612 (0.5662)	loss 2.0649 (2.1358)	grad_norm 1.6659 (1.6526)	mem 36269MB
Train: [0/180][520/5004]	eta 0:42:18 lr 0.100000	data 0.0007 (0.0067)	batch 0.5675 (0.5662)	loss 2.2845 (2.1361)	grad_norm 1.5905 (1.6525)	mem 36269MB
Train: [0/180][521/5004]	eta 0:42:18 lr 0.100000	data 0.0007 (0.0067)	batch 0.5517 (0.5661)	loss 2.1649 (2.1362)	grad_norm 1.5647 (1.6523)	mem 36269MB
Train: [0/180][522/5004]	eta 0:42:17 lr 0.100000	data 0.0006 (0.0067)	batch 0.5457 (0.5661)	loss 2.1940 (2.1363)	grad_norm 1.6675 (1.6524)	mem 36269MB
Train: [0/180][523/5004]	eta 0:42:16 lr 0.100000	data 0.0006 (0.0067)	batch 0.5720 (0.5661)	loss 2.4777 (2.1369)	grad_norm 1.7359 (1.6525)	mem 36269MB
Train: [0/180][524/5004]	eta 0:42:16 lr 0.100000	data 0.0007 (0.0066)	batch 0.5485 (0.5661)	loss 2.2136 (2.1371)	grad_norm 1.7191 (1.6527)	mem 36269MB
Train: [0/180][525/5004]	eta 0:42:15 lr 0.100000	data 0.0006 (0.0066)	batch 0.5679 (0.5661)	loss 1.9695 (2.1368)	grad_norm 1.6572 (1.6527)	mem 36269MB
Train: [0/180][526/5004]	eta 0:42:14 lr 0.100000	data 0.0006 (0.0066)	batch 0.5531 (0.5661)	loss 2.2620 (2.1370)	grad_norm 1.5070 (1.6524)	mem 36269MB
Train: [0/180][527/5004]	eta 0:42:14 lr 0.100000	data 0.0006 (0.0066)	batch 0.5658 (0.5661)	loss 2.1332 (2.1370)	grad_norm 1.6186 (1.6523)	mem 36269MB
Train: [0/180][528/5004]	eta 0:42:13 lr 0.100000	data 0.0008 (0.0066)	batch 0.5473 (0.5660)	loss 2.0719 (2.1369)	grad_norm 1.5429 (1.6521)	mem 36269MB
Train: [0/180][529/5004]	eta 0:42:12 lr 0.100000	data 0.0008 (0.0066)	batch 0.5650 (0.5660)	loss 2.2639 (2.1371)	grad_norm 1.6815 (1.6522)	mem 36269MB
Train: [0/180][530/5004]	eta 0:42:12 lr 0.100000	data 0.0007 (0.0066)	batch 0.5620 (0.5660)	loss 2.1478 (2.1371)	grad_norm 1.6623 (1.6522)	mem 36269MB
Train: [0/180][531/5004]	eta 0:42:11 lr 0.100000	data 0.0007 (0.0066)	batch 0.5563 (0.5660)	loss 2.1799 (2.1372)	grad_norm 1.6766 (1.6522)	mem 36269MB
Train: [0/180][532/5004]	eta 0:42:11 lr 0.100000	data 0.0007 (0.0066)	batch 0.5519 (0.5660)	loss 2.2562 (2.1374)	grad_norm 1.6131 (1.6522)	mem 36269MB
Train: [0/180][533/5004]	eta 0:42:10 lr 0.100000	data 0.0006 (0.0065)	batch 0.5646 (0.5660)	loss 2.3214 (2.1378)	grad_norm 1.8047 (1.6525)	mem 36269MB
Train: [0/180][534/5004]	eta 0:42:09 lr 0.100000	data 0.0007 (0.0065)	batch 0.5636 (0.5660)	loss 2.1660 (2.1378)	grad_norm 1.5994 (1.6524)	mem 36269MB
Train: [0/180][535/5004]	eta 0:42:09 lr 0.100000	data 0.0007 (0.0065)	batch 0.5666 (0.5660)	loss 2.2227 (2.1380)	grad_norm 1.5754 (1.6522)	mem 36269MB
Train: [0/180][536/5004]	eta 0:42:08 lr 0.100000	data 0.0006 (0.0065)	batch 0.5427 (0.5659)	loss 2.3396 (2.1384)	grad_norm 1.6541 (1.6522)	mem 36269MB
Train: [0/180][537/5004]	eta 0:42:07 lr 0.100000	data 0.0006 (0.0065)	batch 0.5650 (0.5659)	loss 2.2536 (2.1386)	grad_norm 1.6147 (1.6521)	mem 36269MB
Train: [0/180][538/5004]	eta 0:42:07 lr 0.100000	data 0.0007 (0.0065)	batch 0.5693 (0.5659)	loss 2.2184 (2.1387)	grad_norm 1.7185 (1.6523)	mem 36269MB
Train: [0/180][539/5004]	eta 0:42:06 lr 0.100000	data 0.0006 (0.0065)	batch 0.5523 (0.5659)	loss 2.2498 (2.1389)	grad_norm 1.7430 (1.6524)	mem 36269MB
Train: [0/180][540/5004]	eta 0:42:06 lr 0.100000	data 0.0007 (0.0065)	batch 0.5675 (0.5659)	loss 2.0328 (2.1387)	grad_norm 1.6026 (1.6523)	mem 36269MB
Train: [0/180][541/5004]	eta 0:42:05 lr 0.100000	data 0.0007 (0.0065)	batch 0.5698 (0.5659)	loss 2.4605 (2.1393)	grad_norm 1.9156 (1.6528)	mem 36269MB
Train: [0/180][542/5004]	eta 0:42:05 lr 0.100000	data 0.0007 (0.0064)	batch 0.5577 (0.5659)	loss 2.2034 (2.1394)	grad_norm 1.5581 (1.6527)	mem 36269MB
Train: [0/180][543/5004]	eta 0:42:04 lr 0.100000	data 0.0006 (0.0064)	batch 0.5650 (0.5659)	loss 2.2866 (2.1397)	grad_norm 1.7786 (1.6529)	mem 36269MB
Train: [0/180][544/5004]	eta 0:42:03 lr 0.100000	data 0.0006 (0.0064)	batch 0.5485 (0.5659)	loss 2.3370 (2.1401)	grad_norm 1.6513 (1.6529)	mem 36269MB
Train: [0/180][545/5004]	eta 0:42:03 lr 0.100000	data 0.0007 (0.0064)	batch 0.5763 (0.5659)	loss 2.1706 (2.1401)	grad_norm 1.7009 (1.6530)	mem 36269MB
Train: [0/180][546/5004]	eta 0:42:02 lr 0.100000	data 0.0007 (0.0064)	batch 0.5423 (0.5658)	loss 2.2277 (2.1403)	grad_norm 1.7267 (1.6531)	mem 36269MB
Train: [0/180][547/5004]	eta 0:42:01 lr 0.100000	data 0.0006 (0.0064)	batch 0.5498 (0.5658)	loss 2.2949 (2.1406)	grad_norm 1.7086 (1.6532)	mem 36269MB
Train: [0/180][548/5004]	eta 0:42:01 lr 0.100000	data 0.0007 (0.0064)	batch 0.5480 (0.5658)	loss 2.0561 (2.1404)	grad_norm 1.5464 (1.6530)	mem 36269MB
Train: [0/180][549/5004]	eta 0:42:00 lr 0.100000	data 0.0007 (0.0064)	batch 0.5543 (0.5658)	loss 2.1097 (2.1404)	grad_norm 1.6262 (1.6530)	mem 36269MB
Train: [0/180][550/5004]	eta 0:41:59 lr 0.100000	data 0.0006 (0.0064)	batch 0.5511 (0.5657)	loss 2.1914 (2.1405)	grad_norm 1.5556 (1.6528)	mem 36269MB
Train: [0/180][551/5004]	eta 0:41:59 lr 0.100000	data 0.0007 (0.0064)	batch 0.5717 (0.5657)	loss 2.3187 (2.1408)	grad_norm 1.7211 (1.6529)	mem 36269MB
Train: [0/180][552/5004]	eta 0:41:58 lr 0.100000	data 0.0008 (0.0063)	batch 0.5825 (0.5658)	loss 2.0518 (2.1406)	grad_norm 1.4859 (1.6526)	mem 36269MB
Train: [0/180][553/5004]	eta 0:41:58 lr 0.100000	data 0.0007 (0.0063)	batch 0.5444 (0.5657)	loss 2.2290 (2.1408)	grad_norm 1.6435 (1.6526)	mem 36269MB
Train: [0/180][554/5004]	eta 0:41:57 lr 0.100000	data 0.0006 (0.0063)	batch 0.5435 (0.5657)	loss 2.5163 (2.1415)	grad_norm 1.5987 (1.6525)	mem 36269MB
Train: [0/180][555/5004]	eta 0:41:56 lr 0.100000	data 0.0007 (0.0063)	batch 0.5436 (0.5657)	loss 2.2347 (2.1416)	grad_norm 1.6600 (1.6525)	mem 36269MB
Train: [0/180][556/5004]	eta 0:41:56 lr 0.100000	data 0.0006 (0.0063)	batch 0.5633 (0.5657)	loss 2.3113 (2.1419)	grad_norm 1.6845 (1.6526)	mem 36269MB
Train: [0/180][557/5004]	eta 0:41:55 lr 0.100000	data 0.0008 (0.0063)	batch 0.5637 (0.5656)	loss 2.2678 (2.1422)	grad_norm 1.7105 (1.6527)	mem 36269MB
Train: [0/180][558/5004]	eta 0:41:54 lr 0.100000	data 0.0006 (0.0063)	batch 0.5603 (0.5656)	loss 2.0953 (2.1421)	grad_norm 1.6935 (1.6527)	mem 36269MB
Train: [0/180][559/5004]	eta 0:41:54 lr 0.100000	data 0.0006 (0.0063)	batch 0.5660 (0.5656)	loss 2.1696 (2.1421)	grad_norm 1.5912 (1.6526)	mem 36269MB
Train: [0/180][560/5004]	eta 0:41:53 lr 0.100000	data 0.0007 (0.0063)	batch 0.5700 (0.5656)	loss 2.3232 (2.1424)	grad_norm 1.7747 (1.6529)	mem 36269MB
Train: [0/180][561/5004]	eta 0:41:53 lr 0.100000	data 0.0007 (0.0063)	batch 0.5695 (0.5657)	loss 2.5896 (2.1432)	grad_norm 1.8891 (1.6533)	mem 36269MB
Train: [0/180][562/5004]	eta 0:41:52 lr 0.100000	data 0.0006 (0.0062)	batch 0.5641 (0.5657)	loss 2.2654 (2.1435)	grad_norm 1.6379 (1.6532)	mem 36269MB
Train: [0/180][563/5004]	eta 0:41:51 lr 0.100000	data 0.0007 (0.0062)	batch 0.5509 (0.5656)	loss 2.2881 (2.1437)	grad_norm 1.6732 (1.6533)	mem 36269MB
Train: [0/180][564/5004]	eta 0:41:51 lr 0.100000	data 0.0007 (0.0062)	batch 0.5484 (0.5656)	loss 2.2837 (2.1440)	grad_norm 1.7600 (1.6535)	mem 36269MB
Train: [0/180][565/5004]	eta 0:41:50 lr 0.100000	data 0.0007 (0.0062)	batch 0.5630 (0.5656)	loss 2.2215 (2.1441)	grad_norm 1.8268 (1.6538)	mem 36269MB
Train: [0/180][566/5004]	eta 0:41:50 lr 0.100000	data 0.0007 (0.0062)	batch 0.5652 (0.5656)	loss 2.1197 (2.1441)	grad_norm 1.5772 (1.6536)	mem 36269MB
Train: [0/180][567/5004]	eta 0:41:49 lr 0.100000	data 0.0007 (0.0062)	batch 0.5662 (0.5656)	loss 2.1051 (2.1440)	grad_norm 1.5111 (1.6534)	mem 36269MB
Train: [0/180][568/5004]	eta 0:41:48 lr 0.100000	data 0.0007 (0.0062)	batch 0.5478 (0.5656)	loss 2.1553 (2.1440)	grad_norm 1.5871 (1.6533)	mem 36269MB
Train: [0/180][569/5004]	eta 0:41:48 lr 0.100000	data 0.0008 (0.0062)	batch 0.5675 (0.5656)	loss 2.3123 (2.1443)	grad_norm 1.7624 (1.6535)	mem 36269MB
Train: [0/180][570/5004]	eta 0:41:47 lr 0.100000	data 0.0008 (0.0062)	batch 0.5703 (0.5656)	loss 2.2298 (2.1445)	grad_norm 1.8214 (1.6538)	mem 36269MB
Train: [0/180][571/5004]	eta 0:41:47 lr 0.100000	data 0.0010 (0.0062)	batch 0.5588 (0.5656)	loss 2.1761 (2.1445)	grad_norm 1.5149 (1.6535)	mem 36269MB
Train: [0/180][572/5004]	eta 0:41:46 lr 0.100000	data 0.0007 (0.0061)	batch 0.5495 (0.5655)	loss 2.1809 (2.1446)	grad_norm 1.6880 (1.6536)	mem 36269MB
Train: [0/180][573/5004]	eta 0:41:45 lr 0.100000	data 0.0007 (0.0061)	batch 0.5615 (0.5655)	loss 2.2809 (2.1448)	grad_norm 1.7276 (1.6537)	mem 36269MB
Train: [0/180][574/5004]	eta 0:41:45 lr 0.100000	data 0.0007 (0.0061)	batch 0.5661 (0.5655)	loss 2.3064 (2.1451)	grad_norm 1.5849 (1.6536)	mem 36269MB
Train: [0/180][575/5004]	eta 0:41:44 lr 0.100000	data 0.0007 (0.0061)	batch 0.5602 (0.5655)	loss 2.2218 (2.1452)	grad_norm 1.6543 (1.6536)	mem 36269MB
Train: [0/180][576/5004]	eta 0:41:44 lr 0.100000	data 0.0006 (0.0061)	batch 0.5529 (0.5655)	loss 2.0420 (2.1450)	grad_norm 1.6013 (1.6535)	mem 36269MB
Train: [0/180][577/5004]	eta 0:41:43 lr 0.100000	data 0.0007 (0.0061)	batch 0.5672 (0.5655)	loss 2.2370 (2.1452)	grad_norm 1.5612 (1.6533)	mem 36269MB
Train: [0/180][578/5004]	eta 0:41:42 lr 0.100000	data 0.0007 (0.0061)	batch 0.5506 (0.5655)	loss 2.1682 (2.1452)	grad_norm 1.7236 (1.6535)	mem 36269MB
Train: [0/180][579/5004]	eta 0:41:42 lr 0.100000	data 0.0007 (0.0061)	batch 0.5613 (0.5655)	loss 2.0950 (2.1452)	grad_norm 1.7470 (1.6536)	mem 36269MB
Train: [0/180][580/5004]	eta 0:41:41 lr 0.100000	data 0.0007 (0.0061)	batch 0.5669 (0.5655)	loss 2.1698 (2.1452)	grad_norm 1.6751 (1.6537)	mem 36269MB
Train: [0/180][581/5004]	eta 0:41:40 lr 0.100000	data 0.0007 (0.0061)	batch 0.5492 (0.5654)	loss 2.2041 (2.1453)	grad_norm 1.6324 (1.6536)	mem 36269MB
Train: [0/180][582/5004]	eta 0:41:40 lr 0.100000	data 0.0009 (0.0061)	batch 0.5671 (0.5654)	loss 2.3784 (2.1457)	grad_norm 1.6602 (1.6536)	mem 36269MB
Train: [0/180][583/5004]	eta 0:41:39 lr 0.100000	data 0.0007 (0.0060)	batch 0.5485 (0.5654)	loss 2.1367 (2.1457)	grad_norm 1.6564 (1.6536)	mem 36269MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::leaky_relu encountered 35 time(s)
Unsupported operator aten::sub encountered 35 time(s)
Unsupported operator aten::mul encountered 35 time(s)
Unsupported operator aten::add encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 6:36:20 lr 0.100000	data 1.6628 (1.6628)	batch 2.3759 (2.3759)	loss 2.1303 (2.1303)	grad_norm 3.3551 (3.3551)	mem 18117MB
Train: [0/180][1/10009]	eta 3:58:13 lr 0.100000	data 0.0008 (0.8318)	batch 0.4806 (1.4283)	loss 2.1383 (2.1343)	grad_norm 3.6150 (3.4850)	mem 18158MB
Train: [0/180][2/10009]	eta 2:53:22 lr 0.100000	data 0.0006 (0.5547)	batch 0.2621 (1.0395)	loss 2.1266 (2.1317)	grad_norm 3.3972 (3.4558)	mem 18158MB
Train: [0/180][3/10009]	eta 2:20:21 lr 0.100000	data 0.0007 (0.4162)	batch 0.2479 (0.8416)	loss 2.2790 (2.1686)	grad_norm 3.6724 (3.5099)	mem 18158MB
Train: [0/180][4/10009]	eta 2:00:29 lr 0.100000	data 0.0006 (0.3331)	batch 0.2466 (0.7226)	loss 2.3556 (2.2060)	grad_norm 4.1335 (3.6347)	mem 18158MB
Train: [0/180][5/10009]	eta 1:47:35 lr 0.100000	data 0.0006 (0.2777)	batch 0.2586 (0.6453)	loss 2.3842 (2.2357)	grad_norm 4.0518 (3.7042)	mem 18158MB
Train: [0/180][6/10009]	eta 1:38:41 lr 0.100000	data 0.0007 (0.2381)	batch 0.2718 (0.5919)	loss 2.4700 (2.2692)	grad_norm 4.0571 (3.7546)	mem 18158MB
Train: [0/180][7/10009]	eta 1:31:33 lr 0.100000	data 0.0007 (0.2084)	batch 0.2505 (0.5493)	loss 2.5620 (2.3058)	grad_norm 4.2025 (3.8106)	mem 18158MB
Train: [0/180][8/10009]	eta 1:26:28 lr 0.100000	data 0.0007 (0.1854)	batch 0.2749 (0.5188)	loss 2.8260 (2.3636)	grad_norm 4.7929 (3.9197)	mem 18158MB
Train: [0/180][9/10009]	eta 1:22:31 lr 0.100000	data 0.0006 (0.1669)	batch 0.2827 (0.4952)	loss 2.4491 (2.3721)	grad_norm 3.8885 (3.9166)	mem 18158MB
Train: [0/180][10/10009]	eta 1:18:56 lr 0.100000	data 0.0007 (0.1518)	batch 0.2589 (0.4737)	loss 2.9505 (2.4247)	grad_norm 4.4641 (3.9664)	mem 18158MB
Train: [0/180][11/10009]	eta 1:16:03 lr 0.100000	data 0.0095 (0.1399)	batch 0.2672 (0.4565)	loss 2.5667 (2.4365)	grad_norm 4.3877 (4.0015)	mem 18158MB
Train: [0/180][12/10009]	eta 1:13:25 lr 0.100000	data 0.0009 (0.1292)	batch 0.2511 (0.4407)	loss 3.0444 (2.4833)	grad_norm 4.8228 (4.0647)	mem 18158MB
Train: [0/180][13/10009]	eta 1:11:25 lr 0.100000	data 0.0007 (0.1200)	batch 0.2727 (0.4287)	loss 2.8625 (2.5104)	grad_norm 4.5645 (4.1004)	mem 18158MB
Train: [0/180][14/10009]	eta 1:09:32 lr 0.100000	data 0.0008 (0.1121)	batch 0.2600 (0.4174)	loss 3.0024 (2.5432)	grad_norm 4.7113 (4.1411)	mem 18158MB
Train: [0/180][15/10009]	eta 1:08:00 lr 0.100000	data 0.0008 (0.1051)	batch 0.2706 (0.4083)	loss 3.1779 (2.5829)	grad_norm 4.6592 (4.1735)	mem 18158MB
Train: [0/180][16/10009]	eta 1:06:39 lr 0.100000	data 0.0006 (0.0990)	batch 0.2717 (0.4002)	loss 3.0289 (2.6091)	grad_norm 4.7823 (4.2093)	mem 18158MB
Train: [0/180][17/10009]	eta 1:05:20 lr 0.100000	data 0.0006 (0.0935)	batch 0.2592 (0.3924)	loss 3.1265 (2.6378)	grad_norm 4.9458 (4.2502)	mem 18158MB
Train: [0/180][18/10009]	eta 1:04:11 lr 0.100000	data 0.0007 (0.0886)	batch 0.2611 (0.3855)	loss 3.4479 (2.6805)	grad_norm 4.3138 (4.2536)	mem 18158MB
Train: [0/180][19/10009]	eta 1:03:12 lr 0.100000	data 0.0010 (0.0843)	batch 0.2693 (0.3797)	loss 3.0088 (2.6969)	grad_norm 4.2601 (4.2539)	mem 18158MB
Train: [0/180][20/10009]	eta 1:02:16 lr 0.100000	data 0.0007 (0.0803)	batch 0.2611 (0.3740)	loss 3.0528 (2.7138)	grad_norm 4.4685 (4.2641)	mem 18158MB
Train: [0/180][21/10009]	eta 1:01:18 lr 0.100000	data 0.0006 (0.0767)	batch 0.2477 (0.3683)	loss 3.1747 (2.7348)	grad_norm 4.1721 (4.2599)	mem 18158MB
Train: [0/180][22/10009]	eta 1:00:29 lr 0.100000	data 0.0006 (0.0733)	batch 0.2574 (0.3635)	loss 3.4336 (2.7652)	grad_norm 4.5363 (4.2719)	mem 18158MB
Train: [0/180][23/10009]	eta 0:59:49 lr 0.100000	data 0.0008 (0.0703)	batch 0.2682 (0.3595)	loss 3.0193 (2.7757)	grad_norm 4.2619 (4.2715)	mem 18158MB
Train: [0/180][24/10009]	eta 0:59:07 lr 0.100000	data 0.0013 (0.0676)	batch 0.2553 (0.3553)	loss 3.2803 (2.7959)	grad_norm 4.3999 (4.2767)	mem 18158MB
Train: [0/180][25/10009]	eta 0:58:38 lr 0.100000	data 0.0008 (0.0650)	batch 0.2788 (0.3524)	loss 3.3956 (2.8190)	grad_norm 4.3430 (4.2792)	mem 18158MB
Train: [0/180][26/10009]	eta 0:58:01 lr 0.100000	data 0.0007 (0.0626)	batch 0.2545 (0.3488)	loss 3.4735 (2.8432)	grad_norm 4.5213 (4.2882)	mem 18158MB
Train: [0/180][27/10009]	eta 0:57:27 lr 0.100000	data 0.0006 (0.0604)	batch 0.2527 (0.3453)	loss 3.5574 (2.8687)	grad_norm 4.3765 (4.2913)	mem 18158MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108814
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 8:00:41 lr 0.100000	data 1.7095 (1.7095)	batch 2.8815 (2.8815)	loss 38.4826 (38.4826)	grad_norm 116.3161 (116.3161)	mem 26830MB
Train: [0/180][1/10009]	eta 4:45:29 lr 0.100000	data 0.0011 (0.8553)	batch 0.5416 (1.7116)	loss 6.1801 (22.3313)	grad_norm 5.1184 (60.7172)	mem 26880MB
Train: [0/180][2/10009]	eta 3:39:26 lr 0.100000	data 0.0007 (0.5705)	batch 0.5241 (1.3158)	loss 6.0561 (16.9062)	grad_norm 5.2501 (42.2282)	mem 26880MB
Train: [0/180][3/10009]	eta 3:07:53 lr 0.100000	data 0.0008 (0.4280)	batch 0.5596 (1.1267)	loss 5.8928 (14.1529)	grad_norm 4.8933 (32.8945)	mem 26880MB
Train: [0/180][4/10009]	eta 2:47:02 lr 0.100000	data 0.0007 (0.3426)	batch 0.5018 (1.0017)	loss 5.7609 (12.4745)	grad_norm 5.3573 (27.3870)	mem 26880MB
Train: [0/180][5/10009]	eta 2:33:13 lr 0.100000	data 0.0006 (0.2856)	batch 0.5049 (0.9189)	loss 5.6673 (11.3400)	grad_norm 5.3812 (23.7194)	mem 26880MB
Train: [0/180][6/10009]	eta 2:23:40 lr 0.100000	data 0.0007 (0.2449)	batch 0.5186 (0.8617)	loss 5.3636 (10.4862)	grad_norm 5.2312 (21.0782)	mem 26880MB
Train: [0/180][7/10009]	eta 2:16:23 lr 0.100000	data 0.0009 (0.2144)	batch 0.5133 (0.8182)	loss 5.6137 (9.8771)	grad_norm 5.2431 (19.0988)	mem 26880MB
Train: [0/180][8/10009]	eta 2:10:41 lr 0.100000	data 0.0007 (0.1906)	batch 0.5108 (0.7840)	loss 5.4341 (9.3835)	grad_norm 4.8830 (17.5193)	mem 26880MB
Train: [0/180][9/10009]	eta 2:06:18 lr 0.100000	data 0.0008 (0.1716)	batch 0.5226 (0.7579)	loss 5.1719 (8.9623)	grad_norm 4.7456 (16.2419)	mem 26880MB
Train: [0/180][10/10009]	eta 2:02:22 lr 0.100000	data 0.0006 (0.1561)	batch 0.4991 (0.7344)	loss 5.2339 (8.6234)	grad_norm 4.4306 (15.1682)	mem 26880MB
Train: [0/180][11/10009]	eta 1:59:11 lr 0.100000	data 0.0007 (0.1432)	batch 0.5051 (0.7153)	loss 5.1503 (8.3339)	grad_norm 4.4082 (14.2715)	mem 26880MB
Train: [0/180][12/10009]	eta 1:56:34 lr 0.100000	data 0.0008 (0.1322)	batch 0.5121 (0.6996)	loss 5.5441 (8.1193)	grad_norm 4.4985 (13.5197)	mem 26880MB
Train: [0/180][13/10009]	eta 1:54:19 lr 0.100000	data 0.0006 (0.1228)	batch 0.5114 (0.6862)	loss 5.2778 (7.9164)	grad_norm 4.3637 (12.8657)	mem 26880MB
Train: [0/180][14/10009]	eta 1:52:32 lr 0.100000	data 0.0007 (0.1147)	batch 0.5270 (0.6756)	loss 5.4669 (7.7531)	grad_norm 3.9221 (12.2695)	mem 26880MB
Train: [0/180][15/10009]	eta 1:50:43 lr 0.100000	data 0.0007 (0.1075)	batch 0.5030 (0.6648)	loss 5.4540 (7.6094)	grad_norm 4.1463 (11.7618)	mem 26880MB
Train: [0/180][16/10009]	eta 1:49:02 lr 0.100000	data 0.0006 (0.1013)	batch 0.4938 (0.6547)	loss 5.3993 (7.4794)	grad_norm 4.3265 (11.3244)	mem 26880MB
Train: [0/180][17/10009]	eta 1:47:48 lr 0.100000	data 0.0006 (0.0957)	batch 0.5231 (0.6474)	loss 5.4370 (7.3659)	grad_norm 3.7144 (10.9016)	mem 26880MB
Train: [0/180][18/10009]	eta 1:46:43 lr 0.100000	data 0.0007 (0.0907)	batch 0.5236 (0.6409)	loss 5.3720 (7.2610)	grad_norm 3.9124 (10.5338)	mem 26880MB
Train: [0/180][19/10009]	eta 1:45:34 lr 0.100000	data 0.0008 (0.0862)	batch 0.5054 (0.6341)	loss 5.1962 (7.1577)	grad_norm 3.9709 (10.2056)	mem 26880MB
Train: [0/180][20/10009]	eta 1:44:28 lr 0.100000	data 0.0007 (0.0821)	batch 0.4969 (0.6276)	loss 5.2362 (7.0662)	grad_norm 4.2818 (9.9235)	mem 26880MB
Train: [0/180][21/10009]	eta 1:43:41 lr 0.100000	data 0.0006 (0.0784)	batch 0.5237 (0.6229)	loss 5.2781 (6.9849)	grad_norm 4.0178 (9.6551)	mem 26880MB
Train: [0/180][22/10009]	eta 1:42:55 lr 0.100000	data 0.0007 (0.0750)	batch 0.5192 (0.6184)	loss 4.8437 (6.8919)	grad_norm 3.9408 (9.4067)	mem 26880MB
Train: [0/180][23/10009]	eta 1:42:20 lr 0.100000	data 0.0006 (0.0719)	batch 0.5367 (0.6150)	loss 5.2417 (6.8231)	grad_norm 4.1846 (9.1891)	mem 26880MB
Train: [0/180][24/10009]	eta 1:41:36 lr 0.100000	data 0.0007 (0.0691)	batch 0.5051 (0.6106)	loss 5.5322 (6.7715)	grad_norm 4.1228 (8.9864)	mem 26880MB
Train: [0/180][25/10009]	eta 1:40:58 lr 0.100000	data 0.0008 (0.0664)	batch 0.5128 (0.6068)	loss 5.3888 (6.7183)	grad_norm 3.9796 (8.7938)	mem 26880MB
Train: [0/180][26/10009]	eta 1:40:21 lr 0.100000	data 0.0006 (0.0640)	batch 0.5096 (0.6032)	loss 5.2192 (6.6628)	grad_norm 3.9334 (8.6138)	mem 26880MB
Train: [0/180][27/10009]	eta 1:39:47 lr 0.100000	data 0.0006 (0.0617)	batch 0.5080 (0.5998)	loss 5.2447 (6.6121)	grad_norm 3.8494 (8.4437)	mem 26880MB
Train: [0/180][28/10009]	eta 1:39:13 lr 0.100000	data 0.0007 (0.0596)	batch 0.5029 (0.5965)	loss 5.1335 (6.5611)	grad_norm 3.7010 (8.2801)	mem 26880MB
Train: [0/180][29/10009]	eta 1:39:25 lr 0.100000	data 0.0009 (0.0577)	batch 0.6366 (0.5978)	loss 5.0367 (6.5103)	grad_norm 3.7231 (8.1282)	mem 26880MB
Train: [0/180][30/10009]	eta 1:38:59 lr 0.100000	data 0.0007 (0.0558)	batch 0.5180 (0.5952)	loss 5.0994 (6.4648)	grad_norm 3.8339 (7.9897)	mem 26880MB
Train: [0/180][31/10009]	eta 1:38:29 lr 0.100000	data 0.0007 (0.0541)	batch 0.4989 (0.5922)	loss 5.2725 (6.4275)	grad_norm 3.8449 (7.8602)	mem 26880MB
Train: [0/180][32/10009]	eta 1:38:03 lr 0.100000	data 0.0005 (0.0525)	batch 0.5097 (0.5897)	loss 5.3100 (6.3937)	grad_norm 3.8340 (7.7382)	mem 26880MB
Train: [0/180][33/10009]	eta 1:37:36 lr 0.100000	data 0.0006 (0.0510)	batch 0.5013 (0.5871)	loss 5.3133 (6.3619)	grad_norm 3.6670 (7.6184)	mem 26880MB
Train: [0/180][34/10009]	eta 1:37:13 lr 0.100000	data 0.0006 (0.0495)	batch 0.5073 (0.5848)	loss 5.1056 (6.3260)	grad_norm 3.4776 (7.5001)	mem 26880MB
Train: [0/180][35/10009]	eta 1:36:52 lr 0.100000	data 0.0010 (0.0482)	batch 0.5100 (0.5827)	loss 4.9442 (6.2876)	grad_norm 3.3952 (7.3861)	mem 26880MB
Train: [0/180][36/10009]	eta 1:36:31 lr 0.100000	data 0.0008 (0.0469)	batch 0.5062 (0.5807)	loss 5.0658 (6.2546)	grad_norm 3.4885 (7.2808)	mem 26880MB
Train: [0/180][37/10009]	eta 1:36:12 lr 0.100000	data 0.0006 (0.0457)	batch 0.5125 (0.5789)	loss 5.0699 (6.2234)	grad_norm 3.4933 (7.1811)	mem 26880MB
Train: [0/180][38/10009]	eta 1:35:56 lr 0.100000	data 0.0008 (0.0445)	batch 0.5185 (0.5773)	loss 5.0403 (6.1931)	grad_norm 3.5092 (7.0869)	mem 26880MB
Train: [0/180][39/10009]	eta 1:35:36 lr 0.100000	data 0.0007 (0.0434)	batch 0.4972 (0.5753)	loss 5.2587 (6.1697)	grad_norm 3.6101 (7.0000)	mem 26880MB
Train: [0/180][40/10009]	eta 1:35:16 lr 0.100000	data 0.0006 (0.0424)	batch 0.4975 (0.5734)	loss 4.8046 (6.1364)	grad_norm 3.3556 (6.9111)	mem 26880MB
Train: [0/180][41/10009]	eta 1:35:00 lr 0.100000	data 0.0007 (0.0414)	batch 0.5078 (0.5719)	loss 5.1102 (6.1120)	grad_norm 3.6302 (6.8330)	mem 26880MB
Train: [0/180][42/10009]	eta 1:34:43 lr 0.100000	data 0.0006 (0.0404)	batch 0.5034 (0.5703)	loss 4.9809 (6.0857)	grad_norm 3.5417 (6.7565)	mem 26880MB
Train: [0/180][43/10009]	eta 1:34:30 lr 0.100000	data 0.0008 (0.0395)	batch 0.5149 (0.5690)	loss 4.8710 (6.0581)	grad_norm 3.4681 (6.6817)	mem 26880MB
Train: [0/180][44/10009]	eta 1:34:14 lr 0.100000	data 0.0007 (0.0387)	batch 0.4988 (0.5675)	loss 5.2035 (6.0391)	grad_norm 3.4929 (6.6109)	mem 26880MB
Train: [0/180][45/10009]	eta 1:34:07 lr 0.100000	data 0.0008 (0.0379)	batch 0.5353 (0.5668)	loss 5.1734 (6.0203)	grad_norm 3.2702 (6.5382)	mem 26880MB
Train: [0/180][46/10009]	eta 1:33:53 lr 0.100000	data 0.0007 (0.0371)	batch 0.5024 (0.5654)	loss 5.3847 (6.0067)	grad_norm 3.4608 (6.4728)	mem 26880MB
Train: [0/180][47/10009]	eta 1:33:40 lr 0.100000	data 0.0007 (0.0363)	batch 0.5076 (0.5642)	loss 4.7768 (5.9811)	grad_norm 3.4156 (6.4091)	mem 26880MB
Train: [0/180][48/10009]	eta 1:33:32 lr 0.100000	data 0.0006 (0.0356)	batch 0.5260 (0.5634)	loss 4.9543 (5.9602)	grad_norm 3.5574 (6.3509)	mem 26880MB
Train: [0/180][49/10009]	eta 1:33:18 lr 0.100000	data 0.0010 (0.0349)	batch 0.4964 (0.5621)	loss 5.1398 (5.9438)	grad_norm 3.3257 (6.2904)	mem 26880MB
Train: [0/180][50/10009]	eta 1:33:10 lr 0.100000	data 0.0007 (0.0342)	batch 0.5231 (0.5613)	loss 4.9630 (5.9245)	grad_norm 3.3049 (6.2318)	mem 26880MB
Train: [0/180][51/10009]	eta 1:33:04 lr 0.100000	data 0.0006 (0.0336)	batch 0.5363 (0.5608)	loss 5.0024 (5.9068)	grad_norm 3.2536 (6.1746)	mem 26880MB
Train: [0/180][52/10009]	eta 1:32:54 lr 0.100000	data 0.0007 (0.0330)	batch 0.5113 (0.5599)	loss 5.0268 (5.8902)	grad_norm 3.3495 (6.1213)	mem 26880MB
Train: [0/180][53/10009]	eta 1:32:46 lr 0.100000	data 0.0006 (0.0324)	batch 0.5189 (0.5591)	loss 5.0738 (5.8751)	grad_norm 3.4029 (6.0709)	mem 26880MB
Train: [0/180][54/10009]	eta 1:32:38 lr 0.100000	data 0.0007 (0.0318)	batch 0.5175 (0.5584)	loss 4.9165 (5.8576)	grad_norm 3.3851 (6.0221)	mem 26880MB
Train: [0/180][55/10009]	eta 1:32:28 lr 0.100000	data 0.0006 (0.0312)	batch 0.5071 (0.5575)	loss 4.9979 (5.8423)	grad_norm 3.3481 (5.9743)	mem 26880MB
Train: [0/180][56/10009]	eta 1:32:17 lr 0.100000	data 0.0007 (0.0307)	batch 0.4974 (0.5564)	loss 4.7842 (5.8237)	grad_norm 3.2997 (5.9274)	mem 26880MB
Train: [0/180][57/10009]	eta 1:32:13 lr 0.100000	data 0.0006 (0.0302)	batch 0.5325 (0.5560)	loss 5.0357 (5.8101)	grad_norm 3.2773 (5.8817)	mem 26880MB
Train: [0/180][58/10009]	eta 1:32:04 lr 0.100000	data 0.0007 (0.0297)	batch 0.5096 (0.5552)	loss 4.9302 (5.7952)	grad_norm 3.1899 (5.8361)	mem 26880MB
Train: [0/180][59/10009]	eta 1:31:59 lr 0.100000	data 0.0006 (0.0292)	batch 0.5260 (0.5547)	loss 4.9313 (5.7808)	grad_norm 3.2442 (5.7929)	mem 26880MB
Train: [0/180][60/10009]	eta 1:31:51 lr 0.100000	data 0.0007 (0.0287)	batch 0.5076 (0.5539)	loss 4.8264 (5.7652)	grad_norm 3.2245 (5.7508)	mem 26880MB
Train: [0/180][61/10009]	eta 1:31:43 lr 0.100000	data 0.0005 (0.0283)	batch 0.5074 (0.5532)	loss 5.0522 (5.7537)	grad_norm 3.4325 (5.7134)	mem 26880MB
Train: [0/180][62/10009]	eta 1:31:38 lr 0.100000	data 0.0008 (0.0278)	batch 0.5246 (0.5527)	loss 4.8760 (5.7398)	grad_norm 3.3409 (5.6757)	mem 26880MB
Train: [0/180][63/10009]	eta 1:31:31 lr 0.100000	data 0.0008 (0.0274)	batch 0.5154 (0.5522)	loss 5.3311 (5.7334)	grad_norm 3.3270 (5.6390)	mem 26880MB
Train: [0/180][64/10009]	eta 1:31:23 lr 0.100000	data 0.0008 (0.0270)	batch 0.5023 (0.5514)	loss 4.7830 (5.7187)	grad_norm 3.2790 (5.6027)	mem 26880MB
Train: [0/180][65/10009]	eta 1:31:18 lr 0.100000	data 0.0006 (0.0266)	batch 0.5236 (0.5510)	loss 4.6904 (5.7032)	grad_norm 3.1688 (5.5659)	mem 26880MB
Train: [0/180][66/10009]	eta 1:31:12 lr 0.100000	data 0.0007 (0.0262)	batch 0.5100 (0.5504)	loss 4.8329 (5.6902)	grad_norm 3.3389 (5.5326)	mem 26880MB
Train: [0/180][67/10009]	eta 1:31:08 lr 0.100000	data 0.0007 (0.0258)	batch 0.5272 (0.5500)	loss 5.0099 (5.6802)	grad_norm 3.4101 (5.5014)	mem 26880MB
Train: [0/180][68/10009]	eta 1:31:00 lr 0.100000	data 0.0008 (0.0255)	batch 0.4997 (0.5493)	loss 4.9686 (5.6699)	grad_norm 3.2223 (5.4684)	mem 26880MB
Train: [0/180][69/10009]	eta 1:30:53 lr 0.100000	data 0.0007 (0.0251)	batch 0.5014 (0.5486)	loss 4.8064 (5.6575)	grad_norm 3.2472 (5.4366)	mem 26880MB
Train: [0/180][70/10009]	eta 1:30:47 lr 0.100000	data 0.0009 (0.0248)	batch 0.5101 (0.5481)	loss 4.6055 (5.6427)	grad_norm 3.1263 (5.4041)	mem 26880MB
Train: [0/180][71/10009]	eta 1:30:40 lr 0.100000	data 0.0006 (0.0244)	batch 0.5018 (0.5474)	loss 4.9647 (5.6333)	grad_norm 3.1528 (5.3728)	mem 26880MB
Train: [0/180][72/10009]	eta 1:30:34 lr 0.100000	data 0.0007 (0.0241)	batch 0.5104 (0.5469)	loss 4.9615 (5.6241)	grad_norm 3.1530 (5.3424)	mem 26880MB
Train: [0/180][73/10009]	eta 1:30:28 lr 0.100000	data 0.0007 (0.0238)	batch 0.5035 (0.5463)	loss 4.8200 (5.6132)	grad_norm 3.1531 (5.3128)	mem 26880MB
Train: [0/180][74/10009]	eta 1:30:22 lr 0.100000	data 0.0007 (0.0235)	batch 0.5061 (0.5458)	loss 4.6628 (5.6005)	grad_norm 3.1987 (5.2847)	mem 26880MB
Train: [0/180][75/10009]	eta 1:30:16 lr 0.100000	data 0.0006 (0.0232)	batch 0.5035 (0.5452)	loss 4.8200 (5.5903)	grad_norm 3.1895 (5.2571)	mem 26880MB
Train: [0/180][76/10009]	eta 1:30:11 lr 0.100000	data 0.0007 (0.0229)	batch 0.5119 (0.5448)	loss 4.6811 (5.5785)	grad_norm 3.0813 (5.2288)	mem 26880MB
Train: [0/180][77/10009]	eta 1:30:05 lr 0.100000	data 0.0006 (0.0226)	batch 0.5009 (0.5442)	loss 4.7946 (5.5684)	grad_norm 3.1808 (5.2026)	mem 26880MB
Train: [0/180][78/10009]	eta 1:30:04 lr 0.100000	data 0.0007 (0.0223)	batch 0.5396 (0.5442)	loss 4.7292 (5.5578)	grad_norm 3.1928 (5.1771)	mem 26880MB
Train: [0/180][79/10009]	eta 1:30:01 lr 0.100000	data 0.0008 (0.0221)	batch 0.5223 (0.5439)	loss 4.9513 (5.5502)	grad_norm 3.2338 (5.1528)	mem 26880MB
Train: [0/180][80/10009]	eta 1:29:54 lr 0.100000	data 0.0006 (0.0218)	batch 0.4987 (0.5434)	loss 4.5961 (5.5384)	grad_norm 3.2690 (5.1296)	mem 26880MB
Train: [0/180][81/10009]	eta 1:29:49 lr 0.100000	data 0.0008 (0.0215)	batch 0.5029 (0.5429)	loss 4.7644 (5.5290)	grad_norm 3.3861 (5.1083)	mem 26880MB
Train: [0/180][82/10009]	eta 1:29:46 lr 0.100000	data 0.0007 (0.0213)	batch 0.5226 (0.5426)	loss 4.8621 (5.5210)	grad_norm 3.3225 (5.0868)	mem 26880MB
Train: [0/180][83/10009]	eta 1:29:40 lr 0.100000	data 0.0007 (0.0210)	batch 0.4982 (0.5421)	loss 4.9122 (5.5137)	grad_norm 3.2739 (5.0652)	mem 26880MB
Train: [0/180][84/10009]	eta 1:29:37 lr 0.100000	data 0.0005 (0.0208)	batch 0.5222 (0.5419)	loss 4.8928 (5.5064)	grad_norm 3.1635 (5.0428)	mem 26880MB
Train: [0/180][85/10009]	eta 1:29:33 lr 0.100000	data 0.0008 (0.0206)	batch 0.5072 (0.5414)	loss 4.7398 (5.4975)	grad_norm 3.1634 (5.0210)	mem 26880MB
Train: [0/180][86/10009]	eta 1:29:30 lr 0.100000	data 0.0006 (0.0203)	batch 0.5236 (0.5412)	loss 4.8276 (5.4898)	grad_norm 3.0479 (4.9983)	mem 26880MB
Train: [0/180][87/10009]	eta 1:29:27 lr 0.100000	data 0.0005 (0.0201)	batch 0.5164 (0.5410)	loss 4.7841 (5.4818)	grad_norm 3.1710 (4.9775)	mem 26880MB
Train: [0/180][88/10009]	eta 1:29:22 lr 0.100000	data 0.0008 (0.0199)	batch 0.5012 (0.5405)	loss 4.6477 (5.4724)	grad_norm 3.2229 (4.9578)	mem 26880MB
Train: [0/180][89/10009]	eta 1:29:17 lr 0.100000	data 0.0006 (0.0197)	batch 0.5005 (0.5401)	loss 4.6607 (5.4634)	grad_norm 3.1073 (4.9373)	mem 26880MB
Train: [0/180][90/10009]	eta 1:29:13 lr 0.100000	data 0.0006 (0.0195)	batch 0.5079 (0.5397)	loss 4.4928 (5.4527)	grad_norm 3.1312 (4.9174)	mem 26880MB
Train: [0/180][91/10009]	eta 1:29:09 lr 0.100000	data 0.0008 (0.0193)	batch 0.5072 (0.5394)	loss 4.6622 (5.4441)	grad_norm 3.1053 (4.8977)	mem 26880MB
Train: [0/180][92/10009]	eta 1:29:06 lr 0.100000	data 0.0006 (0.0191)	batch 0.5198 (0.5392)	loss 4.9046 (5.4383)	grad_norm 3.3044 (4.8806)	mem 26880MB
Train: [0/180][93/10009]	eta 1:29:01 lr 0.100000	data 0.0007 (0.0189)	batch 0.4987 (0.5387)	loss 4.8499 (5.4321)	grad_norm 3.2026 (4.8627)	mem 26880MB
Train: [0/180][94/10009]	eta 1:28:58 lr 0.100000	data 0.0008 (0.0187)	batch 0.5148 (0.5385)	loss 4.6214 (5.4235)	grad_norm 3.3200 (4.8465)	mem 26880MB
Train: [0/180][95/10009]	eta 1:28:57 lr 0.100000	data 0.0007 (0.0185)	batch 0.5283 (0.5384)	loss 4.9124 (5.4182)	grad_norm 3.1172 (4.8285)	mem 26880MB
Train: [0/180][96/10009]	eta 1:28:54 lr 0.100000	data 0.0006 (0.0183)	batch 0.5115 (0.5381)	loss 4.6399 (5.4102)	grad_norm 3.1241 (4.8109)	mem 26880MB
Train: [0/180][97/10009]	eta 1:28:51 lr 0.100000	data 0.0008 (0.0181)	batch 0.5142 (0.5378)	loss 4.7619 (5.4036)	grad_norm 3.0922 (4.7934)	mem 26880MB
Train: [0/180][98/10009]	eta 1:28:48 lr 0.100000	data 0.0009 (0.0180)	batch 0.5206 (0.5377)	loss 4.9843 (5.3993)	grad_norm 3.0541 (4.7758)	mem 26880MB
Train: [0/180][99/10009]	eta 1:28:44 lr 0.100000	data 0.0010 (0.0178)	batch 0.5025 (0.5373)	loss 4.5332 (5.3907)	grad_norm 3.0326 (4.7584)	mem 26880MB
Train: [0/180][100/10009]	eta 1:28:42 lr 0.100000	data 0.0007 (0.0176)	batch 0.5171 (0.5371)	loss 4.7202 (5.3840)	grad_norm 3.1729 (4.7427)	mem 26880MB
Train: [0/180][101/10009]	eta 1:28:38 lr 0.100000	data 0.0006 (0.0175)	batch 0.5062 (0.5368)	loss 4.6507 (5.3768)	grad_norm 3.0773 (4.7264)	mem 26880MB
Train: [0/180][102/10009]	eta 1:28:43 lr 0.100000	data 0.0008 (0.0173)	batch 0.5909 (0.5373)	loss 4.8027 (5.3713)	grad_norm 3.1057 (4.7106)	mem 26880MB
Train: [0/180][103/10009]	eta 1:28:40 lr 0.100000	data 0.0007 (0.0171)	batch 0.5165 (0.5371)	loss 4.7178 (5.3650)	grad_norm 3.0674 (4.6948)	mem 26880MB
Train: [0/180][104/10009]	eta 1:28:37 lr 0.100000	data 0.0007 (0.0170)	batch 0.5044 (0.5368)	loss 4.5596 (5.3573)	grad_norm 2.9449 (4.6782)	mem 26880MB
Train: [0/180][105/10009]	eta 1:28:34 lr 0.100000	data 0.0007 (0.0168)	batch 0.5083 (0.5366)	loss 4.7165 (5.3513)	grad_norm 3.1081 (4.6633)	mem 26880MB
Train: [0/180][106/10009]	eta 1:28:30 lr 0.100000	data 0.0005 (0.0167)	batch 0.5060 (0.5363)	loss 4.6515 (5.3447)	grad_norm 3.0565 (4.6483)	mem 26880MB
Train: [0/180][107/10009]	eta 1:28:27 lr 0.100000	data 0.0007 (0.0165)	batch 0.5116 (0.5360)	loss 4.6189 (5.3380)	grad_norm 2.9513 (4.6326)	mem 26880MB
Train: [0/180][108/10009]	eta 1:28:24 lr 0.100000	data 0.0007 (0.0164)	batch 0.5087 (0.5358)	loss 4.6422 (5.3316)	grad_norm 3.0986 (4.6185)	mem 26880MB
Train: [0/180][109/10009]	eta 1:28:22 lr 0.100000	data 0.0007 (0.0162)	batch 0.5121 (0.5356)	loss 4.5262 (5.3243)	grad_norm 3.1372 (4.6051)	mem 26880MB
Train: [0/180][110/10009]	eta 1:28:18 lr 0.100000	data 0.0009 (0.0161)	batch 0.5047 (0.5353)	loss 4.6569 (5.3183)	grad_norm 3.1042 (4.5916)	mem 26880MB
Train: [0/180][111/10009]	eta 1:28:17 lr 0.100000	data 0.0009 (0.0160)	batch 0.5210 (0.5352)	loss 4.5734 (5.3116)	grad_norm 3.0356 (4.5777)	mem 26880MB
Train: [0/180][112/10009]	eta 1:28:14 lr 0.100000	data 0.0007 (0.0158)	batch 0.5080 (0.5349)	loss 4.5983 (5.3053)	grad_norm 3.1966 (4.5654)	mem 26880MB
Train: [0/180][113/10009]	eta 1:28:11 lr 0.100000	data 0.0007 (0.0157)	batch 0.5075 (0.5347)	loss 4.5266 (5.2985)	grad_norm 3.0705 (4.5523)	mem 26880MB
Train: [0/180][114/10009]	eta 1:28:07 lr 0.100000	data 0.0007 (0.0156)	batch 0.5008 (0.5344)	loss 4.6058 (5.2925)	grad_norm 3.1931 (4.5405)	mem 26880MB
Train: [0/180][115/10009]	eta 1:28:05 lr 0.100000	data 0.0006 (0.0154)	batch 0.5087 (0.5342)	loss 4.5324 (5.2859)	grad_norm 3.1932 (4.5289)	mem 26880MB
Train: [0/180][116/10009]	eta 1:28:03 lr 0.100000	data 0.0006 (0.0153)	batch 0.5162 (0.5340)	loss 4.5186 (5.2794)	grad_norm 3.1168 (4.5168)	mem 26880MB
Train: [0/180][117/10009]	eta 1:27:59 lr 0.100000	data 0.0007 (0.0152)	batch 0.4996 (0.5337)	loss 4.8906 (5.2761)	grad_norm 3.2301 (4.5059)	mem 26880MB
Train: [0/180][118/10009]	eta 1:27:57 lr 0.100000	data 0.0006 (0.0151)	batch 0.5111 (0.5335)	loss 4.6386 (5.2707)	grad_norm 3.2351 (4.4952)	mem 26880MB
Train: [0/180][119/10009]	eta 1:27:55 lr 0.100000	data 0.0005 (0.0149)	batch 0.5156 (0.5334)	loss 4.5064 (5.2643)	grad_norm 3.1833 (4.4843)	mem 26880MB
Train: [0/180][120/10009]	eta 1:27:53 lr 0.100000	data 0.0007 (0.0148)	batch 0.5174 (0.5333)	loss 4.4851 (5.2579)	grad_norm 3.1140 (4.4730)	mem 26880MB
Train: [0/180][121/10009]	eta 1:27:52 lr 0.100000	data 0.0006 (0.0147)	batch 0.5273 (0.5332)	loss 4.8546 (5.2546)	grad_norm 3.3690 (4.4639)	mem 26880MB
Train: [0/180][122/10009]	eta 1:27:50 lr 0.100000	data 0.0007 (0.0146)	batch 0.5149 (0.5331)	loss 4.7536 (5.2505)	grad_norm 3.1442 (4.4532)	mem 26880MB
Train: [0/180][123/10009]	eta 1:27:47 lr 0.100000	data 0.0007 (0.0145)	batch 0.5050 (0.5328)	loss 4.7413 (5.2464)	grad_norm 3.0539 (4.4419)	mem 26880MB
Train: [0/180][124/10009]	eta 1:27:46 lr 0.100000	data 0.0008 (0.0144)	batch 0.5215 (0.5327)	loss 4.5057 (5.2405)	grad_norm 3.0746 (4.4310)	mem 26880MB
Train: [0/180][125/10009]	eta 1:27:43 lr 0.100000	data 0.0007 (0.0143)	batch 0.5011 (0.5325)	loss 4.8236 (5.2372)	grad_norm 3.1202 (4.4206)	mem 26880MB
Train: [0/180][126/10009]	eta 1:27:40 lr 0.100000	data 0.0007 (0.0142)	batch 0.5087 (0.5323)	loss 4.5115 (5.2315)	grad_norm 3.0407 (4.4097)	mem 26880MB
Train: [0/180][127/10009]	eta 1:27:39 lr 0.100000	data 0.0008 (0.0141)	batch 0.5192 (0.5322)	loss 4.5081 (5.2258)	grad_norm 2.9177 (4.3981)	mem 26880MB
Train: [0/180][128/10009]	eta 1:27:36 lr 0.100000	data 0.0007 (0.0139)	batch 0.5077 (0.5320)	loss 4.8170 (5.2226)	grad_norm 3.0631 (4.3877)	mem 26880MB
Train: [0/180][129/10009]	eta 1:27:35 lr 0.100000	data 0.0007 (0.0138)	batch 0.5167 (0.5319)	loss 4.6248 (5.2181)	grad_norm 3.0696 (4.3776)	mem 26880MB
Train: [0/180][130/10009]	eta 1:27:32 lr 0.100000	data 0.0009 (0.0137)	batch 0.5068 (0.5317)	loss 4.6154 (5.2134)	grad_norm 3.1133 (4.3679)	mem 26880MB
Train: [0/180][131/10009]	eta 1:27:30 lr 0.100000	data 0.0007 (0.0136)	batch 0.5145 (0.5316)	loss 4.4593 (5.2077)	grad_norm 2.9720 (4.3573)	mem 26880MB
Train: [0/180][132/10009]	eta 1:27:30 lr 0.100000	data 0.0008 (0.0136)	batch 0.5302 (0.5316)	loss 4.6370 (5.2034)	grad_norm 2.9767 (4.3470)	mem 26880MB
Train: [0/180][133/10009]	eta 1:27:27 lr 0.100000	data 0.0011 (0.0135)	batch 0.5080 (0.5314)	loss 4.9036 (5.2012)	grad_norm 3.1463 (4.3380)	mem 26880MB
Train: [0/180][134/10009]	eta 1:27:25 lr 0.100000	data 0.0007 (0.0134)	batch 0.5047 (0.5312)	loss 4.4131 (5.1954)	grad_norm 3.0823 (4.3287)	mem 26880MB
Train: [0/180][135/10009]	eta 1:27:24 lr 0.100000	data 0.0006 (0.0133)	batch 0.5211 (0.5311)	loss 4.7439 (5.1921)	grad_norm 3.1313 (4.3199)	mem 26880MB
Train: [0/180][136/10009]	eta 1:27:23 lr 0.100000	data 0.0008 (0.0132)	batch 0.5236 (0.5311)	loss 4.5940 (5.1877)	grad_norm 3.0358 (4.3105)	mem 26880MB
Train: [0/180][137/10009]	eta 1:27:21 lr 0.100000	data 0.0009 (0.0131)	batch 0.5143 (0.5309)	loss 4.4119 (5.1821)	grad_norm 3.0197 (4.3012)	mem 26880MB
Train: [0/180][138/10009]	eta 1:27:20 lr 0.100000	data 0.0006 (0.0130)	batch 0.5193 (0.5309)	loss 4.2398 (5.1753)	grad_norm 2.9842 (4.2917)	mem 26880MB
Train: [0/180][139/10009]	eta 1:27:17 lr 0.100000	data 0.0006 (0.0129)	batch 0.5082 (0.5307)	loss 4.3982 (5.1697)	grad_norm 2.9234 (4.2819)	mem 26880MB
Train: [0/180][140/10009]	eta 1:27:16 lr 0.100000	data 0.0008 (0.0128)	batch 0.5234 (0.5306)	loss 4.6229 (5.1659)	grad_norm 2.9923 (4.2728)	mem 26880MB
Train: [0/180][141/10009]	eta 1:27:15 lr 0.100000	data 0.0008 (0.0127)	batch 0.5203 (0.5306)	loss 4.5273 (5.1614)	grad_norm 3.0355 (4.2641)	mem 26880MB
Train: [0/180][142/10009]	eta 1:27:13 lr 0.100000	data 0.0007 (0.0127)	batch 0.5004 (0.5304)	loss 4.3278 (5.1555)	grad_norm 2.8939 (4.2545)	mem 26880MB
Train: [0/180][143/10009]	eta 1:27:12 lr 0.100000	data 0.0007 (0.0126)	batch 0.5255 (0.5303)	loss 4.4634 (5.1507)	grad_norm 2.8615 (4.2448)	mem 26880MB
Train: [0/180][144/10009]	eta 1:27:10 lr 0.100000	data 0.0006 (0.0125)	batch 0.5148 (0.5302)	loss 4.2994 (5.1449)	grad_norm 3.0252 (4.2364)	mem 26880MB
Train: [0/180][145/10009]	eta 1:27:08 lr 0.100000	data 0.0006 (0.0124)	batch 0.5044 (0.5300)	loss 4.7179 (5.1419)	grad_norm 3.0637 (4.2284)	mem 26880MB
Train: [0/180][146/10009]	eta 1:27:05 lr 0.100000	data 0.0007 (0.0123)	batch 0.5020 (0.5298)	loss 4.2785 (5.1361)	grad_norm 2.9756 (4.2198)	mem 26880MB
Train: [0/180][147/10009]	eta 1:27:04 lr 0.100000	data 0.0007 (0.0123)	batch 0.5180 (0.5298)	loss 4.3661 (5.1309)	grad_norm 2.9100 (4.2110)	mem 26880MB
Train: [0/180][148/10009]	eta 1:27:02 lr 0.100000	data 0.0006 (0.0122)	batch 0.5041 (0.5296)	loss 4.5254 (5.1268)	grad_norm 3.0195 (4.2030)	mem 26880MB
Train: [0/180][149/10009]	eta 1:26:59 lr 0.100000	data 0.0006 (0.0121)	batch 0.4988 (0.5294)	loss 4.7393 (5.1242)	grad_norm 2.9309 (4.1945)	mem 26880MB
Train: [0/180][150/10009]	eta 1:26:59 lr 0.100000	data 0.0006 (0.0120)	batch 0.5307 (0.5294)	loss 4.3066 (5.1188)	grad_norm 2.9107 (4.1860)	mem 26880MB
Train: [0/180][151/10009]	eta 1:26:57 lr 0.100000	data 0.0010 (0.0119)	batch 0.5124 (0.5293)	loss 4.3613 (5.1138)	grad_norm 3.1043 (4.1789)	mem 26880MB
Train: [0/180][152/10009]	eta 1:26:55 lr 0.100000	data 0.0006 (0.0119)	batch 0.5077 (0.5291)	loss 4.5137 (5.1099)	grad_norm 3.0349 (4.1714)	mem 26880MB
Train: [0/180][153/10009]	eta 1:26:53 lr 0.100000	data 0.0006 (0.0118)	batch 0.5012 (0.5290)	loss 4.5426 (5.1062)	grad_norm 3.0794 (4.1643)	mem 26880MB
Train: [0/180][154/10009]	eta 1:26:52 lr 0.100000	data 0.0005 (0.0117)	batch 0.5162 (0.5289)	loss 4.6036 (5.1030)	grad_norm 3.0289 (4.1570)	mem 26880MB
Train: [0/180][155/10009]	eta 1:26:49 lr 0.100000	data 0.0005 (0.0117)	batch 0.4941 (0.5287)	loss 4.7012 (5.1004)	grad_norm 3.0626 (4.1500)	mem 26880MB
Train: [0/180][156/10009]	eta 1:26:47 lr 0.100000	data 0.0005 (0.0116)	batch 0.5134 (0.5286)	loss 4.6003 (5.0972)	grad_norm 3.0999 (4.1433)	mem 26880MB
Train: [0/180][157/10009]	eta 1:26:45 lr 0.100000	data 0.0007 (0.0115)	batch 0.5008 (0.5284)	loss 4.2519 (5.0918)	grad_norm 3.0295 (4.1362)	mem 26880MB
Train: [0/180][158/10009]	eta 1:26:44 lr 0.100000	data 0.0006 (0.0114)	batch 0.5175 (0.5283)	loss 4.5247 (5.0883)	grad_norm 2.9789 (4.1290)	mem 26880MB
Train: [0/180][159/10009]	eta 1:26:42 lr 0.100000	data 0.0008 (0.0114)	batch 0.5017 (0.5282)	loss 4.5695 (5.0850)	grad_norm 3.0180 (4.1220)	mem 26880MB
Train: [0/180][160/10009]	eta 1:26:41 lr 0.100000	data 0.0006 (0.0113)	batch 0.5209 (0.5281)	loss 4.4942 (5.0814)	grad_norm 3.1954 (4.1163)	mem 26880MB
Train: [0/180][161/10009]	eta 1:26:40 lr 0.100000	data 0.0007 (0.0112)	batch 0.5180 (0.5280)	loss 4.8484 (5.0799)	grad_norm 3.1775 (4.1105)	mem 26880MB
Train: [0/180][162/10009]	eta 1:26:43 lr 0.100000	data 0.0007 (0.0112)	batch 0.5915 (0.5284)	loss 4.2947 (5.0751)	grad_norm 2.9460 (4.1033)	mem 26880MB
Train: [0/180][163/10009]	eta 1:26:41 lr 0.100000	data 0.0007 (0.0111)	batch 0.5071 (0.5283)	loss 4.5692 (5.0720)	grad_norm 3.0547 (4.0969)	mem 26880MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 6:32:50 lr 0.100000	data 2.6483 (2.6483)	batch 4.7104 (4.7104)	loss 26.7870 (26.7870)	grad_norm 75.7082 (75.7082)	mem 53669MB
Train: [0/180][1/5004]	eta 3:58:48 lr 0.100000	data 0.0010 (1.3246)	batch 1.0174 (2.8639)	loss 26.0718 (26.4294)	grad_norm 42.1981 (58.9531)	mem 53721MB
Train: [0/180][2/5004]	eta 3:10:13 lr 0.100000	data 0.0009 (0.8834)	batch 1.1176 (2.2818)	loss 24.7260 (25.8616)	grad_norm 49.2846 (55.7303)	mem 53721MB
Train: [0/180][3/5004]	eta 2:43:54 lr 0.100000	data 0.0007 (0.6627)	batch 1.0207 (1.9665)	loss 20.3973 (24.4955)	grad_norm 38.1991 (51.3475)	mem 53721MB
Train: [0/180][4/5004]	eta 2:27:58 lr 0.100000	data 0.0010 (0.5304)	batch 1.0127 (1.7758)	loss 19.6070 (23.5178)	grad_norm 37.1546 (48.5089)	mem 53721MB
Train: [0/180][5/5004]	eta 2:17:38 lr 0.100000	data 0.0007 (0.4421)	batch 1.0329 (1.6520)	loss 17.0637 (22.4421)	grad_norm 33.6581 (46.0338)	mem 53721MB
Train: [0/180][6/5004]	eta 2:10:08 lr 0.100000	data 0.0009 (0.3791)	batch 1.0249 (1.5624)	loss 17.2218 (21.6964)	grad_norm 35.3571 (44.5085)	mem 53721MB
Train: [0/180][7/5004]	eta 2:04:24 lr 0.100000	data 0.0006 (0.3318)	batch 1.0133 (1.4937)	loss 17.1536 (21.1285)	grad_norm 28.1474 (42.4634)	mem 53721MB
Train: [0/180][8/5004]	eta 2:00:05 lr 0.100000	data 0.0006 (0.2950)	batch 1.0308 (1.4423)	loss 16.4644 (20.6103)	grad_norm 22.5055 (40.2458)	mem 53721MB
Train: [0/180][9/5004]	eta 1:56:26 lr 0.100000	data 0.0011 (0.2656)	batch 1.0069 (1.3988)	loss 16.3300 (20.1823)	grad_norm 19.0142 (38.1227)	mem 53721MB
Train: [0/180][10/5004]	eta 1:53:26 lr 0.100000	data 0.0007 (0.2415)	batch 1.0055 (1.3630)	loss 16.4827 (19.8459)	grad_norm 23.2748 (36.7729)	mem 53721MB
Train: [0/180][11/5004]	eta 1:50:59 lr 0.100000	data 0.0006 (0.2214)	batch 1.0120 (1.3338)	loss 13.4829 (19.3157)	grad_norm 18.9777 (35.2899)	mem 53721MB
Train: [0/180][12/5004]	eta 1:48:57 lr 0.100000	data 0.0006 (0.2044)	batch 1.0191 (1.3096)	loss 18.0178 (19.2158)	grad_norm 32.8746 (35.1041)	mem 53721MB
Train: [0/180][13/5004]	eta 1:47:17 lr 0.100000	data 0.0006 (0.1899)	batch 1.0339 (1.2899)	loss 17.1563 (19.0687)	grad_norm 21.6292 (34.1416)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:11:00 lr 0.100000	data 3.0461 (3.0461)	batch 5.1679 (5.1679)	loss 1.8446 (1.8446)	grad_norm 0.9791 (0.9791)	mem 53669MB
Train: [0/180][1/5004]	eta 4:18:44 lr 0.100000	data 0.0009 (1.5235)	batch 1.0382 (3.1031)	loss 1.8111 (1.8279)	grad_norm 1.0292 (1.0041)	mem 53721MB
Train: [0/180][2/5004]	eta 3:21:30 lr 0.100000	data 0.0006 (1.0159)	batch 1.0454 (2.4172)	loss 1.7637 (1.8065)	grad_norm 0.9184 (0.9755)	mem 53721MB
Train: [0/180][3/5004]	eta 2:52:51 lr 0.100000	data 0.0007 (0.7621)	batch 1.0437 (2.0738)	loss 1.7455 (1.7912)	grad_norm 1.2495 (1.0440)	mem 53721MB
Train: [0/180][4/5004]	eta 2:35:56 lr 0.100000	data 0.0007 (0.6098)	batch 1.0615 (1.8714)	loss 2.0532 (1.8436)	grad_norm 1.3549 (1.1062)	mem 53721MB
Train: [0/180][5/5004]	eta 2:24:32 lr 0.100000	data 0.0007 (0.5083)	batch 1.0524 (1.7349)	loss 1.9323 (1.8584)	grad_norm 1.2029 (1.1223)	mem 53721MB
Train: [0/180][6/5004]	eta 2:16:19 lr 0.100000	data 0.0007 (0.4358)	batch 1.0466 (1.6365)	loss 1.8111 (1.8516)	grad_norm 1.1515 (1.1265)	mem 53721MB
Train: [0/180][7/5004]	eta 2:10:08 lr 0.100000	data 0.0007 (0.3814)	batch 1.0445 (1.5625)	loss 1.5530 (1.8143)	grad_norm 0.9719 (1.1072)	mem 53721MB
Train: [0/180][8/5004]	eta 2:05:13 lr 0.100000	data 0.0009 (0.3391)	batch 1.0349 (1.5039)	loss 1.8421 (1.8174)	grad_norm 1.2335 (1.1212)	mem 53721MB
Train: [0/180][9/5004]	eta 2:01:23 lr 0.100000	data 0.0006 (0.3053)	batch 1.0470 (1.4582)	loss 1.8431 (1.8200)	grad_norm 1.1076 (1.1198)	mem 53721MB
Train: [0/180][10/5004]	eta 1:58:13 lr 0.100000	data 0.0007 (0.2776)	batch 1.0420 (1.4204)	loss 1.7152 (1.8104)	grad_norm 1.0931 (1.1174)	mem 53721MB
Train: [0/180][11/5004]	eta 1:55:40 lr 0.100000	data 0.0007 (0.2545)	batch 1.0551 (1.3899)	loss 1.8882 (1.8169)	grad_norm 1.2270 (1.1265)	mem 53721MB
Train: [0/180][12/5004]	eta 1:53:23 lr 0.100000	data 0.0007 (0.2350)	batch 1.0377 (1.3628)	loss 1.7620 (1.8127)	grad_norm 1.2208 (1.1338)	mem 53721MB
Train: [0/180][13/5004]	eta 1:51:40 lr 0.100000	data 0.0008 (0.2182)	batch 1.0781 (1.3425)	loss 1.8854 (1.8179)	grad_norm 1.1717 (1.1365)	mem 53721MB
Train: [0/180][14/5004]	eta 1:50:06 lr 0.100000	data 0.0007 (0.2037)	batch 1.0638 (1.3239)	loss 1.8535 (1.8203)	grad_norm 1.4545 (1.1577)	mem 53721MB
Train: [0/180][15/5004]	eta 1:48:42 lr 0.100000	data 0.0007 (0.1910)	batch 1.0595 (1.3074)	loss 1.8518 (1.8222)	grad_norm 1.2281 (1.1621)	mem 53721MB
Train: [0/180][16/5004]	eta 1:47:23 lr 0.100000	data 0.0009 (0.1799)	batch 1.0408 (1.2917)	loss 1.8989 (1.8267)	grad_norm 1.2607 (1.1679)	mem 53721MB
Train: [0/180][17/5004]	eta 1:46:14 lr 0.100000	data 0.0007 (0.1699)	batch 1.0501 (1.2783)	loss 1.8326 (1.8271)	grad_norm 1.3523 (1.1781)	mem 53721MB
Train: [0/180][18/5004]	eta 1:45:09 lr 0.100000	data 0.0007 (0.1610)	batch 1.0330 (1.2654)	loss 1.8567 (1.8286)	grad_norm 1.3259 (1.1859)	mem 53721MB
Train: [0/180][19/5004]	eta 1:44:13 lr 0.100000	data 0.0006 (0.1530)	batch 1.0481 (1.2545)	loss 1.7675 (1.8256)	grad_norm 1.2047 (1.1869)	mem 53721MB
Train: [0/180][20/5004]	eta 1:43:20 lr 0.100000	data 0.0007 (0.1457)	batch 1.0369 (1.2442)	loss 1.9674 (1.8323)	grad_norm 1.3076 (1.1926)	mem 53721MB
Train: [0/180][21/5004]	eta 1:42:33 lr 0.100000	data 0.0005 (0.1391)	batch 1.0395 (1.2349)	loss 1.7045 (1.8265)	grad_norm 1.3784 (1.2011)	mem 53721MB
Train: [0/180][22/5004]	eta 1:42:14 lr 0.100000	data 0.0007 (0.1331)	batch 1.1536 (1.2313)	loss 1.9627 (1.8324)	grad_norm 1.4939 (1.2138)	mem 53721MB
Train: [0/180][23/5004]	eta 1:41:35 lr 0.100000	data 0.0007 (0.1276)	batch 1.0493 (1.2237)	loss 1.9564 (1.8376)	grad_norm 1.3369 (1.2189)	mem 53721MB
Train: [0/180][24/5004]	eta 1:40:59 lr 0.100000	data 0.0007 (0.1225)	batch 1.0505 (1.2168)	loss 1.8440 (1.8379)	grad_norm 1.1915 (1.2178)	mem 53721MB
Train: [0/180][25/5004]	eta 1:40:25 lr 0.100000	data 0.0007 (0.1178)	batch 1.0468 (1.2103)	loss 1.9286 (1.8413)	grad_norm 1.5864 (1.2320)	mem 53721MB
Train: [0/180][26/5004]	eta 1:39:53 lr 0.100000	data 0.0007 (0.1135)	batch 1.0405 (1.2040)	loss 1.8803 (1.8428)	grad_norm 1.6022 (1.2457)	mem 53721MB
Train: [0/180][27/5004]	eta 1:39:24 lr 0.100000	data 0.0006 (0.1095)	batch 1.0480 (1.1984)	loss 1.8438 (1.8428)	grad_norm 1.4007 (1.2512)	mem 53721MB
Train: [0/180][28/5004]	eta 1:38:57 lr 0.100000	data 0.0006 (0.1057)	batch 1.0459 (1.1932)	loss 2.0404 (1.8496)	grad_norm 1.3417 (1.2544)	mem 53721MB
Train: [0/180][29/5004]	eta 1:38:29 lr 0.100000	data 0.0005 (0.1022)	batch 1.0347 (1.1879)	loss 1.9134 (1.8518)	grad_norm 1.4487 (1.2608)	mem 53721MB
Train: [0/180][30/5004]	eta 1:38:08 lr 0.100000	data 0.0008 (0.0989)	batch 1.0642 (1.1839)	loss 1.7860 (1.8496)	grad_norm 1.3914 (1.2651)	mem 53721MB
Train: [0/180][31/5004]	eta 1:37:48 lr 0.100000	data 0.0011 (0.0959)	batch 1.0633 (1.1801)	loss 1.7027 (1.8451)	grad_norm 1.3945 (1.2691)	mem 53721MB
Train: [0/180][32/5004]	eta 1:37:27 lr 0.100000	data 0.0006 (0.0930)	batch 1.0463 (1.1761)	loss 1.8808 (1.8461)	grad_norm 1.3287 (1.2709)	mem 53721MB
Train: [0/180][33/5004]	eta 1:37:07 lr 0.100000	data 0.0007 (0.0903)	batch 1.0473 (1.1723)	loss 1.7717 (1.8439)	grad_norm 1.4550 (1.2763)	mem 53721MB
Train: [0/180][34/5004]	eta 1:36:46 lr 0.100000	data 0.0006 (0.0877)	batch 1.0368 (1.1684)	loss 1.9159 (1.8460)	grad_norm 1.4159 (1.2803)	mem 53721MB
Train: [0/180][35/5004]	eta 1:36:28 lr 0.100000	data 0.0007 (0.0853)	batch 1.0408 (1.1649)	loss 2.1761 (1.8552)	grad_norm 1.6362 (1.2902)	mem 53721MB
Train: [0/180][36/5004]	eta 1:36:10 lr 0.100000	data 0.0007 (0.0830)	batch 1.0422 (1.1615)	loss 1.8740 (1.8557)	grad_norm 1.4502 (1.2945)	mem 53721MB
Train: [0/180][37/5004]	eta 1:35:54 lr 0.100000	data 0.0007 (0.0808)	batch 1.0447 (1.1585)	loss 1.9620 (1.8585)	grad_norm 1.5858 (1.3022)	mem 53721MB
Train: [0/180][38/5004]	eta 1:35:40 lr 0.100000	data 0.0008 (0.0788)	batch 1.0593 (1.1559)	loss 1.6959 (1.8543)	grad_norm 1.5449 (1.3084)	mem 53721MB
Train: [0/180][39/5004]	eta 1:35:26 lr 0.100000	data 0.0006 (0.0768)	batch 1.0511 (1.1533)	loss 2.0172 (1.8584)	grad_norm 1.6251 (1.3163)	mem 53721MB
Train: [0/180][40/5004]	eta 1:35:10 lr 0.100000	data 0.0007 (0.0750)	batch 1.0346 (1.1504)	loss 2.1075 (1.8645)	grad_norm 1.6133 (1.3236)	mem 53721MB
Train: [0/180][41/5004]	eta 1:34:57 lr 0.100000	data 0.0007 (0.0732)	batch 1.0510 (1.1480)	loss 2.2155 (1.8728)	grad_norm 1.5726 (1.3295)	mem 53721MB
Train: [0/180][42/5004]	eta 1:34:46 lr 0.100000	data 0.0007 (0.0715)	batch 1.0625 (1.1461)	loss 1.7556 (1.8701)	grad_norm 1.4877 (1.3332)	mem 53721MB
Train: [0/180][43/5004]	eta 1:34:33 lr 0.100000	data 0.0008 (0.0699)	batch 1.0395 (1.1436)	loss 2.0692 (1.8746)	grad_norm 1.4787 (1.3365)	mem 53721MB
Train: [0/180][44/5004]	eta 1:34:23 lr 0.100000	data 0.0008 (0.0684)	batch 1.0634 (1.1419)	loss 1.9373 (1.8760)	grad_norm 1.5285 (1.3408)	mem 53721MB
Train: [0/180][45/5004]	eta 1:34:12 lr 0.100000	data 0.0011 (0.0669)	batch 1.0527 (1.1399)	loss 1.9097 (1.8767)	grad_norm 1.5173 (1.3446)	mem 53721MB
Train: [0/180][46/5004]	eta 1:34:02 lr 0.100000	data 0.0007 (0.0655)	batch 1.0503 (1.1380)	loss 2.0011 (1.8794)	grad_norm 1.4401 (1.3466)	mem 53721MB
Train: [0/180][47/5004]	eta 1:33:51 lr 0.100000	data 0.0007 (0.0642)	batch 1.0434 (1.1360)	loss 1.8003 (1.8777)	grad_norm 1.5218 (1.3503)	mem 53721MB
Train: [0/180][48/5004]	eta 1:33:41 lr 0.100000	data 0.0007 (0.0629)	batch 1.0538 (1.1344)	loss 1.8875 (1.8779)	grad_norm 1.5327 (1.3540)	mem 53721MB
Train: [0/180][49/5004]	eta 1:33:31 lr 0.100000	data 0.0006 (0.0616)	batch 1.0435 (1.1325)	loss 2.0052 (1.8805)	grad_norm 1.4451 (1.3558)	mem 53721MB
Train: [0/180][50/5004]	eta 1:33:21 lr 0.100000	data 0.0006 (0.0604)	batch 1.0401 (1.1307)	loss 1.9317 (1.8815)	grad_norm 1.6638 (1.3619)	mem 53721MB
Train: [0/180][51/5004]	eta 1:33:15 lr 0.100000	data 0.0006 (0.0593)	batch 1.0755 (1.1297)	loss 1.9679 (1.8831)	grad_norm 1.3637 (1.3619)	mem 53721MB
Train: [0/180][52/5004]	eta 1:33:06 lr 0.100000	data 0.0006 (0.0582)	batch 1.0475 (1.1281)	loss 1.9905 (1.8852)	grad_norm 1.5498 (1.3654)	mem 53721MB
Train: [0/180][53/5004]	eta 1:32:57 lr 0.100000	data 0.0006 (0.0571)	batch 1.0453 (1.1266)	loss 1.9438 (1.8863)	grad_norm 1.5781 (1.3694)	mem 53721MB
Train: [0/180][54/5004]	eta 1:32:49 lr 0.100000	data 0.0007 (0.0561)	batch 1.0518 (1.1252)	loss 2.1772 (1.8916)	grad_norm 1.6903 (1.3752)	mem 53721MB
Train: [0/180][55/5004]	eta 1:32:40 lr 0.100000	data 0.0007 (0.0551)	batch 1.0332 (1.1236)	loss 1.9701 (1.8930)	grad_norm 1.4306 (1.3762)	mem 53721MB
Train: [0/180][56/5004]	eta 1:32:32 lr 0.100000	data 0.0007 (0.0541)	batch 1.0450 (1.1222)	loss 1.9481 (1.8939)	grad_norm 1.6589 (1.3812)	mem 53721MB
Train: [0/180][57/5004]	eta 1:32:24 lr 0.100000	data 0.0006 (0.0532)	batch 1.0370 (1.1207)	loss 1.7204 (1.8909)	grad_norm 1.5728 (1.3845)	mem 53721MB
Train: [0/180][58/5004]	eta 1:32:17 lr 0.100000	data 0.0006 (0.0523)	batch 1.0478 (1.1195)	loss 2.0414 (1.8935)	grad_norm 1.5557 (1.3874)	mem 53721MB
Train: [0/180][59/5004]	eta 1:32:10 lr 0.100000	data 0.0007 (0.0515)	batch 1.0511 (1.1184)	loss 1.9535 (1.8945)	grad_norm 1.4951 (1.3892)	mem 53721MB
Train: [0/180][60/5004]	eta 1:32:02 lr 0.100000	data 0.0007 (0.0506)	batch 1.0372 (1.1170)	loss 2.1084 (1.8980)	grad_norm 1.7064 (1.3944)	mem 53721MB
Train: [0/180][61/5004]	eta 1:31:55 lr 0.100000	data 0.0006 (0.0498)	batch 1.0449 (1.1159)	loss 2.0288 (1.9001)	grad_norm 1.6091 (1.3978)	mem 53721MB
Train: [0/180][62/5004]	eta 1:31:49 lr 0.100000	data 0.0006 (0.0490)	batch 1.0461 (1.1148)	loss 2.0965 (1.9032)	grad_norm 1.7655 (1.4037)	mem 53721MB
Train: [0/180][63/5004]	eta 1:31:42 lr 0.100000	data 0.0008 (0.0483)	batch 1.0476 (1.1137)	loss 1.9305 (1.9036)	grad_norm 1.4295 (1.4041)	mem 53721MB
Train: [0/180][64/5004]	eta 1:31:37 lr 0.100000	data 0.0008 (0.0475)	batch 1.0550 (1.1128)	loss 1.9494 (1.9043)	grad_norm 1.4635 (1.4050)	mem 53721MB
Train: [0/180][65/5004]	eta 1:31:30 lr 0.100000	data 0.0007 (0.0468)	batch 1.0360 (1.1116)	loss 1.8111 (1.9029)	grad_norm 1.5379 (1.4070)	mem 53721MB
Train: [0/180][66/5004]	eta 1:31:23 lr 0.100000	data 0.0008 (0.0461)	batch 1.0333 (1.1105)	loss 1.8875 (1.9027)	grad_norm 1.4499 (1.4076)	mem 53721MB
Train: [0/180][67/5004]	eta 1:31:17 lr 0.100000	data 0.0007 (0.0455)	batch 1.0462 (1.1095)	loss 2.0501 (1.9049)	grad_norm 1.5041 (1.4091)	mem 53721MB
Train: [0/180][68/5004]	eta 1:31:11 lr 0.100000	data 0.0006 (0.0448)	batch 1.0365 (1.1085)	loss 2.1378 (1.9082)	grad_norm 1.6212 (1.4121)	mem 53721MB
Train: [0/180][69/5004]	eta 1:31:06 lr 0.100000	data 0.0006 (0.0442)	batch 1.0556 (1.1077)	loss 1.8034 (1.9067)	grad_norm 1.6248 (1.4152)	mem 53721MB
Train: [0/180][70/5004]	eta 1:31:01 lr 0.100000	data 0.0007 (0.0436)	batch 1.0511 (1.1069)	loss 1.9812 (1.9078)	grad_norm 1.7253 (1.4195)	mem 53721MB
Train: [0/180][71/5004]	eta 1:30:56 lr 0.100000	data 0.0007 (0.0430)	batch 1.0499 (1.1061)	loss 2.1184 (1.9107)	grad_norm 1.8479 (1.4255)	mem 53721MB
Train: [0/180][72/5004]	eta 1:30:51 lr 0.100000	data 0.0007 (0.0424)	batch 1.0471 (1.1053)	loss 2.1683 (1.9142)	grad_norm 1.7566 (1.4300)	mem 53721MB
Train: [0/180][73/5004]	eta 1:30:46 lr 0.100000	data 0.0007 (0.0418)	batch 1.0443 (1.1045)	loss 2.0006 (1.9154)	grad_norm 1.5470 (1.4316)	mem 53721MB
Train: [0/180][74/5004]	eta 1:30:41 lr 0.100000	data 0.0007 (0.0413)	batch 1.0472 (1.1037)	loss 2.0569 (1.9173)	grad_norm 1.5656 (1.4334)	mem 53721MB
Train: [0/180][75/5004]	eta 1:30:37 lr 0.100000	data 0.0006 (0.0408)	batch 1.0565 (1.1031)	loss 2.0815 (1.9195)	grad_norm 1.6853 (1.4367)	mem 53721MB
Train: [0/180][76/5004]	eta 1:30:31 lr 0.100000	data 0.0007 (0.0402)	batch 1.0384 (1.1023)	loss 1.9666 (1.9201)	grad_norm 1.4785 (1.4372)	mem 53721MB
Train: [0/180][77/5004]	eta 1:30:27 lr 0.100000	data 0.0010 (0.0397)	batch 1.0480 (1.1016)	loss 1.9402 (1.9203)	grad_norm 1.7706 (1.4415)	mem 53721MB
Train: [0/180][78/5004]	eta 1:30:22 lr 0.100000	data 0.0007 (0.0392)	batch 1.0452 (1.1009)	loss 1.8863 (1.9199)	grad_norm 1.5642 (1.4431)	mem 53721MB
Train: [0/180][79/5004]	eta 1:30:17 lr 0.100000	data 0.0007 (0.0388)	batch 1.0333 (1.1000)	loss 2.0815 (1.9219)	grad_norm 1.6484 (1.4456)	mem 53721MB
Train: [0/180][80/5004]	eta 1:30:15 lr 0.100000	data 0.0008 (0.0383)	batch 1.0765 (1.0997)	loss 2.1148 (1.9243)	grad_norm 1.6779 (1.4485)	mem 53721MB
Train: [0/180][81/5004]	eta 1:30:10 lr 0.100000	data 0.0009 (0.0378)	batch 1.0497 (1.0991)	loss 1.9374 (1.9245)	grad_norm 1.7479 (1.4522)	mem 53721MB
Train: [0/180][82/5004]	eta 1:30:06 lr 0.100000	data 0.0006 (0.0374)	batch 1.0407 (1.0984)	loss 2.1150 (1.9268)	grad_norm 1.6287 (1.4543)	mem 53721MB
Train: [0/180][83/5004]	eta 1:30:01 lr 0.100000	data 0.0007 (0.0370)	batch 1.0361 (1.0977)	loss 1.8379 (1.9257)	grad_norm 1.5525 (1.4555)	mem 53721MB
Train: [0/180][84/5004]	eta 1:29:57 lr 0.100000	data 0.0007 (0.0365)	batch 1.0447 (1.0970)	loss 2.1345 (1.9282)	grad_norm 1.5935 (1.4571)	mem 53721MB
Train: [0/180][85/5004]	eta 1:29:53 lr 0.100000	data 0.0006 (0.0361)	batch 1.0456 (1.0964)	loss 2.1573 (1.9308)	grad_norm 1.6217 (1.4590)	mem 53721MB
Train: [0/180][86/5004]	eta 1:29:49 lr 0.100000	data 0.0007 (0.0357)	batch 1.0523 (1.0959)	loss 2.2273 (1.9342)	grad_norm 1.5797 (1.4604)	mem 53721MB
Train: [0/180][87/5004]	eta 1:29:45 lr 0.100000	data 0.0006 (0.0353)	batch 1.0409 (1.0953)	loss 2.0562 (1.9356)	grad_norm 1.6522 (1.4626)	mem 53721MB
Train: [0/180][88/5004]	eta 1:29:41 lr 0.100000	data 0.0007 (0.0349)	batch 1.0327 (1.0946)	loss 2.2324 (1.9390)	grad_norm 1.6113 (1.4642)	mem 53721MB
Train: [0/180][89/5004]	eta 1:29:37 lr 0.100000	data 0.0009 (0.0345)	batch 1.0407 (1.0940)	loss 2.1190 (1.9410)	grad_norm 1.6391 (1.4662)	mem 53721MB
Train: [0/180][90/5004]	eta 1:29:34 lr 0.100000	data 0.0007 (0.0342)	batch 1.0597 (1.0936)	loss 1.9743 (1.9413)	grad_norm 1.5005 (1.4665)	mem 53721MB
Train: [0/180][91/5004]	eta 1:29:34 lr 0.100000	data 0.0007 (0.0338)	batch 1.1297 (1.0940)	loss 2.0207 (1.9422)	grad_norm 1.5727 (1.4677)	mem 53721MB
Train: [0/180][92/5004]	eta 1:29:31 lr 0.100000	data 0.0008 (0.0334)	batch 1.0431 (1.0935)	loss 2.0519 (1.9434)	grad_norm 1.5618 (1.4687)	mem 53721MB
Train: [0/180][93/5004]	eta 1:29:26 lr 0.100000	data 0.0006 (0.0331)	batch 1.0336 (1.0928)	loss 1.9618 (1.9436)	grad_norm 1.6468 (1.4706)	mem 53721MB
Train: [0/180][94/5004]	eta 1:29:23 lr 0.100000	data 0.0005 (0.0328)	batch 1.0485 (1.0924)	loss 2.1747 (1.9460)	grad_norm 1.5628 (1.4716)	mem 53721MB
Train: [0/180][95/5004]	eta 1:29:19 lr 0.100000	data 0.0006 (0.0324)	batch 1.0413 (1.0918)	loss 2.2005 (1.9486)	grad_norm 1.7258 (1.4742)	mem 53721MB
Train: [0/180][96/5004]	eta 1:29:15 lr 0.100000	data 0.0007 (0.0321)	batch 1.0350 (1.0913)	loss 2.1880 (1.9511)	grad_norm 1.6114 (1.4756)	mem 53721MB
Train: [0/180][97/5004]	eta 1:29:13 lr 0.100000	data 0.0007 (0.0318)	batch 1.0635 (1.0910)	loss 1.7594 (1.9492)	grad_norm 1.4208 (1.4751)	mem 53721MB
Train: [0/180][98/5004]	eta 1:29:10 lr 0.100000	data 0.0012 (0.0315)	batch 1.0589 (1.0906)	loss 2.1488 (1.9512)	grad_norm 1.6451 (1.4768)	mem 53721MB
Train: [0/180][99/5004]	eta 1:29:06 lr 0.100000	data 0.0006 (0.0312)	batch 1.0335 (1.0901)	loss 1.9608 (1.9513)	grad_norm 1.5805 (1.4778)	mem 53721MB
Train: [0/180][100/5004]	eta 1:29:03 lr 0.100000	data 0.0005 (0.0309)	batch 1.0454 (1.0896)	loss 1.9216 (1.9510)	grad_norm 1.4506 (1.4776)	mem 53721MB
Train: [0/180][101/5004]	eta 1:29:00 lr 0.100000	data 0.0007 (0.0306)	batch 1.0383 (1.0891)	loss 2.0735 (1.9522)	grad_norm 1.5203 (1.4780)	mem 53721MB
Train: [0/180][102/5004]	eta 1:28:56 lr 0.100000	data 0.0007 (0.0303)	batch 1.0430 (1.0887)	loss 2.0624 (1.9532)	grad_norm 1.7304 (1.4804)	mem 53721MB
Train: [0/180][103/5004]	eta 1:28:54 lr 0.100000	data 0.0008 (0.0300)	batch 1.0545 (1.0884)	loss 1.7692 (1.9515)	grad_norm 1.5183 (1.4808)	mem 53721MB
Train: [0/180][104/5004]	eta 1:28:51 lr 0.100000	data 0.0007 (0.0297)	batch 1.0582 (1.0881)	loss 2.2080 (1.9539)	grad_norm 1.9092 (1.4849)	mem 53721MB
Train: [0/180][105/5004]	eta 1:28:48 lr 0.100000	data 0.0010 (0.0294)	batch 1.0419 (1.0876)	loss 2.0475 (1.9548)	grad_norm 1.6670 (1.4866)	mem 53721MB
Train: [0/180][106/5004]	eta 1:28:44 lr 0.100000	data 0.0006 (0.0292)	batch 1.0368 (1.0872)	loss 1.9348 (1.9546)	grad_norm 1.6232 (1.4879)	mem 53721MB
Train: [0/180][107/5004]	eta 1:28:42 lr 0.100000	data 0.0007 (0.0289)	batch 1.0501 (1.0868)	loss 2.0744 (1.9557)	grad_norm 1.6907 (1.4898)	mem 53721MB
Train: [0/180][108/5004]	eta 1:28:39 lr 0.100000	data 0.0007 (0.0286)	batch 1.0510 (1.0865)	loss 1.9734 (1.9559)	grad_norm 1.6018 (1.4908)	mem 53721MB
Train: [0/180][109/5004]	eta 1:28:36 lr 0.100000	data 0.0008 (0.0284)	batch 1.0455 (1.0861)	loss 2.1008 (1.9572)	grad_norm 1.6919 (1.4926)	mem 53721MB
Train: [0/180][110/5004]	eta 1:28:33 lr 0.100000	data 0.0006 (0.0281)	batch 1.0407 (1.0857)	loss 2.0572 (1.9581)	grad_norm 1.7941 (1.4953)	mem 53721MB
Train: [0/180][111/5004]	eta 1:28:30 lr 0.100000	data 0.0007 (0.0279)	batch 1.0452 (1.0853)	loss 1.9373 (1.9579)	grad_norm 1.5999 (1.4963)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:30:01 lr 0.100000	data 2.4784 (2.4784)	batch 5.3960 (5.3960)	loss 17.2814 (17.2814)	grad_norm 46.7787 (46.7787)	mem 53669MB
Train: [0/180][1/5004]	eta 4:29:17 lr 0.100000	data 0.0009 (1.2397)	batch 1.0631 (3.2296)	loss 20.6906 (18.9860)	grad_norm 48.5490 (47.6639)	mem 53721MB
Train: [0/180][2/5004]	eta 3:28:27 lr 0.100000	data 0.0006 (0.8267)	batch 1.0423 (2.5005)	loss 18.4581 (18.8100)	grad_norm 35.8441 (43.7239)	mem 53721MB
Train: [0/180][3/5004]	eta 2:58:01 lr 0.100000	data 0.0006 (0.6202)	batch 1.0421 (2.1359)	loss 16.2907 (18.1802)	grad_norm 25.5546 (39.1816)	mem 53721MB
Train: [0/180][4/5004]	eta 2:39:27 lr 0.100000	data 0.0006 (0.4962)	batch 1.0244 (1.9136)	loss 14.9665 (17.5374)	grad_norm 23.9777 (36.1408)	mem 53721MB
Train: [0/180][5/5004]	eta 2:27:04 lr 0.100000	data 0.0007 (0.4136)	batch 1.0240 (1.7653)	loss 13.9424 (16.9383)	grad_norm 21.2582 (33.6604)	mem 53721MB
Train: [0/180][6/5004]	eta 2:18:14 lr 0.100000	data 0.0005 (0.3546)	batch 1.0245 (1.6595)	loss 14.4949 (16.5892)	grad_norm 20.8845 (31.8353)	mem 53721MB
Train: [0/180][7/5004]	eta 2:11:37 lr 0.100000	data 0.0007 (0.3104)	batch 1.0276 (1.5805)	loss 13.2427 (16.1709)	grad_norm 26.3896 (31.1546)	mem 53721MB
Train: [0/180][8/5004]	eta 2:06:29 lr 0.100000	data 0.0006 (0.2760)	batch 1.0277 (1.5191)	loss 12.1986 (15.7295)	grad_norm 21.4656 (30.0780)	mem 53721MB
Train: [0/180][9/5004]	eta 2:02:28 lr 0.100000	data 0.0006 (0.2484)	batch 1.0393 (1.4711)	loss 11.5428 (15.3109)	grad_norm 17.4647 (28.8167)	mem 53721MB
Train: [0/180][10/5004]	eta 1:59:24 lr 0.100000	data 0.0007 (0.2259)	batch 1.0702 (1.4347)	loss 11.4235 (14.9575)	grad_norm 17.8811 (27.8225)	mem 53721MB
Train: [0/180][11/5004]	eta 1:56:46 lr 0.100000	data 0.0007 (0.2071)	batch 1.0583 (1.4033)	loss 9.7803 (14.5260)	grad_norm 12.5098 (26.5465)	mem 53721MB
Train: [0/180][12/5004]	eta 1:54:25 lr 0.100000	data 0.0006 (0.1913)	batch 1.0398 (1.3753)	loss 9.8724 (14.1681)	grad_norm 13.3669 (25.5327)	mem 53721MB
Train: [0/180][13/5004]	eta 1:52:30 lr 0.100000	data 0.0008 (0.1776)	batch 1.0566 (1.3526)	loss 9.7882 (13.8552)	grad_norm 16.7566 (24.9058)	mem 53721MB
Train: [0/180][14/5004]	eta 1:50:53 lr 0.100000	data 0.0006 (0.1658)	batch 1.0652 (1.3334)	loss 9.3136 (13.5524)	grad_norm 14.9006 (24.2388)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:45:51 lr 0.100000	data 2.8899 (2.8899)	batch 5.5858 (5.5858)	loss 16.9220 (16.9220)	grad_norm 46.1891 (46.1891)	mem 53669MB
Train: [0/180][50/5004]	eta 1:33:53 lr 0.100000	data 0.0004 (0.0570)	batch 1.0390 (1.1371)	loss 9.9692 (9.9709)	grad_norm 12.4370 (14.8725)	mem 53721MB
Train: [0/180][100/5004]	eta 1:29:17 lr 0.100000	data 0.0004 (0.0290)	batch 1.0436 (1.0925)	loss 15.5196 (11.2605)	grad_norm 28.0212 (15.2462)	mem 53721MB
Train: [0/180][150/5004]	eta 1:27:05 lr 0.100000	data 0.0003 (0.0195)	batch 1.0547 (1.0765)	loss 18.3119 (12.9729)	grad_norm 15.0035 (16.7964)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:41:44 lr 0.100000	data 2.6369 (2.6369)	batch 5.5364 (5.5364)	loss 18.9902 (18.9902)	grad_norm 33.2700 (33.2700)	mem 53669MB
Train: [0/180][50/5004]	eta 1:33:37 lr 0.100000	data 0.0006 (0.0521)	batch 1.0652 (1.1339)	loss 20.6478 (13.6855)	grad_norm 25.3918 (17.1896)	mem 53721MB
Train: [0/180][100/5004]	eta 1:29:06 lr 0.100000	data 0.0003 (0.0265)	batch 1.0358 (1.0902)	loss 21.9699 (17.0481)	grad_norm 14.5136 (18.3426)	mem 53721MB
Train: [0/180][150/5004]	eta 1:27:00 lr 0.100000	data 0.0003 (0.0178)	batch 1.0336 (1.0756)	loss 28.5842 (19.8886)	grad_norm 18.7971 (20.3049)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:37:45 lr 0.100000	data 2.8599 (2.8599)	batch 5.4886 (5.4886)	loss 1.6576 (1.6576)	grad_norm 0.9734 (0.9734)	mem 53669MB
Train: [0/180][50/5004]	eta 1:33:19 lr 0.100000	data 0.0004 (0.0564)	batch 1.0468 (1.1303)	loss 2.1183 (1.9093)	grad_norm 1.5400 (1.3608)	mem 53721MB
Train: [0/180][100/5004]	eta 1:28:57 lr 0.100000	data 0.0003 (0.0287)	batch 1.0424 (1.0885)	loss 1.9951 (1.9671)	grad_norm 1.7099 (1.5142)	mem 53721MB
Train: [0/180][150/5004]	eta 1:26:51 lr 0.100000	data 0.0003 (0.0193)	batch 1.0467 (1.0737)	loss 2.0062 (2.0115)	grad_norm 1.7679 (1.5775)	mem 53721MB
Train: [0/180][200/5004]	eta 1:25:26 lr 0.100000	data 0.0003 (0.0146)	batch 1.0403 (1.0672)	loss 2.1740 (2.0377)	grad_norm 1.7947 (1.6148)	mem 53721MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 7:54:30 lr 0.100000	data 2.7888 (2.7888)	batch 5.6895 (5.6895)	loss 6.9571 (6.9571)	grad_norm 7.8823 (7.8823)	mem 53644MB
Train: [0/180][50/5004]	eta 1:26:46 lr 0.100000	data 0.0004 (0.0551)	batch 0.9606 (1.0509)	loss 7.0569 (7.0402)	grad_norm 2.1293 (3.0076)	mem 53696MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
