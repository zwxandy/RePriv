Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: ''
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 13:55:45 lr 0.050000	data 19.0650 (19.0650)	batch 22.2745 (22.2745)	loss 26.1722 (26.1722)	grad_norm 78.1417 (78.1417)	mem 28676MB
Train: [0/180][50/10009]	eta 4:03:47 lr 0.050000	data 0.0005 (0.3745)	batch 1.0338 (1.4687)	loss 22.6965 (17.5224)	grad_norm 26.2484 (37.3264)	mem 28676MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: ''
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 17:22:25 lr 0.050000	data 20.3197 (20.3197)	batch 23.5134 (23.5134)	loss 26.1720 (26.1720)	grad_norm 78.1497 (78.1497)	mem 28677MB
Train: [0/180][50/10009]	eta 4:07:13 lr 0.050000	data 0.0005 (0.3991)	batch 1.0294 (1.4894)	loss 23.9042 (17.4893)	grad_norm 54.3308 (37.7026)	mem 28677MB
Train: [0/180][100/10009]	eta 3:29:02 lr 0.050000	data 0.0005 (0.2018)	batch 1.0176 (1.2657)	loss 32.0689 (21.5857)	grad_norm 36.6223 (40.6974)	mem 28677MB
Train: [0/180][150/10009]	eta 3:15:36 lr 0.050000	data 0.0004 (0.1351)	batch 1.0208 (1.1904)	loss 33.8439 (25.2545)	grad_norm 65.6065 (42.6321)	mem 28677MB
Train: [0/180][200/10009]	eta 3:08:26 lr 0.050000	data 0.0004 (0.1017)	batch 1.0483 (1.1527)	loss 35.9788 (27.6283)	grad_norm 49.3241 (43.7951)	mem 28677MB
Train: [0/180][250/10009]	eta 3:03:35 lr 0.050000	data 0.0005 (0.0815)	batch 1.0408 (1.1288)	loss 40.5244 (29.9999)	grad_norm 74.6959 (45.2115)	mem 28677MB
Train: [0/180][300/10009]	eta 3:00:23 lr 0.050000	data 0.0006 (0.0681)	batch 1.0353 (1.1148)	loss 31.9914 (31.6943)	grad_norm 38.4567 (45.5874)	mem 28677MB
Train: [0/180][350/10009]	eta 2:59:05 lr 0.050000	data 0.0058 (0.0585)	batch 1.2894 (1.1124)	loss 45.7168 (32.9790)	grad_norm 26.5322 (46.7129)	mem 28677MB
Train: [0/180][400/10009]	eta 3:01:44 lr 0.050000	data 0.0016 (0.0513)	batch 1.2909 (1.1349)	loss 45.4703 (34.2171)	grad_norm 26.6656 (47.6775)	mem 28677MB
Train: [0/180][450/10009]	eta 3:03:31 lr 0.050000	data 0.0004 (0.0458)	batch 1.2847 (1.1520)	loss 31.1468 (35.0163)	grad_norm 48.9501 (48.5405)	mem 28677MB
Train: [0/180][500/10009]	eta 3:04:45 lr 0.050000	data 0.0005 (0.0414)	batch 1.2690 (1.1658)	loss 39.6054 (35.6517)	grad_norm 31.4895 (49.2337)	mem 28677MB
Train: [0/180][550/10009]	eta 3:05:33 lr 0.050000	data 0.0005 (0.0378)	batch 1.2567 (1.1770)	loss 47.9599 (36.0357)	grad_norm 54.2383 (49.2323)	mem 28677MB
Train: [0/180][600/10009]	eta 3:06:04 lr 0.050000	data 0.0003 (0.0348)	batch 1.2921 (1.1866)	loss 29.5946 (36.3325)	grad_norm 30.5565 (49.5357)	mem 28677MB
Train: [0/180][650/10009]	eta 3:06:21 lr 0.050000	data 0.0004 (0.0322)	batch 1.3286 (1.1948)	loss 43.4999 (36.7158)	grad_norm 38.9715 (49.2719)	mem 28677MB
Train: [0/180][700/10009]	eta 3:06:27 lr 0.050000	data 0.0012 (0.0300)	batch 1.2724 (1.2018)	loss 31.2479 (36.9284)	grad_norm 69.9298 (49.0068)	mem 28677MB
Train: [0/180][750/10009]	eta 3:06:25 lr 0.050000	data 0.0014 (0.0281)	batch 1.2922 (1.2080)	loss 37.1795 (37.0176)	grad_norm 32.2837 (48.7604)	mem 28677MB
Train: [0/180][800/10009]	eta 3:06:13 lr 0.050000	data 0.0004 (0.0265)	batch 1.2643 (1.2134)	loss 36.4087 (37.0829)	grad_norm 44.5989 (48.3748)	mem 28677MB
Train: [0/180][850/10009]	eta 3:05:55 lr 0.050000	data 0.0051 (0.0250)	batch 1.3093 (1.2180)	loss 40.2347 (36.9490)	grad_norm 24.8080 (47.9724)	mem 28677MB
Train: [0/180][900/10009]	eta 3:05:35 lr 0.050000	data 0.0005 (0.0237)	batch 1.3009 (1.2224)	loss 29.8079 (36.8836)	grad_norm 33.5783 (47.4510)	mem 28677MB
Train: [0/180][950/10009]	eta 3:05:10 lr 0.050000	data 0.0006 (0.0226)	batch 1.2815 (1.2265)	loss 30.2129 (36.7359)	grad_norm 47.1180 (47.1328)	mem 28677MB
Train: [0/180][1000/10009]	eta 3:04:38 lr 0.050000	data 0.0004 (0.0215)	batch 1.3043 (1.2297)	loss 26.5026 (36.5959)	grad_norm 22.8952 (46.9796)	mem 28677MB
Train: [0/180][1050/10009]	eta 3:04:05 lr 0.050000	data 0.0012 (0.0206)	batch 1.2858 (1.2329)	loss 28.2497 (36.3468)	grad_norm 22.9636 (46.3979)	mem 28677MB
Train: [0/180][1100/10009]	eta 3:03:28 lr 0.050000	data 0.0005 (0.0197)	batch 1.2993 (1.2356)	loss 27.1655 (36.0150)	grad_norm 40.1273 (45.7431)	mem 28677MB
Train: [0/180][1150/10009]	eta 3:02:50 lr 0.050000	data 0.0004 (0.0189)	batch 1.3145 (1.2383)	loss 32.6995 (35.6665)	grad_norm 37.2550 (45.0910)	mem 28677MB
Train: [0/180][1200/10009]	eta 3:02:09 lr 0.050000	data 0.0005 (0.0182)	batch 1.3048 (1.2407)	loss 25.9321 (35.3227)	grad_norm 26.0116 (44.6838)	mem 28677MB
Train: [0/180][1250/10009]	eta 3:01:30 lr 0.050000	data 0.0004 (0.0176)	batch 1.3731 (1.2433)	loss 24.7411 (34.9740)	grad_norm 31.4001 (44.0175)	mem 28677MB
Train: [0/180][1300/10009]	eta 3:00:45 lr 0.050000	data 0.0052 (0.0170)	batch 1.3342 (1.2453)	loss 21.6029 (34.5268)	grad_norm 16.8509 (43.4465)	mem 28677MB
Train: [0/180][1350/10009]	eta 2:59:59 lr 0.050000	data 0.0027 (0.0164)	batch 1.3479 (1.2472)	loss 22.5051 (34.1406)	grad_norm 17.8034 (42.9394)	mem 28677MB
Train: [0/180][1400/10009]	eta 2:59:13 lr 0.050000	data 0.0006 (0.0159)	batch 1.2746 (1.2491)	loss 20.4150 (33.7481)	grad_norm 41.4397 (42.4591)	mem 28677MB
Train: [0/180][1450/10009]	eta 2:58:22 lr 0.050000	data 0.0005 (0.0154)	batch 1.2868 (1.2504)	loss 26.6200 (33.4094)	grad_norm 33.8859 (41.9816)	mem 28677MB
Train: [0/180][1500/10009]	eta 2:57:26 lr 0.050000	data 0.0004 (0.0149)	batch 1.2518 (1.2512)	loss 19.7975 (33.0462)	grad_norm 20.8121 (41.4876)	mem 28677MB
Train: [0/180][1550/10009]	eta 2:56:08 lr 0.050000	data 0.0054 (0.0145)	batch 1.1855 (1.2494)	loss 22.2768 (32.7413)	grad_norm 20.6534 (41.0755)	mem 28677MB
Train: [0/180][1600/10009]	eta 2:54:46 lr 0.050000	data 0.0006 (0.0141)	batch 1.0376 (1.2470)	loss 24.1764 (32.4204)	grad_norm 17.7026 (40.6721)	mem 28677MB
Train: [0/180][1650/10009]	eta 2:52:51 lr 0.050000	data 0.0004 (0.0137)	batch 1.0436 (1.2407)	loss 23.6794 (32.0986)	grad_norm 21.5382 (40.1914)	mem 28677MB
Train: [0/180][1700/10009]	eta 2:51:00 lr 0.050000	data 0.0004 (0.0133)	batch 1.0416 (1.2348)	loss 21.1030 (31.8096)	grad_norm 26.8903 (39.8893)	mem 28677MB
Train: [0/180][1750/10009]	eta 2:49:13 lr 0.050000	data 0.0005 (0.0130)	batch 1.0334 (1.2294)	loss 24.5922 (31.5391)	grad_norm 88.8516 (39.5075)	mem 28677MB
Train: [0/180][1800/10009]	eta 2:47:28 lr 0.050000	data 0.0005 (0.0126)	batch 1.0387 (1.2241)	loss 21.0945 (31.3003)	grad_norm 15.9459 (39.1702)	mem 28677MB
Train: [0/180][1850/10009]	eta 2:45:46 lr 0.050000	data 0.0004 (0.0123)	batch 1.0389 (1.2191)	loss 20.8079 (31.0245)	grad_norm 26.1726 (38.7757)	mem 28677MB
Train: [0/180][1900/10009]	eta 2:44:06 lr 0.050000	data 0.0004 (0.0120)	batch 1.0407 (1.2143)	loss 23.1898 (30.7481)	grad_norm 36.3080 (38.4363)	mem 28677MB
Train: [0/180][1950/10009]	eta 2:42:28 lr 0.050000	data 0.0005 (0.0117)	batch 1.0355 (1.2097)	loss 22.2302 (30.5005)	grad_norm 14.6812 (38.0586)	mem 28677MB
Train: [0/180][2000/10009]	eta 2:40:53 lr 0.050000	data 0.0005 (0.0114)	batch 1.0392 (1.2053)	loss 16.5343 (30.2219)	grad_norm 18.8989 (37.6745)	mem 28677MB
Train: [0/180][2050/10009]	eta 2:39:20 lr 0.050000	data 0.0005 (0.0112)	batch 1.0142 (1.2012)	loss 15.3817 (29.9344)	grad_norm 18.5148 (37.3947)	mem 28677MB
Train: [0/180][2100/10009]	eta 2:37:49 lr 0.050000	data 0.0006 (0.0109)	batch 1.0439 (1.1974)	loss 17.9598 (29.6726)	grad_norm 18.3932 (37.0510)	mem 28677MB
Train: [0/180][2150/10009]	eta 2:36:20 lr 0.050000	data 0.0005 (0.0107)	batch 1.0363 (1.1936)	loss 22.4334 (29.4287)	grad_norm 41.4803 (36.7461)	mem 28677MB
Train: [0/180][2200/10009]	eta 2:34:52 lr 0.050000	data 0.0004 (0.0104)	batch 1.0421 (1.1900)	loss 16.2568 (29.1806)	grad_norm 19.9805 (36.4083)	mem 28677MB
Train: [0/180][2250/10009]	eta 2:33:27 lr 0.050000	data 0.0004 (0.0102)	batch 1.0388 (1.1866)	loss 20.4940 (28.9294)	grad_norm 44.3071 (36.0835)	mem 28677MB
Train: [0/180][2300/10009]	eta 2:32:01 lr 0.050000	data 0.0004 (0.0100)	batch 1.0169 (1.1833)	loss 16.2100 (28.6906)	grad_norm 25.7675 (35.7843)	mem 28677MB
Train: [0/180][2350/10009]	eta 2:30:38 lr 0.050000	data 0.0005 (0.0098)	batch 1.0157 (1.1802)	loss 21.3760 (28.4612)	grad_norm 40.9848 (35.4696)	mem 28677MB
Train: [0/180][2400/10009]	eta 2:29:17 lr 0.050000	data 0.0004 (0.0096)	batch 1.0405 (1.1772)	loss 15.1745 (28.2364)	grad_norm 14.3174 (35.1835)	mem 28677MB
Train: [0/180][2450/10009]	eta 2:27:57 lr 0.050000	data 0.0005 (0.0094)	batch 1.0386 (1.1745)	loss 18.5904 (27.9916)	grad_norm 16.2827 (34.8475)	mem 28677MB
Train: [0/180][2500/10009]	eta 2:26:37 lr 0.050000	data 0.0004 (0.0092)	batch 1.0299 (1.1717)	loss 14.0887 (27.7464)	grad_norm 23.0063 (34.5554)	mem 28677MB
Train: [0/180][2550/10009]	eta 2:25:19 lr 0.050000	data 0.0005 (0.0091)	batch 1.0290 (1.1690)	loss 15.7568 (27.5063)	grad_norm 18.7405 (34.2637)	mem 28677MB
Train: [0/180][2600/10009]	eta 2:24:01 lr 0.050000	data 0.0004 (0.0089)	batch 1.0422 (1.1664)	loss 16.8186 (27.2792)	grad_norm 16.6622 (33.9862)	mem 28677MB
Train: [0/180][2650/10009]	eta 2:22:44 lr 0.050000	data 0.0005 (0.0087)	batch 1.0314 (1.1639)	loss 13.0744 (27.0574)	grad_norm 21.7009 (33.6748)	mem 28677MB
Train: [0/180][2700/10009]	eta 2:21:40 lr 0.050000	data 0.0055 (0.0086)	batch 1.1813 (1.1630)	loss 14.9468 (26.8327)	grad_norm 14.0585 (33.3750)	mem 28677MB
Train: [0/180][2750/10009]	eta 2:20:43 lr 0.050000	data 0.0009 (0.0085)	batch 1.0215 (1.1632)	loss 16.6389 (26.6248)	grad_norm 42.5881 (33.1574)	mem 28677MB
Train: [0/180][2800/10009]	eta 2:19:29 lr 0.050000	data 0.0006 (0.0083)	batch 1.0472 (1.1610)	loss 16.0162 (26.4206)	grad_norm 12.7362 (32.8820)	mem 28677MB
Train: [0/180][2850/10009]	eta 2:18:15 lr 0.050000	data 0.0004 (0.0082)	batch 1.0465 (1.1588)	loss 13.4939 (26.2108)	grad_norm 21.9609 (32.6208)	mem 28677MB
Train: [0/180][2900/10009]	eta 2:17:02 lr 0.050000	data 0.0004 (0.0081)	batch 1.0343 (1.1567)	loss 15.5014 (25.9978)	grad_norm 22.9508 (32.3765)	mem 28677MB
Train: [0/180][2950/10009]	eta 2:15:50 lr 0.050000	data 0.0005 (0.0079)	batch 1.0625 (1.1547)	loss 11.9602 (25.7987)	grad_norm 15.0098 (32.1203)	mem 28677MB
Train: [0/180][3000/10009]	eta 2:14:39 lr 0.050000	data 0.0004 (0.0078)	batch 1.0438 (1.1527)	loss 11.8010 (25.5960)	grad_norm 34.5644 (31.8604)	mem 28677MB
Train: [0/180][3050/10009]	eta 2:13:28 lr 0.050000	data 0.0005 (0.0077)	batch 1.0151 (1.1508)	loss 11.5268 (25.3884)	grad_norm 13.1229 (31.6036)	mem 28677MB
Train: [0/180][3100/10009]	eta 2:12:17 lr 0.050000	data 0.0004 (0.0076)	batch 1.0369 (1.1489)	loss 10.2654 (25.1825)	grad_norm 15.7089 (31.3425)	mem 28677MB
Train: [0/180][3150/10009]	eta 2:11:07 lr 0.050000	data 0.0005 (0.0075)	batch 1.0359 (1.1471)	loss 12.8388 (24.9822)	grad_norm 13.2563 (31.1040)	mem 28677MB
Train: [0/180][3200/10009]	eta 2:09:58 lr 0.050000	data 0.0004 (0.0074)	batch 1.0292 (1.1453)	loss 11.6420 (24.7852)	grad_norm 13.0619 (30.8527)	mem 28677MB
Train: [0/180][3250/10009]	eta 2:08:49 lr 0.050000	data 0.0005 (0.0073)	batch 1.0442 (1.1436)	loss 10.9436 (24.5869)	grad_norm 14.1952 (30.5991)	mem 28677MB
Train: [0/180][3300/10009]	eta 2:07:41 lr 0.050000	data 0.0005 (0.0072)	batch 1.0291 (1.1419)	loss 12.4921 (24.3905)	grad_norm 12.4182 (30.3650)	mem 28677MB
Train: [0/180][3350/10009]	eta 2:06:33 lr 0.050000	data 0.0004 (0.0071)	batch 1.0329 (1.1403)	loss 12.0937 (24.1905)	grad_norm 10.9062 (30.1293)	mem 28677MB
Train: [0/180][3400/10009]	eta 2:05:26 lr 0.050000	data 0.0004 (0.0070)	batch 1.0394 (1.1388)	loss 9.7002 (23.9881)	grad_norm 14.5174 (29.8859)	mem 28677MB
Train: [0/180][3450/10009]	eta 2:04:19 lr 0.050000	data 0.0004 (0.0069)	batch 1.0321 (1.1373)	loss 10.1205 (23.7977)	grad_norm 10.9847 (29.6659)	mem 28677MB
Train: [0/180][3500/10009]	eta 2:03:13 lr 0.050000	data 0.0006 (0.0068)	batch 1.0361 (1.1359)	loss 8.5429 (23.6085)	grad_norm 15.2432 (29.4289)	mem 28677MB
Train: [0/180][3550/10009]	eta 2:02:08 lr 0.050000	data 0.0005 (0.0067)	batch 1.0429 (1.1346)	loss 9.0949 (23.4128)	grad_norm 10.1579 (29.1935)	mem 28677MB
Train: [0/180][3600/10009]	eta 2:01:02 lr 0.050000	data 0.0004 (0.0066)	batch 1.0388 (1.1332)	loss 7.6703 (23.2166)	grad_norm 11.8564 (28.9603)	mem 28677MB
Train: [0/180][3650/10009]	eta 1:59:57 lr 0.049999	data 0.0004 (0.0065)	batch 1.0225 (1.1319)	loss 8.4773 (23.0193)	grad_norm 11.1737 (28.7339)	mem 28677MB
Train: [0/180][3700/10009]	eta 1:58:52 lr 0.049999	data 0.0004 (0.0064)	batch 1.0415 (1.1306)	loss 8.8496 (22.8254)	grad_norm 18.0068 (28.5061)	mem 28677MB
Train: [0/180][3750/10009]	eta 1:57:48 lr 0.049999	data 0.0005 (0.0064)	batch 1.0116 (1.1293)	loss 8.1304 (22.6353)	grad_norm 10.5890 (28.2828)	mem 28677MB
Train: [0/180][3800/10009]	eta 1:56:44 lr 0.049999	data 0.0005 (0.0063)	batch 1.0451 (1.1281)	loss 9.0103 (22.4468)	grad_norm 11.1469 (28.0623)	mem 28677MB
Train: [0/180][3850/10009]	eta 1:55:40 lr 0.049999	data 0.0005 (0.0062)	batch 1.0320 (1.1269)	loss 7.0636 (22.2571)	grad_norm 10.1090 (27.8469)	mem 28677MB
Train: [0/180][3900/10009]	eta 1:54:36 lr 0.049999	data 0.0005 (0.0061)	batch 1.0408 (1.1257)	loss 7.5452 (22.0719)	grad_norm 10.4509 (27.6343)	mem 28677MB
Train: [0/180][3950/10009]	eta 1:53:33 lr 0.049999	data 0.0004 (0.0061)	batch 1.0173 (1.1246)	loss 8.6493 (21.8931)	grad_norm 10.6735 (27.4250)	mem 28677MB
Train: [0/180][4000/10009]	eta 1:52:31 lr 0.049999	data 0.0005 (0.0060)	batch 1.0386 (1.1236)	loss 7.3086 (21.7143)	grad_norm 13.4759 (27.2164)	mem 28677MB
Train: [0/180][4050/10009]	eta 1:51:28 lr 0.049999	data 0.0007 (0.0059)	batch 1.0418 (1.1225)	loss 7.1078 (21.5359)	grad_norm 10.3078 (27.0100)	mem 28677MB
Train: [0/180][4100/10009]	eta 1:50:26 lr 0.049999	data 0.0004 (0.0059)	batch 1.0409 (1.1215)	loss 7.0322 (21.3612)	grad_norm 9.2433 (26.8066)	mem 28677MB
Train: [0/180][4150/10009]	eta 1:49:25 lr 0.049999	data 0.0006 (0.0058)	batch 1.0444 (1.1206)	loss 6.7543 (21.1865)	grad_norm 9.6214 (26.6060)	mem 28677MB
Train: [0/180][4200/10009]	eta 1:48:23 lr 0.049999	data 0.0004 (0.0057)	batch 1.0333 (1.1196)	loss 6.5029 (21.0160)	grad_norm 8.9380 (26.4069)	mem 28677MB
Train: [0/180][4250/10009]	eta 1:47:21 lr 0.049999	data 0.0005 (0.0057)	batch 1.0317 (1.1186)	loss 5.9934 (20.8469)	grad_norm 8.5313 (26.2113)	mem 28677MB
Train: [0/180][4300/10009]	eta 1:46:20 lr 0.049999	data 0.0004 (0.0056)	batch 1.0381 (1.1176)	loss 6.4441 (20.6823)	grad_norm 9.7145 (26.0185)	mem 28677MB
Train: [0/180][4350/10009]	eta 1:45:18 lr 0.049999	data 0.0005 (0.0056)	batch 1.0248 (1.1166)	loss 5.9266 (20.5204)	grad_norm 8.4864 (25.8319)	mem 28677MB
Train: [0/180][4400/10009]	eta 1:44:17 lr 0.049999	data 0.0004 (0.0055)	batch 1.0467 (1.1157)	loss 6.4227 (20.3605)	grad_norm 9.9220 (25.6449)	mem 28677MB
Train: [0/180][4450/10009]	eta 1:43:17 lr 0.049999	data 0.0004 (0.0054)	batch 1.0419 (1.1149)	loss 5.7219 (20.2033)	grad_norm 8.3020 (25.4611)	mem 28677MB
Train: [0/180][4500/10009]	eta 1:42:18 lr 0.049999	data 0.0005 (0.0054)	batch 1.0659 (1.1142)	loss 5.9625 (20.0490)	grad_norm 8.7237 (25.2798)	mem 28677MB
Train: [0/180][4550/10009]	eta 1:41:18 lr 0.049999	data 0.0005 (0.0053)	batch 1.0360 (1.1135)	loss 6.2640 (19.8969)	grad_norm 9.4776 (25.1019)	mem 28677MB
Train: [0/180][4600/10009]	eta 1:40:18 lr 0.049999	data 0.0005 (0.0053)	batch 1.1232 (1.1126)	loss 6.4229 (19.7476)	grad_norm 8.3971 (24.9260)	mem 28677MB
Train: [0/180][4650/10009]	eta 1:39:27 lr 0.049999	data 0.0003 (0.0053)	batch 1.2663 (1.1135)	loss 5.9663 (19.6031)	grad_norm 8.4283 (24.7581)	mem 28677MB
Train: [0/180][4700/10009]	eta 1:38:30 lr 0.049999	data 0.0008 (0.0052)	batch 1.0357 (1.1133)	loss 6.0173 (19.4605)	grad_norm 7.9572 (24.5918)	mem 28677MB
Train: [0/180][4750/10009]	eta 1:37:31 lr 0.049999	data 0.0004 (0.0052)	batch 1.0692 (1.1126)	loss 5.6025 (19.3197)	grad_norm 8.2403 (24.4248)	mem 28677MB
Train: [0/180][4800/10009]	eta 1:36:33 lr 0.049999	data 0.0004 (0.0051)	batch 1.0684 (1.1121)	loss 5.8418 (19.1808)	grad_norm 8.1555 (24.2593)	mem 28677MB
Train: [0/180][4850/10009]	eta 1:35:35 lr 0.049999	data 0.0005 (0.0051)	batch 1.0655 (1.1117)	loss 5.9239 (19.0442)	grad_norm 8.3971 (24.0971)	mem 28677MB
Train: [0/180][4900/10009]	eta 1:34:35 lr 0.049999	data 0.0004 (0.0050)	batch 1.0339 (1.1109)	loss 5.9782 (18.9089)	grad_norm 7.8947 (23.9364)	mem 28677MB
Train: [0/180][4950/10009]	eta 1:33:36 lr 0.049999	data 0.0005 (0.0050)	batch 1.0312 (1.1102)	loss 5.3480 (18.7771)	grad_norm 7.6386 (23.7785)	mem 28677MB
Train: [0/180][5000/10009]	eta 1:32:43 lr 0.049999	data 0.0053 (0.0049)	batch 1.3654 (1.1108)	loss 6.1407 (18.6477)	grad_norm 7.7629 (23.6227)	mem 28677MB
Train: [0/180][5050/10009]	eta 1:31:49 lr 0.049999	data 0.0004 (0.0049)	batch 1.0341 (1.1110)	loss 6.0531 (18.5202)	grad_norm 8.5966 (23.4676)	mem 28677MB
Train: [0/180][5100/10009]	eta 1:30:50 lr 0.049999	data 0.0005 (0.0049)	batch 1.0400 (1.1103)	loss 5.7060 (18.3940)	grad_norm 7.7798 (23.3154)	mem 28677MB
Train: [0/180][5150/10009]	eta 1:29:51 lr 0.049999	data 0.0006 (0.0048)	batch 1.0401 (1.1096)	loss 5.8939 (18.2693)	grad_norm 7.6640 (23.1641)	mem 28677MB
Train: [0/180][5200/10009]	eta 1:28:52 lr 0.049999	data 0.0004 (0.0048)	batch 1.0244 (1.1089)	loss 5.8694 (18.1474)	grad_norm 8.1689 (23.0149)	mem 28677MB
Train: [0/180][5250/10009]	eta 1:27:53 lr 0.049999	data 0.0005 (0.0047)	batch 1.0255 (1.1082)	loss 5.4548 (18.0271)	grad_norm 7.2993 (22.8683)	mem 28677MB
Train: [0/180][5300/10009]	eta 1:26:55 lr 0.049999	data 0.0005 (0.0047)	batch 1.0359 (1.1075)	loss 5.2795 (17.9093)	grad_norm 7.5546 (22.7243)	mem 28677MB
Train: [0/180][5350/10009]	eta 1:25:56 lr 0.049999	data 0.0005 (0.0047)	batch 1.0400 (1.1068)	loss 5.5322 (17.7930)	grad_norm 7.0362 (22.5811)	mem 28677MB
Train: [0/180][5400/10009]	eta 1:24:58 lr 0.049999	data 0.0005 (0.0046)	batch 1.0384 (1.1062)	loss 5.6961 (17.6776)	grad_norm 7.4639 (22.4393)	mem 28677MB
Train: [0/180][5450/10009]	eta 1:24:00 lr 0.049999	data 0.0005 (0.0046)	batch 1.0290 (1.1055)	loss 5.0766 (17.5649)	grad_norm 6.9553 (22.3001)	mem 28677MB
Train: [0/180][5500/10009]	eta 1:23:01 lr 0.049999	data 0.0005 (0.0045)	batch 1.0425 (1.1049)	loss 5.1190 (17.4539)	grad_norm 7.1933 (22.1634)	mem 28677MB
Train: [0/180][5550/10009]	eta 1:22:03 lr 0.049999	data 0.0005 (0.0045)	batch 1.0370 (1.1042)	loss 5.3234 (17.3445)	grad_norm 6.8289 (22.0273)	mem 28677MB
Train: [0/180][5600/10009]	eta 1:21:06 lr 0.049999	data 0.0005 (0.0045)	batch 1.0362 (1.1037)	loss 5.3843 (17.2372)	grad_norm 6.7473 (21.8945)	mem 28677MB
Train: [0/180][5650/10009]	eta 1:20:08 lr 0.049999	data 0.0005 (0.0044)	batch 1.0359 (1.1031)	loss 5.1445 (17.1319)	grad_norm 6.5336 (21.7620)	mem 28677MB
Train: [0/180][5700/10009]	eta 1:19:10 lr 0.049999	data 0.0005 (0.0044)	batch 1.0297 (1.1025)	loss 4.7930 (17.0270)	grad_norm 6.2673 (21.6299)	mem 28677MB
Train: [0/180][5750/10009]	eta 1:18:12 lr 0.049999	data 0.0004 (0.0044)	batch 1.0382 (1.1019)	loss 5.1624 (16.9236)	grad_norm 6.3837 (21.4992)	mem 28677MB
Train: [0/180][5800/10009]	eta 1:17:15 lr 0.049999	data 0.0005 (0.0043)	batch 1.0603 (1.1014)	loss 5.0352 (16.8217)	grad_norm 6.7784 (21.3710)	mem 28677MB
Train: [0/180][5850/10009]	eta 1:16:18 lr 0.049999	data 0.0005 (0.0043)	batch 1.0284 (1.1010)	loss 5.0420 (16.7217)	grad_norm 6.3031 (21.2440)	mem 28677MB
Train: [0/180][5900/10009]	eta 1:15:21 lr 0.049999	data 0.0005 (0.0043)	batch 1.0372 (1.1004)	loss 5.2939 (16.6225)	grad_norm 6.9294 (21.1186)	mem 28677MB
Train: [0/180][5950/10009]	eta 1:14:24 lr 0.049999	data 0.0005 (0.0042)	batch 1.0412 (1.0999)	loss 4.9263 (16.5250)	grad_norm 6.1857 (20.9946)	mem 28677MB
Train: [0/180][6000/10009]	eta 1:13:27 lr 0.049999	data 0.0005 (0.0042)	batch 1.0347 (1.0993)	loss 5.4968 (16.4292)	grad_norm 6.5328 (20.8722)	mem 28677MB
Train: [0/180][6050/10009]	eta 1:12:30 lr 0.049999	data 0.0004 (0.0042)	batch 1.0456 (1.0988)	loss 4.9247 (16.3339)	grad_norm 6.4344 (20.7511)	mem 28677MB
Train: [0/180][6100/10009]	eta 1:11:33 lr 0.049999	data 0.0005 (0.0041)	batch 1.0462 (1.0983)	loss 5.0066 (16.2402)	grad_norm 6.1791 (20.6305)	mem 28677MB
Train: [0/180][6150/10009]	eta 1:10:36 lr 0.049999	data 0.0004 (0.0041)	batch 1.0412 (1.0978)	loss 5.0987 (16.1477)	grad_norm 6.0178 (20.5129)	mem 28677MB
Train: [0/180][6200/10009]	eta 1:09:39 lr 0.049999	data 0.0005 (0.0041)	batch 1.0300 (1.0973)	loss 4.9350 (16.0567)	grad_norm 5.6722 (20.3959)	mem 28677MB
Train: [0/180][6250/10009]	eta 1:08:42 lr 0.049999	data 0.0004 (0.0041)	batch 1.0142 (1.0968)	loss 5.2175 (15.9666)	grad_norm 6.0786 (20.2802)	mem 28677MB
Train: [0/180][6300/10009]	eta 1:07:46 lr 0.049998	data 0.0005 (0.0040)	batch 1.0333 (1.0963)	loss 4.6955 (15.8779)	grad_norm 6.0116 (20.1656)	mem 28677MB
Train: [0/180][6350/10009]	eta 1:06:49 lr 0.049998	data 0.0005 (0.0040)	batch 1.0285 (1.0958)	loss 4.8535 (15.7901)	grad_norm 5.7383 (20.0522)	mem 28677MB
Train: [0/180][6400/10009]	eta 1:05:53 lr 0.049998	data 0.0005 (0.0040)	batch 1.0509 (1.0953)	loss 4.7642 (15.7032)	grad_norm 5.5056 (19.9397)	mem 28677MB
Train: [0/180][6450/10009]	eta 1:04:56 lr 0.049998	data 0.0005 (0.0039)	batch 1.0332 (1.0949)	loss 5.1862 (15.6179)	grad_norm 5.8818 (19.8288)	mem 28677MB
Train: [0/180][6500/10009]	eta 1:04:00 lr 0.049998	data 0.0005 (0.0039)	batch 1.0385 (1.0944)	loss 4.4674 (15.5339)	grad_norm 5.4612 (19.7191)	mem 28677MB
Train: [0/180][6550/10009]	eta 1:03:04 lr 0.049998	data 0.0004 (0.0039)	batch 1.0345 (1.0940)	loss 4.5711 (15.4502)	grad_norm 5.5479 (19.6109)	mem 28677MB
Train: [0/180][6600/10009]	eta 1:02:08 lr 0.049998	data 0.0005 (0.0039)	batch 1.0411 (1.0936)	loss 4.3750 (15.3678)	grad_norm 5.2798 (19.5045)	mem 28677MB
Train: [0/180][6650/10009]	eta 1:01:12 lr 0.049998	data 0.0004 (0.0038)	batch 1.0337 (1.0933)	loss 4.6633 (15.2864)	grad_norm 5.3975 (19.3989)	mem 28677MB
Train: [0/180][6700/10009]	eta 1:00:16 lr 0.049998	data 0.0005 (0.0038)	batch 1.0358 (1.0929)	loss 4.1598 (15.2058)	grad_norm 5.4677 (19.2944)	mem 28677MB
Train: [0/180][6750/10009]	eta 0:59:20 lr 0.049998	data 0.0004 (0.0038)	batch 1.0423 (1.0925)	loss 4.4225 (15.1266)	grad_norm 5.2470 (19.1914)	mem 28677MB
Train: [0/180][6800/10009]	eta 0:58:24 lr 0.049998	data 0.0005 (0.0038)	batch 1.0378 (1.0921)	loss 4.4015 (15.0481)	grad_norm 5.3481 (19.0894)	mem 28677MB
Train: [0/180][6850/10009]	eta 0:57:28 lr 0.049998	data 0.0004 (0.0037)	batch 1.0844 (1.0917)	loss 4.4671 (14.9708)	grad_norm 5.4940 (18.9890)	mem 28677MB
Train: [0/180][6900/10009]	eta 0:56:32 lr 0.049998	data 0.0005 (0.0037)	batch 1.0392 (1.0913)	loss 4.3593 (14.8943)	grad_norm 5.4934 (18.8899)	mem 28677MB
Train: [0/180][6950/10009]	eta 0:55:37 lr 0.049998	data 0.0005 (0.0037)	batch 1.0371 (1.0910)	loss 4.3588 (14.8184)	grad_norm 5.2380 (18.7919)	mem 28677MB
Train: [0/180][7000/10009]	eta 0:54:41 lr 0.049998	data 0.0004 (0.0037)	batch 1.0150 (1.0906)	loss 4.6666 (14.7433)	grad_norm 5.3696 (18.6951)	mem 28677MB
Train: [0/180][7050/10009]	eta 0:53:46 lr 0.049998	data 0.0004 (0.0037)	batch 1.2047 (1.0903)	loss 3.9237 (14.6690)	grad_norm 4.9907 (18.5996)	mem 28677MB
Train: [0/180][7100/10009]	eta 0:52:50 lr 0.049998	data 0.0005 (0.0036)	batch 1.0371 (1.0899)	loss 4.2409 (14.5957)	grad_norm 5.1923 (18.5055)	mem 28677MB
Train: [0/180][7150/10009]	eta 0:51:55 lr 0.049998	data 0.0005 (0.0036)	batch 1.0408 (1.0896)	loss 4.5600 (14.5232)	grad_norm 5.3002 (18.4125)	mem 28677MB
Train: [0/180][7200/10009]	eta 0:50:59 lr 0.049998	data 0.0005 (0.0036)	batch 1.0294 (1.0893)	loss 4.0594 (14.4515)	grad_norm 5.1096 (18.3206)	mem 28677MB
Train: [0/180][7250/10009]	eta 0:50:04 lr 0.049998	data 0.0005 (0.0036)	batch 1.0363 (1.0889)	loss 3.9351 (14.3804)	grad_norm 4.9866 (18.2300)	mem 28677MB
Train: [0/180][7300/10009]	eta 0:49:09 lr 0.049998	data 0.0005 (0.0035)	batch 1.0551 (1.0886)	loss 4.0615 (14.3102)	grad_norm 5.1924 (18.1406)	mem 28677MB
Train: [0/180][7350/10009]	eta 0:48:13 lr 0.049998	data 0.0004 (0.0035)	batch 1.0240 (1.0883)	loss 3.7715 (14.2404)	grad_norm 5.1258 (18.0523)	mem 28677MB
Train: [0/180][7400/10009]	eta 0:47:18 lr 0.049998	data 0.0005 (0.0035)	batch 1.0388 (1.0879)	loss 3.7684 (14.1715)	grad_norm 5.1021 (17.9651)	mem 28677MB
Train: [0/180][7450/10009]	eta 0:46:23 lr 0.049998	data 0.0004 (0.0035)	batch 1.0476 (1.0877)	loss 4.2080 (14.1032)	grad_norm 5.2709 (17.8791)	mem 28677MB
Train: [0/180][7500/10009]	eta 0:45:28 lr 0.049998	data 0.0004 (0.0035)	batch 1.0402 (1.0874)	loss 4.1055 (14.0357)	grad_norm 5.1506 (17.7941)	mem 28677MB
Train: [0/180][7550/10009]	eta 0:44:34 lr 0.049998	data 0.0002 (0.0034)	batch 1.5251 (1.0877)	loss 3.9153 (13.9691)	grad_norm 4.9935 (17.7103)	mem 28677MB
Train: [0/180][7600/10009]	eta 0:43:46 lr 0.049998	data 0.0005 (0.0034)	batch 1.5012 (1.0905)	loss 3.8859 (13.9031)	grad_norm 5.0397 (17.6276)	mem 28677MB
Train: [0/180][7650/10009]	eta 0:42:51 lr 0.049998	data 0.0005 (0.0034)	batch 1.0397 (1.0902)	loss 4.0249 (13.8381)	grad_norm 5.1039 (17.5459)	mem 28677MB
Train: [0/180][7700/10009]	eta 0:41:58 lr 0.049998	data 0.0005 (0.0034)	batch 1.0211 (1.0909)	loss 3.5824 (13.7737)	grad_norm 5.0000 (17.4653)	mem 28677MB
Train: [0/180][7750/10009]	eta 0:41:03 lr 0.049998	data 0.0004 (0.0034)	batch 1.0400 (1.0906)	loss 3.6871 (13.7101)	grad_norm 5.0565 (17.3857)	mem 28677MB
Train: [0/180][7800/10009]	eta 0:40:08 lr 0.049998	data 0.0004 (0.0033)	batch 1.0410 (1.0902)	loss 3.6400 (13.6470)	grad_norm 4.9783 (17.3071)	mem 28677MB
Train: [0/180][7850/10009]	eta 0:39:16 lr 0.049998	data 0.0005 (0.0033)	batch 1.6784 (1.0915)	loss 3.8017 (13.5844)	grad_norm 5.1357 (17.2293)	mem 28677MB
Train: [0/180][7900/10009]	eta 0:38:23 lr 0.049998	data 0.0005 (0.0033)	batch 1.0409 (1.0921)	loss 3.7689 (13.5226)	grad_norm 4.8955 (17.1526)	mem 28677MB
Train: [0/180][7950/10009]	eta 0:37:27 lr 0.049998	data 0.0004 (0.0033)	batch 1.0874 (1.0918)	loss 4.0050 (13.4614)	grad_norm 5.1649 (17.0769)	mem 28677MB
Train: [0/180][8000/10009]	eta 0:36:35 lr 0.049998	data 0.0005 (0.0033)	batch 1.0341 (1.0930)	loss 3.7040 (13.4010)	grad_norm 5.1897 (17.0020)	mem 28677MB
Train: [0/180][8050/10009]	eta 0:35:40 lr 0.049998	data 0.0004 (0.0033)	batch 1.0412 (1.0926)	loss 3.5041 (13.3409)	grad_norm 5.0544 (16.9281)	mem 28677MB
Train: [0/180][8100/10009]	eta 0:34:45 lr 0.049998	data 0.0006 (0.0032)	batch 1.0215 (1.0923)	loss 3.4140 (13.2817)	grad_norm 5.0271 (16.8550)	mem 28677MB
Train: [0/180][8150/10009]	eta 0:33:49 lr 0.049997	data 0.0005 (0.0032)	batch 1.0407 (1.0919)	loss 3.6268 (13.2229)	grad_norm 5.0827 (16.7829)	mem 28677MB
Train: [0/180][8200/10009]	eta 0:32:54 lr 0.049997	data 0.0005 (0.0032)	batch 1.0263 (1.0916)	loss 3.6224 (13.1649)	grad_norm 5.0565 (16.7116)	mem 28677MB
Train: [0/180][8250/10009]	eta 0:31:59 lr 0.049997	data 0.0005 (0.0032)	batch 1.0308 (1.0912)	loss 3.7174 (13.1074)	grad_norm 5.0982 (16.6411)	mem 28677MB
Train: [0/180][8300/10009]	eta 0:31:04 lr 0.049997	data 0.0005 (0.0032)	batch 1.0316 (1.0909)	loss 3.5811 (13.0504)	grad_norm 5.1840 (16.5715)	mem 28677MB
Train: [0/180][8350/10009]	eta 0:30:09 lr 0.049997	data 0.0005 (0.0032)	batch 1.0344 (1.0906)	loss 3.4179 (12.9941)	grad_norm 4.9596 (16.5027)	mem 28677MB
Train: [0/180][8400/10009]	eta 0:29:14 lr 0.049997	data 0.0005 (0.0031)	batch 1.0313 (1.0903)	loss 3.5330 (12.9384)	grad_norm 5.0699 (16.4347)	mem 28677MB
Train: [0/180][8450/10009]	eta 0:28:19 lr 0.049997	data 0.0005 (0.0031)	batch 1.0341 (1.0899)	loss 3.4150 (12.8833)	grad_norm 4.8741 (16.3676)	mem 28677MB
Train: [0/180][8500/10009]	eta 0:27:24 lr 0.049997	data 0.0005 (0.0031)	batch 1.0110 (1.0896)	loss 3.3553 (12.8288)	grad_norm 4.9831 (16.3012)	mem 28677MB
Train: [0/180][8550/10009]	eta 0:26:29 lr 0.049997	data 0.0006 (0.0031)	batch 1.0399 (1.0893)	loss 3.4785 (12.7748)	grad_norm 5.0964 (16.2356)	mem 28677MB
Train: [0/180][8600/10009]	eta 0:25:34 lr 0.049997	data 0.0005 (0.0031)	batch 1.0217 (1.0890)	loss 3.4138 (12.7216)	grad_norm 5.1060 (16.1707)	mem 28677MB
Train: [0/180][8650/10009]	eta 0:24:39 lr 0.049997	data 0.0005 (0.0031)	batch 1.0333 (1.0887)	loss 3.4880 (12.6688)	grad_norm 5.0882 (16.1066)	mem 28677MB
Train: [0/180][8700/10009]	eta 0:23:44 lr 0.049997	data 0.0003 (0.0031)	batch 1.0205 (1.0884)	loss 3.6968 (12.6165)	grad_norm 5.1073 (16.0433)	mem 28677MB
Train: [0/180][8750/10009]	eta 0:22:50 lr 0.049997	data 0.0005 (0.0030)	batch 1.0426 (1.0882)	loss 3.9444 (12.5650)	grad_norm 5.1389 (15.9807)	mem 28677MB
Train: [0/180][8800/10009]	eta 0:21:55 lr 0.049997	data 0.0005 (0.0030)	batch 1.0418 (1.0880)	loss 3.7844 (12.5139)	grad_norm 5.0893 (15.9187)	mem 28677MB
Train: [0/180][8850/10009]	eta 0:21:00 lr 0.049997	data 0.0005 (0.0030)	batch 1.0329 (1.0877)	loss 3.3559 (12.4630)	grad_norm 5.0561 (15.8574)	mem 28677MB
Train: [0/180][8900/10009]	eta 0:20:05 lr 0.049997	data 0.0004 (0.0030)	batch 1.0289 (1.0874)	loss 3.6090 (12.4127)	grad_norm 5.2119 (15.7968)	mem 28677MB
Train: [0/180][8950/10009]	eta 0:19:11 lr 0.049997	data 0.0005 (0.0030)	batch 1.0223 (1.0871)	loss 3.2929 (12.3630)	grad_norm 5.1523 (15.7368)	mem 28677MB
Train: [0/180][9000/10009]	eta 0:18:16 lr 0.049997	data 0.0005 (0.0030)	batch 1.0384 (1.0868)	loss 3.5998 (12.3137)	grad_norm 5.1566 (15.6776)	mem 28677MB
Train: [0/180][9050/10009]	eta 0:17:22 lr 0.049997	data 0.0005 (0.0030)	batch 1.0203 (1.0866)	loss 3.5309 (12.2649)	grad_norm 5.0212 (15.6190)	mem 28677MB
Train: [0/180][9100/10009]	eta 0:16:27 lr 0.049997	data 0.0005 (0.0029)	batch 1.0369 (1.0863)	loss 3.6112 (12.2167)	grad_norm 5.1360 (15.5610)	mem 28677MB
Train: [0/180][9150/10009]	eta 0:15:32 lr 0.049997	data 0.0005 (0.0029)	batch 1.0324 (1.0860)	loss 3.9304 (12.1690)	grad_norm 5.2152 (15.5038)	mem 28677MB
Train: [0/180][9200/10009]	eta 0:14:38 lr 0.049997	data 0.0005 (0.0029)	batch 1.0433 (1.0857)	loss 3.5643 (12.1216)	grad_norm 5.0351 (15.4471)	mem 28677MB
Train: [0/180][9250/10009]	eta 0:13:43 lr 0.049997	data 0.0004 (0.0029)	batch 1.0455 (1.0855)	loss 3.4764 (12.0748)	grad_norm 5.0152 (15.3910)	mem 28677MB
Train: [0/180][9300/10009]	eta 0:12:49 lr 0.049997	data 0.0005 (0.0029)	batch 1.0139 (1.0852)	loss 3.6096 (12.0284)	grad_norm 5.1366 (15.3355)	mem 28677MB
Train: [0/180][9350/10009]	eta 0:11:55 lr 0.049997	data 0.0005 (0.0029)	batch 1.0353 (1.0850)	loss 3.6647 (11.9825)	grad_norm 5.1797 (15.2806)	mem 28677MB
Train: [0/180][9400/10009]	eta 0:11:00 lr 0.049997	data 0.0004 (0.0029)	batch 1.0376 (1.0847)	loss 3.5959 (11.9370)	grad_norm 5.4106 (15.2263)	mem 28677MB
Train: [0/180][9450/10009]	eta 0:10:06 lr 0.049997	data 0.0004 (0.0029)	batch 1.0406 (1.0845)	loss 3.8764 (11.8921)	grad_norm 5.2223 (15.1724)	mem 28677MB
Train: [0/180][9500/10009]	eta 0:09:11 lr 0.049997	data 0.0005 (0.0028)	batch 1.0332 (1.0842)	loss 3.1631 (11.8475)	grad_norm 4.9801 (15.1192)	mem 28677MB
Train: [0/180][9550/10009]	eta 0:08:17 lr 0.049997	data 0.0005 (0.0028)	batch 1.0373 (1.0840)	loss 3.4306 (11.8035)	grad_norm 5.0564 (15.0665)	mem 28677MB
Train: [0/180][9600/10009]	eta 0:07:23 lr 0.049996	data 0.0005 (0.0028)	batch 1.0374 (1.0837)	loss 3.3297 (11.7600)	grad_norm 5.1397 (15.0139)	mem 28677MB
Train: [0/180][9650/10009]	eta 0:06:28 lr 0.049996	data 0.0004 (0.0028)	batch 1.0450 (1.0835)	loss 3.4772 (11.7166)	grad_norm 6.8460 (14.9622)	mem 28677MB
Train: [0/180][9700/10009]	eta 0:05:34 lr 0.049996	data 0.0004 (0.0028)	batch 1.0485 (1.0833)	loss 3.1630 (11.6738)	grad_norm 5.1216 (14.9107)	mem 28677MB
Train: [0/180][9750/10009]	eta 0:04:40 lr 0.049996	data 0.0004 (0.0028)	batch 1.0439 (1.0831)	loss 3.5180 (11.6312)	grad_norm 5.2025 (14.8596)	mem 28677MB
Train: [0/180][9800/10009]	eta 0:03:46 lr 0.049996	data 0.0005 (0.0028)	batch 1.0370 (1.0829)	loss 3.4740 (11.5892)	grad_norm 4.7878 (14.8095)	mem 28677MB
Train: [0/180][9850/10009]	eta 0:02:52 lr 0.049996	data 0.0005 (0.0028)	batch 1.0232 (1.0826)	loss 3.6581 (11.5477)	grad_norm 5.2092 (14.7595)	mem 28677MB
Train: [0/180][9900/10009]	eta 0:01:57 lr 0.049996	data 0.0005 (0.0027)	batch 1.0377 (1.0824)	loss 3.2915 (11.5064)	grad_norm 5.0197 (14.7104)	mem 28677MB
Train: [0/180][9950/10009]	eta 0:01:03 lr 0.049996	data 0.0005 (0.0027)	batch 1.0345 (1.0822)	loss 3.1814 (11.4655)	grad_norm 5.0748 (14.6619)	mem 28677MB
Train: [0/180][10000/10009]	eta 0:00:09 lr 0.049996	data 0.0002 (0.0027)	batch 1.0185 (1.0820)	loss 3.2361 (11.4250)	grad_norm 4.4963 (14.6137)	mem 28677MB
Current slope: None 	
EPOCH 0 training takes 3:00:29
Test: [0/391]	Time 12.871 (12.871)	Loss 1.3860 (1.3860)	Acc@1 70.312 (70.312)	Acc@5 87.500 (87.500)	Mem 28677MB
Test: [50/391]	Time 0.255 (0.504)	Loss 1.2184 (1.7048)	Acc@1 71.875 (60.509)	Acc@5 88.281 (82.874)	Mem 28677MB
Test: [100/391]	Time 0.256 (0.381)	Loss 1.9861 (1.7631)	Acc@1 46.094 (57.488)	Acc@5 84.375 (83.338)	Mem 28677MB
Test: [150/391]	Time 0.255 (0.340)	Loss 1.6834 (1.7291)	Acc@1 49.219 (58.356)	Acc@5 88.281 (83.878)	Mem 28677MB
Test: [200/391]	Time 0.255 (0.319)	Loss 2.6963 (1.8863)	Acc@1 35.156 (55.772)	Acc@5 69.531 (81.363)	Mem 28677MB
Test: [250/391]	Time 0.275 (0.307)	Loss 2.0148 (1.9780)	Acc@1 54.688 (54.336)	Acc@5 72.656 (79.684)	Mem 28677MB
Test: [300/391]	Time 0.256 (0.298)	Loss 2.3729 (2.0503)	Acc@1 53.125 (53.187)	Acc@5 71.875 (78.460)	Mem 28677MB
Test: [350/391]	Time 0.256 (0.292)	Loss 2.3035 (2.1059)	Acc@1 50.781 (52.295)	Acc@5 71.094 (77.600)	Mem 28677MB
 * Acc@1 52.686 Acc@5 77.962
Accuracy of the network on the 50000 test images: 52.69%
Max accuracy (after decay): 52.69%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/10009]	eta 2 days, 11:31:44 lr 0.049996	data 20.0914 (20.0914)	batch 21.4111 (21.4111)	loss 2.9758 (2.9758)	grad_norm 4.9285 (4.9285)	mem 28677MB
Train: [1/180][50/10009]	eta 3:58:10 lr 0.049996	data 0.0005 (0.3945)	batch 1.0383 (1.4350)	loss 3.3330 (3.3348)	grad_norm 5.0522 (5.0437)	mem 28677MB
Train: [1/180][100/10009]	eta 3:24:31 lr 0.049996	data 0.0004 (0.1994)	batch 1.0348 (1.2384)	loss 3.2989 (3.3334)	grad_norm 4.9229 (5.0459)	mem 28677MB
Train: [1/180][150/10009]	eta 3:12:55 lr 0.049996	data 0.0005 (0.1336)	batch 1.0314 (1.1741)	loss 3.0590 (3.3594)	grad_norm 5.0594 (5.0468)	mem 28677MB
Train: [1/180][200/10009]	eta 3:06:16 lr 0.049996	data 0.0005 (0.1005)	batch 1.0435 (1.1394)	loss 3.1645 (3.3365)	grad_norm 4.9853 (5.0578)	mem 28677MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 18:36:50 lr 0.050000	data 20.4909 (20.4909)	batch 23.9594 (23.9594)	loss 26.1720 (26.1720)	grad_norm 77.7584 (77.7584)	mem 28688MB
Train: [0/180][50/10009]	eta 4:03:21 lr 0.050000	data 0.0007 (0.4025)	batch 1.0151 (1.4661)	loss 21.3813 (18.0123)	grad_norm 44.5680 (39.2141)	mem 28688MB
Train: [0/180][100/10009]	eta 3:24:57 lr 0.050000	data 0.0006 (0.2036)	batch 1.0091 (1.2410)	loss 31.3111 (22.2733)	grad_norm 38.6416 (44.2126)	mem 28688MB
Train: [0/180][150/10009]	eta 3:11:24 lr 0.050000	data 0.0006 (0.1364)	batch 1.0084 (1.1648)	loss 36.8463 (26.4985)	grad_norm 67.9949 (46.8915)	mem 28688MB
Train: [0/180][200/10009]	eta 3:04:04 lr 0.050000	data 0.0006 (0.1026)	batch 1.0102 (1.1259)	loss 34.2665 (29.4380)	grad_norm 37.1258 (47.1660)	mem 28688MB
Train: [0/180][250/10009]	eta 2:59:27 lr 0.050000	data 0.0006 (0.0823)	batch 1.0112 (1.1034)	loss 36.0970 (31.5677)	grad_norm 76.9009 (47.5399)	mem 28688MB
Train: [0/180][300/10009]	eta 2:55:57 lr 0.050000	data 0.0005 (0.0687)	batch 1.0075 (1.0874)	loss 42.1702 (32.7377)	grad_norm 78.6082 (47.6377)	mem 28688MB
Train: [0/180][350/10009]	eta 2:53:17 lr 0.050000	data 0.0006 (0.0590)	batch 1.0108 (1.0764)	loss 50.9760 (33.9115)	grad_norm 32.2644 (48.4668)	mem 28688MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 19:08:47 lr 0.050000	data 20.4998 (20.4998)	batch 24.1510 (24.1510)	loss 26.3928 (26.3928)	grad_norm 78.5018 (78.5018)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 20:37:05 lr 0.050000	data 21.1060 (21.1060)	batch 24.6803 (24.6803)	loss 26.3927 (26.3927)	grad_norm 78.5066 (78.5066)	mem 28740MB
Train: [0/180][50/10009]	eta 4:22:21 lr 0.050000	data 0.0005 (0.4145)	batch 1.1215 (1.5806)	loss 17.4167 (16.7958)	grad_norm 31.8235 (39.1475)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.8
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108795
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.8
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 16:26:09 lr 0.050000	data 20.3606 (20.3606)	batch 23.1761 (23.1761)	loss 29.3612 (29.3612)	grad_norm 109.7287 (109.7287)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.4
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 9:19:00 lr 0.050000	data 17.8729 (17.8729)	batch 20.6155 (20.6155)	loss 38.4489 (38.4489)	grad_norm 136.9953 (136.9953)	mem 24443MB
Train: [0/180][50/10009]	eta 2:54:16 lr 0.050000	data 0.0045 (0.3529)	batch 0.6450 (1.0499)	loss 6.0013 (7.2478)	grad_norm 2.8370 (6.7022)	mem 24443MB
Train: [0/180][100/10009]	eta 2:20:57 lr 0.050000	data 0.0006 (0.1792)	batch 0.6937 (0.8535)	loss 5.6775 (6.5839)	grad_norm 2.5795 (4.7527)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.4
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Keep 40.0%  activation funtions: [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.]
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::leaky_relu encountered 46 time(s)
Unsupported operator aten::sub encountered 46 time(s)
Unsupported operator aten::mul encountered 46 time(s)
Unsupported operator aten::add encountered 46 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 10:32:25 lr 0.050000	data 18.3746 (18.3746)	batch 21.0556 (21.0556)	loss 36.8887 (36.8887)	grad_norm 134.1788 (134.1788)	mem 20527MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 15:36:25 lr 0.050000	data 19.5568 (19.5568)	batch 22.8780 (22.8780)	loss 26.3925 (26.3925)	grad_norm 78.5071 (78.5071)	mem 28740MB
Train: [0/180][50/10009]	eta 4:17:47 lr 0.050000	data 0.0006 (0.3842)	batch 1.0967 (1.5531)	loss 17.3878 (17.2248)	grad_norm 19.2616 (39.7637)	mem 28740MB
Train: [0/180][100/10009]	eta 3:41:18 lr 0.050000	data 0.0007 (0.1943)	batch 1.1243 (1.3401)	loss 28.0392 (21.1658)	grad_norm 52.9788 (43.5419)	mem 28740MB
Train: [0/180][150/10009]	eta 3:28:27 lr 0.050000	data 0.0005 (0.1302)	batch 1.1091 (1.2686)	loss 36.5794 (25.0274)	grad_norm 37.7002 (44.6134)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 16:14:53 lr 0.050000	data 19.4338 (19.4338)	batch 23.1085 (23.1085)	loss 26.3926 (26.3926)	grad_norm 78.5022 (78.5022)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 12:28:29 lr 0.050000	data 18.3738 (18.3738)	batch 21.7514 (21.7514)	loss 26.3928 (26.3928)	grad_norm 78.9190 (78.9190)	mem 28723MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 15:11:17 lr 0.050000	data 19.3871 (19.3871)	batch 22.7273 (22.7273)	loss 15.8777 (15.8777)	grad_norm 30.2834 (30.2834)	mem 28724MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 13:53:04 lr 0.050000	data 18.8869 (18.8869)	batch 22.2584 (22.2584)	loss 26.3927 (26.3927)	grad_norm 78.5029 (78.5029)	mem 28740MB
Train: [0/180][50/10009]	eta 4:15:46 lr 0.050000	data 0.0006 (0.3710)	batch 1.1149 (1.5410)	loss 19.7971 (17.1180)	grad_norm 37.9164 (38.6045)	mem 28740MB
Train: [0/180][100/10009]	eta 3:40:49 lr 0.050000	data 0.0020 (0.1877)	batch 1.1438 (1.3371)	loss 34.6794 (21.7959)	grad_norm 39.8440 (43.4914)	mem 28740MB
Train: [0/180][150/10009]	eta 3:28:13 lr 0.050000	data 0.0008 (0.1258)	batch 1.1151 (1.2672)	loss 42.1576 (25.6200)	grad_norm 26.8793 (44.8707)	mem 28740MB
Train: [0/180][200/10009]	eta 3:21:11 lr 0.050000	data 0.0007 (0.0946)	batch 1.1095 (1.2306)	loss 43.8776 (28.2958)	grad_norm 31.7363 (44.7153)	mem 28740MB
Train: [0/180][250/10009]	eta 3:16:45 lr 0.050000	data 0.0005 (0.0759)	batch 1.1240 (1.2097)	loss 36.0181 (29.9919)	grad_norm 44.1459 (44.7584)	mem 28740MB
Train: [0/180][300/10009]	eta 3:13:22 lr 0.050000	data 0.0005 (0.0634)	batch 1.1189 (1.1950)	loss 35.3503 (31.1017)	grad_norm 38.3818 (45.1930)	mem 28740MB
Train: [0/180][350/10009]	eta 3:10:48 lr 0.050000	data 0.0006 (0.0544)	batch 1.1232 (1.1852)	loss 37.6685 (31.9795)	grad_norm 31.4093 (45.1518)	mem 28740MB
Train: [0/180][400/10009]	eta 3:08:38 lr 0.050000	data 0.0005 (0.0477)	batch 1.1105 (1.1779)	loss 26.9126 (32.7043)	grad_norm 32.2979 (44.9036)	mem 28740MB
Train: [0/180][450/10009]	eta 3:06:40 lr 0.050000	data 0.0006 (0.0425)	batch 1.1311 (1.1718)	loss 37.7066 (33.2279)	grad_norm 32.2750 (45.0364)	mem 28740MB
Train: [0/180][500/10009]	eta 3:04:58 lr 0.050000	data 0.0006 (0.0383)	batch 1.1230 (1.1672)	loss 37.2251 (33.7205)	grad_norm 31.5737 (45.2901)	mem 28740MB
Train: [0/180][550/10009]	eta 3:03:24 lr 0.050000	data 0.0005 (0.0349)	batch 1.1314 (1.1633)	loss 42.4326 (34.2908)	grad_norm 49.7087 (45.4069)	mem 28740MB
Train: [0/180][600/10009]	eta 3:01:55 lr 0.050000	data 0.0006 (0.0320)	batch 1.1370 (1.1601)	loss 34.8362 (34.6531)	grad_norm 67.3586 (45.5515)	mem 28740MB
Train: [0/180][650/10009]	eta 3:00:29 lr 0.050000	data 0.0006 (0.0296)	batch 1.1302 (1.1571)	loss 33.8553 (34.9306)	grad_norm 30.7120 (45.2951)	mem 28740MB
Train: [0/180][700/10009]	eta 2:59:05 lr 0.050000	data 0.0005 (0.0276)	batch 1.1148 (1.1544)	loss 40.5728 (35.1293)	grad_norm 40.0386 (45.1458)	mem 28740MB
Train: [0/180][750/10009]	eta 2:57:46 lr 0.050000	data 0.0005 (0.0258)	batch 1.1226 (1.1520)	loss 38.6506 (35.2600)	grad_norm 32.3301 (45.0586)	mem 28740MB
Train: [0/180][800/10009]	eta 2:56:25 lr 0.050000	data 0.0005 (0.0242)	batch 1.1156 (1.1495)	loss 32.0605 (35.3770)	grad_norm 30.0165 (45.0552)	mem 28740MB
Train: [0/180][850/10009]	eta 2:55:09 lr 0.050000	data 0.0007 (0.0228)	batch 1.1177 (1.1475)	loss 44.0726 (35.5426)	grad_norm 39.3739 (45.0366)	mem 28740MB
Train: [0/180][900/10009]	eta 2:53:56 lr 0.050000	data 0.0005 (0.0216)	batch 1.1181 (1.1458)	loss 39.5124 (35.6214)	grad_norm 25.6164 (44.7896)	mem 28740MB
Train: [0/180][950/10009]	eta 2:52:50 lr 0.050000	data 0.0006 (0.0205)	batch 1.1190 (1.1448)	loss 38.5569 (35.6153)	grad_norm 37.5066 (44.6632)	mem 28740MB
Train: [0/180][1000/10009]	eta 2:51:39 lr 0.050000	data 0.0005 (0.0195)	batch 1.0996 (1.1432)	loss 36.2517 (35.6726)	grad_norm 46.1211 (44.9423)	mem 28740MB
Train: [0/180][1050/10009]	eta 2:50:31 lr 0.050000	data 0.0005 (0.0186)	batch 1.1037 (1.1421)	loss 35.8613 (35.6458)	grad_norm 41.2295 (44.7748)	mem 28740MB
Train: [0/180][1100/10009]	eta 2:49:25 lr 0.050000	data 0.0006 (0.0178)	batch 1.1163 (1.1410)	loss 37.9932 (35.6081)	grad_norm 46.9647 (44.6843)	mem 28740MB
Train: [0/180][1150/10009]	eta 2:48:19 lr 0.050000	data 0.0008 (0.0170)	batch 1.1143 (1.1400)	loss 39.1624 (35.6400)	grad_norm 45.7768 (44.6441)	mem 28740MB
Train: [0/180][1200/10009]	eta 2:47:14 lr 0.050000	data 0.0005 (0.0163)	batch 1.1191 (1.1391)	loss 33.6524 (35.6446)	grad_norm 29.7090 (44.5931)	mem 28740MB
Train: [0/180][1250/10009]	eta 2:46:09 lr 0.050000	data 0.0005 (0.0157)	batch 1.1003 (1.1382)	loss 42.1040 (35.6877)	grad_norm 46.4890 (44.4950)	mem 28740MB
Train: [0/180][1300/10009]	eta 2:45:07 lr 0.050000	data 0.0005 (0.0151)	batch 1.1171 (1.1376)	loss 35.2900 (35.7107)	grad_norm 28.0676 (44.3523)	mem 28740MB
Train: [0/180][1350/10009]	eta 2:44:04 lr 0.050000	data 0.0006 (0.0146)	batch 1.1291 (1.1369)	loss 33.0685 (35.6863)	grad_norm 51.2031 (44.1474)	mem 28740MB
Train: [0/180][1400/10009]	eta 2:43:02 lr 0.050000	data 0.0006 (0.0141)	batch 1.1234 (1.1363)	loss 34.7462 (35.6376)	grad_norm 32.5611 (44.0751)	mem 28740MB
Train: [0/180][1450/10009]	eta 2:42:00 lr 0.050000	data 0.0006 (0.0136)	batch 1.1219 (1.1357)	loss 40.2228 (35.6165)	grad_norm 55.6789 (43.8358)	mem 28740MB
Train: [0/180][1500/10009]	eta 2:40:58 lr 0.050000	data 0.0006 (0.0132)	batch 1.1292 (1.1351)	loss 39.4655 (35.5761)	grad_norm 44.5788 (43.7029)	mem 28740MB
Train: [0/180][1550/10009]	eta 2:39:57 lr 0.050000	data 0.0005 (0.0128)	batch 1.1108 (1.1346)	loss 43.2307 (35.5441)	grad_norm 28.7810 (43.4548)	mem 28740MB
Train: [0/180][1600/10009]	eta 2:38:58 lr 0.050000	data 0.0005 (0.0124)	batch 1.1224 (1.1343)	loss 29.5026 (35.4359)	grad_norm 24.0636 (43.2070)	mem 28740MB
Train: [0/180][1650/10009]	eta 2:37:57 lr 0.050000	data 0.0007 (0.0120)	batch 1.1398 (1.1338)	loss 32.4848 (35.3162)	grad_norm 44.0116 (43.0132)	mem 28740MB
Train: [0/180][1700/10009]	eta 2:36:56 lr 0.050000	data 0.0005 (0.0117)	batch 1.0973 (1.1333)	loss 24.9588 (35.1491)	grad_norm 27.0713 (42.8389)	mem 28740MB
Train: [0/180][1750/10009]	eta 2:35:58 lr 0.050000	data 0.0006 (0.0114)	batch 1.1245 (1.1331)	loss 35.1977 (34.9922)	grad_norm 66.2657 (42.5581)	mem 28740MB
Train: [0/180][1800/10009]	eta 2:34:59 lr 0.050000	data 0.0006 (0.0111)	batch 1.1136 (1.1329)	loss 25.6406 (34.8051)	grad_norm 35.0751 (42.2081)	mem 28740MB
Train: [0/180][1850/10009]	eta 2:34:00 lr 0.050000	data 0.0004 (0.0108)	batch 1.1067 (1.1326)	loss 25.7065 (34.5766)	grad_norm 17.7849 (41.8659)	mem 28740MB
Train: [0/180][1900/10009]	eta 2:33:02 lr 0.050000	data 0.0005 (0.0105)	batch 1.1463 (1.1324)	loss 16.9247 (34.2966)	grad_norm 39.9721 (41.5558)	mem 28740MB
Train: [0/180][1950/10009]	eta 2:32:03 lr 0.050000	data 0.0005 (0.0103)	batch 1.1287 (1.1321)	loss 24.4472 (34.0327)	grad_norm 26.2368 (41.2051)	mem 28740MB
Train: [0/180][2000/10009]	eta 2:31:06 lr 0.050000	data 0.0005 (0.0100)	batch 1.1154 (1.1320)	loss 23.2505 (33.7698)	grad_norm 16.9634 (40.7887)	mem 28740MB
Train: [0/180][2050/10009]	eta 2:30:07 lr 0.050000	data 0.0006 (0.0098)	batch 1.1241 (1.1318)	loss 21.8409 (33.4484)	grad_norm 20.8955 (40.3564)	mem 28740MB
Train: [0/180][2100/10009]	eta 2:29:09 lr 0.050000	data 0.0006 (0.0096)	batch 1.1099 (1.1315)	loss 19.0771 (33.1227)	grad_norm 32.8935 (40.0374)	mem 28740MB
Train: [0/180][2150/10009]	eta 2:28:11 lr 0.050000	data 0.0006 (0.0094)	batch 1.1059 (1.1314)	loss 19.6735 (32.7984)	grad_norm 30.6139 (39.5723)	mem 28740MB
Train: [0/180][2200/10009]	eta 2:27:13 lr 0.050000	data 0.0005 (0.0092)	batch 1.1424 (1.1312)	loss 18.8202 (32.4818)	grad_norm 34.2764 (39.1427)	mem 28740MB
Train: [0/180][2250/10009]	eta 2:26:16 lr 0.050000	data 0.0005 (0.0090)	batch 1.1407 (1.1312)	loss 20.3098 (32.1632)	grad_norm 12.9057 (38.7544)	mem 28740MB
Train: [0/180][2300/10009]	eta 2:25:18 lr 0.050000	data 0.0006 (0.0088)	batch 1.1159 (1.1310)	loss 14.5332 (31.8181)	grad_norm 11.9029 (38.2898)	mem 28740MB
Train: [0/180][2350/10009]	eta 2:24:21 lr 0.050000	data 0.0008 (0.0086)	batch 1.2511 (1.1309)	loss 18.1069 (31.4761)	grad_norm 10.8539 (37.8756)	mem 28740MB
Train: [0/180][2400/10009]	eta 2:23:23 lr 0.050000	data 0.0005 (0.0085)	batch 1.1192 (1.1307)	loss 14.3448 (31.1404)	grad_norm 13.0095 (37.4454)	mem 28740MB
Train: [0/180][2450/10009]	eta 2:22:25 lr 0.050000	data 0.0005 (0.0083)	batch 1.1256 (1.1305)	loss 20.0949 (30.8112)	grad_norm 10.8130 (37.0314)	mem 28740MB
Train: [0/180][2500/10009]	eta 2:21:28 lr 0.050000	data 0.0006 (0.0081)	batch 1.1374 (1.1304)	loss 13.6399 (30.4923)	grad_norm 13.8783 (36.6156)	mem 28740MB
Train: [0/180][2550/10009]	eta 2:20:30 lr 0.050000	data 0.0005 (0.0080)	batch 1.1161 (1.1303)	loss 13.5135 (30.1776)	grad_norm 11.2232 (36.2197)	mem 28740MB
Train: [0/180][2600/10009]	eta 2:19:33 lr 0.050000	data 0.0005 (0.0079)	batch 1.1409 (1.1302)	loss 15.2236 (29.8710)	grad_norm 38.8679 (35.8300)	mem 28740MB
Train: [0/180][2650/10009]	eta 2:18:36 lr 0.050000	data 0.0006 (0.0077)	batch 1.1377 (1.1301)	loss 13.4567 (29.5690)	grad_norm 14.5867 (35.4848)	mem 28740MB
Train: [0/180][2700/10009]	eta 2:17:38 lr 0.050000	data 0.0004 (0.0076)	batch 1.1050 (1.1299)	loss 17.8113 (29.2813)	grad_norm 17.7664 (35.1141)	mem 28740MB
Train: [0/180][2750/10009]	eta 2:16:40 lr 0.050000	data 0.0006 (0.0075)	batch 1.1178 (1.1298)	loss 13.5309 (28.9952)	grad_norm 11.8006 (34.7738)	mem 28740MB
Train: [0/180][2800/10009]	eta 2:15:43 lr 0.050000	data 0.0005 (0.0073)	batch 1.1159 (1.1296)	loss 15.9811 (28.7182)	grad_norm 19.2678 (34.4192)	mem 28740MB
Train: [0/180][2850/10009]	eta 2:14:45 lr 0.050000	data 0.0006 (0.0072)	batch 1.1233 (1.1295)	loss 13.5856 (28.4453)	grad_norm 12.4080 (34.0822)	mem 28740MB
Train: [0/180][2900/10009]	eta 2:13:47 lr 0.050000	data 0.0005 (0.0071)	batch 1.1201 (1.1292)	loss 11.3928 (28.1769)	grad_norm 10.8110 (33.7588)	mem 28740MB
Train: [0/180][2950/10009]	eta 2:12:50 lr 0.050000	data 0.0005 (0.0070)	batch 1.1225 (1.1291)	loss 11.7436 (27.9162)	grad_norm 17.7363 (33.4542)	mem 28740MB
Train: [0/180][3000/10009]	eta 2:11:52 lr 0.050000	data 0.0005 (0.0069)	batch 1.1153 (1.1289)	loss 13.4915 (27.6461)	grad_norm 12.7197 (33.1240)	mem 28740MB
Train: [0/180][3050/10009]	eta 2:10:54 lr 0.050000	data 0.0005 (0.0068)	batch 1.1024 (1.1286)	loss 11.3124 (27.3820)	grad_norm 12.1626 (32.8017)	mem 28740MB
Train: [0/180][3100/10009]	eta 2:09:57 lr 0.050000	data 0.0006 (0.0067)	batch 1.1032 (1.1286)	loss 12.3478 (27.1226)	grad_norm 25.0034 (32.4873)	mem 28740MB
Train: [0/180][3150/10009]	eta 2:09:00 lr 0.050000	data 0.0006 (0.0066)	batch 1.1049 (1.1285)	loss 12.2758 (26.8705)	grad_norm 11.2357 (32.1942)	mem 28740MB
Train: [0/180][3200/10009]	eta 2:08:03 lr 0.050000	data 0.0005 (0.0065)	batch 1.1324 (1.1284)	loss 9.9186 (26.6175)	grad_norm 11.1624 (31.8826)	mem 28740MB
Train: [0/180][3250/10009]	eta 2:07:05 lr 0.050000	data 0.0005 (0.0064)	batch 1.1167 (1.1282)	loss 11.2872 (26.3740)	grad_norm 10.3875 (31.5869)	mem 28740MB
Train: [0/180][3300/10009]	eta 2:06:08 lr 0.050000	data 0.0006 (0.0063)	batch 1.1196 (1.1282)	loss 11.6028 (26.1340)	grad_norm 11.2445 (31.2954)	mem 28740MB
Train: [0/180][3350/10009]	eta 2:05:11 lr 0.050000	data 0.0005 (0.0062)	batch 1.1253 (1.1280)	loss 10.1979 (25.9022)	grad_norm 9.1513 (31.0176)	mem 28740MB
Train: [0/180][3400/10009]	eta 2:04:14 lr 0.050000	data 0.0005 (0.0061)	batch 1.1065 (1.1279)	loss 9.9199 (25.6705)	grad_norm 8.7186 (30.7337)	mem 28740MB
Train: [0/180][3450/10009]	eta 2:03:17 lr 0.050000	data 0.0005 (0.0061)	batch 1.1183 (1.1278)	loss 11.1454 (25.4390)	grad_norm 9.3329 (30.4612)	mem 28740MB
Train: [0/180][3500/10009]	eta 2:02:20 lr 0.050000	data 0.0005 (0.0060)	batch 1.1407 (1.1277)	loss 8.8509 (25.2145)	grad_norm 8.2429 (30.1918)	mem 28740MB
Train: [0/180][3550/10009]	eta 2:01:23 lr 0.050000	data 0.0006 (0.0059)	batch 1.1172 (1.1276)	loss 8.6365 (24.9933)	grad_norm 10.7485 (29.9235)	mem 28740MB
Train: [0/180][3600/10009]	eta 2:00:26 lr 0.050000	data 0.0005 (0.0058)	batch 1.1483 (1.1275)	loss 8.4738 (24.7760)	grad_norm 8.3498 (29.6735)	mem 28740MB
Train: [0/180][3650/10009]	eta 1:59:29 lr 0.049999	data 0.0005 (0.0058)	batch 1.1174 (1.1275)	loss 7.7335 (24.5617)	grad_norm 10.6947 (29.4259)	mem 28740MB
Train: [0/180][3700/10009]	eta 1:58:32 lr 0.049999	data 0.0005 (0.0057)	batch 1.1073 (1.1273)	loss 9.3518 (24.3446)	grad_norm 10.4417 (29.1655)	mem 28740MB
Train: [0/180][3750/10009]	eta 1:57:35 lr 0.049999	data 0.0005 (0.0056)	batch 1.1236 (1.1272)	loss 7.9919 (24.1265)	grad_norm 8.4813 (28.9091)	mem 28740MB
Train: [0/180][3800/10009]	eta 1:56:38 lr 0.049999	data 0.0006 (0.0056)	batch 1.1130 (1.1272)	loss 8.0064 (23.9159)	grad_norm 11.5490 (28.6582)	mem 28740MB
Train: [0/180][3850/10009]	eta 1:55:41 lr 0.049999	data 0.0006 (0.0055)	batch 1.1378 (1.1271)	loss 6.8547 (23.7032)	grad_norm 8.6049 (28.4083)	mem 28740MB
Train: [0/180][3900/10009]	eta 1:54:45 lr 0.049999	data 0.0005 (0.0054)	batch 1.1181 (1.1271)	loss 7.7956 (23.4919)	grad_norm 9.0113 (28.1643)	mem 28740MB
Train: [0/180][3950/10009]	eta 1:53:48 lr 0.049999	data 0.0005 (0.0054)	batch 1.1199 (1.1270)	loss 6.5333 (23.2838)	grad_norm 8.5811 (27.9243)	mem 28740MB
Train: [0/180][4000/10009]	eta 1:52:51 lr 0.049999	data 0.0005 (0.0053)	batch 1.1315 (1.1269)	loss 6.0023 (23.0783)	grad_norm 7.5451 (27.6860)	mem 28740MB
Train: [0/180][4050/10009]	eta 1:51:54 lr 0.049999	data 0.0005 (0.0052)	batch 1.1397 (1.1268)	loss 6.7279 (22.8735)	grad_norm 8.8896 (27.4488)	mem 28740MB
Train: [0/180][4100/10009]	eta 1:50:57 lr 0.049999	data 0.0005 (0.0052)	batch 1.1136 (1.1266)	loss 6.6155 (22.6729)	grad_norm 8.0001 (27.2166)	mem 28740MB
Train: [0/180][4150/10009]	eta 1:50:00 lr 0.049999	data 0.0005 (0.0051)	batch 1.1051 (1.1265)	loss 6.2124 (22.4745)	grad_norm 7.9127 (26.9898)	mem 28740MB
Train: [0/180][4200/10009]	eta 1:49:03 lr 0.049999	data 0.0005 (0.0051)	batch 1.1019 (1.1264)	loss 5.6607 (22.2789)	grad_norm 7.1692 (26.7647)	mem 28740MB
Train: [0/180][4250/10009]	eta 1:48:06 lr 0.049999	data 0.0005 (0.0050)	batch 1.1180 (1.1264)	loss 5.2078 (22.0849)	grad_norm 6.7084 (26.5422)	mem 28740MB
Train: [0/180][4300/10009]	eta 1:47:10 lr 0.049999	data 0.0005 (0.0050)	batch 1.1128 (1.1263)	loss 5.7095 (21.8941)	grad_norm 7.7719 (26.3243)	mem 28740MB
Train: [0/180][4350/10009]	eta 1:46:13 lr 0.049999	data 0.0006 (0.0049)	batch 1.1073 (1.1263)	loss 5.6113 (21.7077)	grad_norm 7.6514 (26.1101)	mem 28740MB
Train: [0/180][4400/10009]	eta 1:45:16 lr 0.049999	data 0.0006 (0.0049)	batch 1.1149 (1.1262)	loss 5.7709 (21.5222)	grad_norm 10.2229 (25.8971)	mem 28740MB
Train: [0/180][4450/10009]	eta 1:44:20 lr 0.049999	data 0.0006 (0.0048)	batch 1.1212 (1.1261)	loss 5.5522 (21.3419)	grad_norm 7.1192 (25.6901)	mem 28740MB
Train: [0/180][4500/10009]	eta 1:43:23 lr 0.049999	data 0.0005 (0.0048)	batch 1.1175 (1.1260)	loss 5.2670 (21.1642)	grad_norm 7.3054 (25.4856)	mem 28740MB
Train: [0/180][4550/10009]	eta 1:42:26 lr 0.049999	data 0.0005 (0.0047)	batch 1.1102 (1.1259)	loss 5.0260 (20.9883)	grad_norm 6.8389 (25.2833)	mem 28740MB
Train: [0/180][4600/10009]	eta 1:41:29 lr 0.049999	data 0.0005 (0.0047)	batch 1.1382 (1.1259)	loss 4.9967 (20.8166)	grad_norm 6.6727 (25.0855)	mem 28740MB
Train: [0/180][4650/10009]	eta 1:40:33 lr 0.049999	data 0.0005 (0.0046)	batch 1.1469 (1.1258)	loss 5.2373 (20.6497)	grad_norm 6.1985 (24.8933)	mem 28740MB
Train: [0/180][4700/10009]	eta 1:39:36 lr 0.049999	data 0.0005 (0.0046)	batch 1.2616 (1.1258)	loss 4.4832 (20.4839)	grad_norm 5.8047 (24.7008)	mem 28740MB
Train: [0/180][4750/10009]	eta 1:38:39 lr 0.049999	data 0.0009 (0.0046)	batch 1.1430 (1.1257)	loss 4.9482 (20.3208)	grad_norm 6.5269 (24.5130)	mem 28740MB
Train: [0/180][4800/10009]	eta 1:37:43 lr 0.049999	data 0.0002 (0.0045)	batch 1.1046 (1.1256)	loss 4.8631 (20.1615)	grad_norm 6.1983 (24.3275)	mem 28740MB
Train: [0/180][4850/10009]	eta 1:36:46 lr 0.049999	data 0.0005 (0.0045)	batch 1.1107 (1.1255)	loss 4.9140 (20.0041)	grad_norm 7.1949 (24.1439)	mem 28740MB
Train: [0/180][4900/10009]	eta 1:35:49 lr 0.049999	data 0.0005 (0.0044)	batch 1.1264 (1.1254)	loss 4.8630 (19.8489)	grad_norm 7.3519 (23.9633)	mem 28740MB
Train: [0/180][4950/10009]	eta 1:34:53 lr 0.049999	data 0.0005 (0.0044)	batch 1.1097 (1.1254)	loss 4.1733 (19.6974)	grad_norm 5.8375 (23.7856)	mem 28740MB
Train: [0/180][5000/10009]	eta 1:33:56 lr 0.049999	data 0.0005 (0.0044)	batch 1.1188 (1.1253)	loss 4.7643 (19.5486)	grad_norm 6.1539 (23.6107)	mem 28740MB
Train: [0/180][5050/10009]	eta 1:32:59 lr 0.049999	data 0.0005 (0.0043)	batch 1.1091 (1.1252)	loss 5.0862 (19.4024)	grad_norm 6.3645 (23.4388)	mem 28740MB
Train: [0/180][5100/10009]	eta 1:32:03 lr 0.049999	data 0.0007 (0.0043)	batch 1.1105 (1.1251)	loss 4.5658 (19.2589)	grad_norm 5.7374 (23.2703)	mem 28740MB
Train: [0/180][5150/10009]	eta 1:31:06 lr 0.049999	data 0.0005 (0.0042)	batch 1.1173 (1.1251)	loss 5.0125 (19.1171)	grad_norm 6.1914 (23.1030)	mem 28740MB
Train: [0/180][5200/10009]	eta 1:30:10 lr 0.049999	data 0.0018 (0.0042)	batch 1.1018 (1.1250)	loss 4.6421 (18.9784)	grad_norm 5.9226 (22.9386)	mem 28740MB
Train: [0/180][5250/10009]	eta 1:29:13 lr 0.049999	data 0.0005 (0.0042)	batch 1.1088 (1.1249)	loss 4.7720 (18.8418)	grad_norm 6.2699 (22.7764)	mem 28740MB
Train: [0/180][5300/10009]	eta 1:28:17 lr 0.049999	data 0.0005 (0.0041)	batch 1.1098 (1.1249)	loss 4.3340 (18.7073)	grad_norm 5.5002 (22.6163)	mem 28740MB
Train: [0/180][5350/10009]	eta 1:27:20 lr 0.049999	data 0.0004 (0.0041)	batch 1.1135 (1.1248)	loss 4.6351 (18.5751)	grad_norm 6.1468 (22.4587)	mem 28740MB
Train: [0/180][5400/10009]	eta 1:26:23 lr 0.049999	data 0.0005 (0.0041)	batch 1.1179 (1.1247)	loss 4.5356 (18.4445)	grad_norm 5.3267 (22.3026)	mem 28740MB
Train: [0/180][5450/10009]	eta 1:25:27 lr 0.049999	data 0.0006 (0.0040)	batch 1.1224 (1.1247)	loss 3.9553 (18.3162)	grad_norm 5.4287 (22.1488)	mem 28740MB
Train: [0/180][5500/10009]	eta 1:24:31 lr 0.049999	data 0.0006 (0.0040)	batch 1.1275 (1.1247)	loss 4.3974 (18.1895)	grad_norm 4.9404 (21.9971)	mem 28740MB
Train: [0/180][5550/10009]	eta 1:23:35 lr 0.049999	data 0.0005 (0.0040)	batch 1.1426 (1.1247)	loss 4.5895 (18.0653)	grad_norm 5.5476 (21.8479)	mem 28740MB
Train: [0/180][5600/10009]	eta 1:22:38 lr 0.049999	data 0.0004 (0.0039)	batch 1.1157 (1.1247)	loss 4.2198 (17.9435)	grad_norm 5.3116 (21.7006)	mem 28740MB
Train: [0/180][5650/10009]	eta 1:21:42 lr 0.049999	data 0.0005 (0.0039)	batch 1.1461 (1.1248)	loss 3.9525 (17.8234)	grad_norm 4.9341 (21.5549)	mem 28740MB
Train: [0/180][5700/10009]	eta 1:20:46 lr 0.049999	data 0.0005 (0.0039)	batch 1.1413 (1.1248)	loss 4.3817 (17.7051)	grad_norm 4.7801 (21.4113)	mem 28740MB
Train: [0/180][5750/10009]	eta 1:19:50 lr 0.049999	data 0.0005 (0.0039)	batch 1.1353 (1.1248)	loss 4.1919 (17.5888)	grad_norm 4.7980 (21.2690)	mem 28740MB
Train: [0/180][5800/10009]	eta 1:18:54 lr 0.049999	data 0.0005 (0.0038)	batch 1.1229 (1.1248)	loss 4.2962 (17.4746)	grad_norm 5.0405 (21.1294)	mem 28740MB
Train: [0/180][5850/10009]	eta 1:17:57 lr 0.049999	data 0.0004 (0.0038)	batch 1.1214 (1.1247)	loss 4.6276 (17.3618)	grad_norm 5.2522 (20.9916)	mem 28740MB
Train: [0/180][5900/10009]	eta 1:17:01 lr 0.049999	data 0.0009 (0.0038)	batch 1.1182 (1.1248)	loss 4.3917 (17.2504)	grad_norm 4.9185 (20.8547)	mem 28740MB
Train: [0/180][5950/10009]	eta 1:16:05 lr 0.049999	data 0.0006 (0.0037)	batch 1.0990 (1.1247)	loss 4.2629 (17.1402)	grad_norm 4.7170 (20.7196)	mem 28740MB
Train: [0/180][6000/10009]	eta 1:15:09 lr 0.049999	data 0.0005 (0.0037)	batch 1.1176 (1.1247)	loss 4.1494 (17.0317)	grad_norm 5.1076 (20.5860)	mem 28740MB
Train: [0/180][6050/10009]	eta 1:14:12 lr 0.049999	data 0.0005 (0.0037)	batch 1.1082 (1.1246)	loss 3.9751 (16.9247)	grad_norm 4.2930 (20.4539)	mem 28740MB
Train: [0/180][6100/10009]	eta 1:13:16 lr 0.049999	data 0.0005 (0.0037)	batch 1.1373 (1.1246)	loss 4.0726 (16.8189)	grad_norm 4.6179 (20.3236)	mem 28740MB
Train: [0/180][6150/10009]	eta 1:12:19 lr 0.049999	data 0.0005 (0.0036)	batch 1.1159 (1.1246)	loss 4.0440 (16.7148)	grad_norm 4.4829 (20.1955)	mem 28740MB
Train: [0/180][6200/10009]	eta 1:11:23 lr 0.049999	data 0.0005 (0.0036)	batch 1.1120 (1.1245)	loss 3.8807 (16.6121)	grad_norm 4.1511 (20.0680)	mem 28740MB
Train: [0/180][6250/10009]	eta 1:10:26 lr 0.049999	data 0.0005 (0.0036)	batch 1.1165 (1.1245)	loss 4.4899 (16.5107)	grad_norm 4.6332 (19.9420)	mem 28740MB
Train: [0/180][6300/10009]	eta 1:09:30 lr 0.049998	data 0.0004 (0.0036)	batch 1.1044 (1.1244)	loss 3.6976 (16.4107)	grad_norm 4.2109 (19.8180)	mem 28740MB
Train: [0/180][6350/10009]	eta 1:08:34 lr 0.049998	data 0.0005 (0.0035)	batch 1.1298 (1.1244)	loss 4.1565 (16.3121)	grad_norm 4.3179 (19.6949)	mem 28740MB
Train: [0/180][6400/10009]	eta 1:07:37 lr 0.049998	data 0.0005 (0.0035)	batch 1.1186 (1.1244)	loss 3.9623 (16.2143)	grad_norm 4.1717 (19.5727)	mem 28740MB
Train: [0/180][6450/10009]	eta 1:06:41 lr 0.049998	data 0.0005 (0.0035)	batch 1.1339 (1.1244)	loss 3.7380 (16.1181)	grad_norm 4.0317 (19.4526)	mem 28740MB
Train: [0/180][6500/10009]	eta 1:05:45 lr 0.049998	data 0.0005 (0.0035)	batch 1.1122 (1.1244)	loss 3.5938 (16.0234)	grad_norm 3.7454 (19.3339)	mem 28740MB
Train: [0/180][6550/10009]	eta 1:04:49 lr 0.049998	data 0.0005 (0.0035)	batch 1.1134 (1.1244)	loss 3.9085 (15.9293)	grad_norm 4.3334 (19.2160)	mem 28740MB
Train: [0/180][6600/10009]	eta 1:03:52 lr 0.049998	data 0.0005 (0.0034)	batch 1.1160 (1.1244)	loss 3.4609 (15.8367)	grad_norm 3.7001 (19.1001)	mem 28740MB
Train: [0/180][6650/10009]	eta 1:02:56 lr 0.049998	data 0.0006 (0.0034)	batch 1.1057 (1.1243)	loss 3.7269 (15.7453)	grad_norm 3.6437 (18.9851)	mem 28740MB
Train: [0/180][6700/10009]	eta 1:02:00 lr 0.049998	data 0.0005 (0.0034)	batch 1.1055 (1.1243)	loss 3.3448 (15.6551)	grad_norm 3.5972 (18.8711)	mem 28740MB
Train: [0/180][6750/10009]	eta 1:01:04 lr 0.049998	data 0.0005 (0.0034)	batch 1.1199 (1.1243)	loss 3.8695 (15.5660)	grad_norm 3.6970 (18.7590)	mem 28740MB
Train: [0/180][6800/10009]	eta 1:00:07 lr 0.049998	data 0.0005 (0.0033)	batch 1.1216 (1.1243)	loss 3.7732 (15.4776)	grad_norm 3.9070 (18.6480)	mem 28740MB
Train: [0/180][6850/10009]	eta 0:59:11 lr 0.049998	data 0.0005 (0.0033)	batch 1.1369 (1.1243)	loss 3.6752 (15.3911)	grad_norm 3.4742 (18.5387)	mem 28740MB
Train: [0/180][6900/10009]	eta 0:58:15 lr 0.049998	data 0.0005 (0.0033)	batch 1.1195 (1.1243)	loss 3.5893 (15.3052)	grad_norm 3.6997 (18.4304)	mem 28740MB
Train: [0/180][6950/10009]	eta 0:57:19 lr 0.049998	data 0.0005 (0.0033)	batch 1.1110 (1.1242)	loss 3.3537 (15.2204)	grad_norm 3.5913 (18.3232)	mem 28740MB
Train: [0/180][7000/10009]	eta 0:56:22 lr 0.049998	data 0.0005 (0.0033)	batch 1.1145 (1.1242)	loss 3.6359 (15.1371)	grad_norm 3.4837 (18.2176)	mem 28740MB
Train: [0/180][7050/10009]	eta 0:55:26 lr 0.049998	data 0.0006 (0.0032)	batch 1.2999 (1.1243)	loss 3.0635 (15.0543)	grad_norm 3.1545 (18.1127)	mem 28740MB
Train: [0/180][7100/10009]	eta 0:54:30 lr 0.049998	data 0.0010 (0.0032)	batch 1.1101 (1.1243)	loss 3.6550 (14.9727)	grad_norm 3.3582 (18.0091)	mem 28740MB
Train: [0/180][7150/10009]	eta 0:53:34 lr 0.049998	data 0.0005 (0.0032)	batch 1.1207 (1.1242)	loss 3.8608 (14.8919)	grad_norm 3.6871 (17.9063)	mem 28740MB
Train: [0/180][7200/10009]	eta 0:52:38 lr 0.049998	data 0.0005 (0.0032)	batch 1.1202 (1.1243)	loss 3.5167 (14.8121)	grad_norm 3.2635 (17.8053)	mem 28740MB
Train: [0/180][7250/10009]	eta 0:51:41 lr 0.049998	data 0.0005 (0.0032)	batch 1.1230 (1.1242)	loss 3.5961 (14.7333)	grad_norm 3.2681 (17.7049)	mem 28740MB
Train: [0/180][7300/10009]	eta 0:50:45 lr 0.049998	data 0.0005 (0.0032)	batch 1.1082 (1.1242)	loss 3.4744 (14.6556)	grad_norm 3.2595 (17.6061)	mem 28740MB
Train: [0/180][7350/10009]	eta 0:49:49 lr 0.049998	data 0.0005 (0.0031)	batch 1.1194 (1.1242)	loss 3.1938 (14.5783)	grad_norm 3.1595 (17.5077)	mem 28740MB
Train: [0/180][7400/10009]	eta 0:48:53 lr 0.049998	data 0.0006 (0.0031)	batch 1.1286 (1.1242)	loss 3.2885 (14.5021)	grad_norm 3.1333 (17.4109)	mem 28740MB
Train: [0/180][7450/10009]	eta 0:47:56 lr 0.049998	data 0.0005 (0.0031)	batch 1.1172 (1.1242)	loss 3.2686 (14.4266)	grad_norm 3.0009 (17.3147)	mem 28740MB
Train: [0/180][7500/10009]	eta 0:47:00 lr 0.049998	data 0.0005 (0.0031)	batch 1.1413 (1.1242)	loss 3.1074 (14.3522)	grad_norm 2.9109 (17.2201)	mem 28740MB
Train: [0/180][7550/10009]	eta 0:46:04 lr 0.049998	data 0.0005 (0.0031)	batch 1.1334 (1.1242)	loss 3.3269 (14.2789)	grad_norm 3.0549 (17.1262)	mem 28740MB
Train: [0/180][7600/10009]	eta 0:45:08 lr 0.049998	data 0.0005 (0.0031)	batch 1.1186 (1.1242)	loss 3.3606 (14.2065)	grad_norm 3.0825 (17.0338)	mem 28740MB
Train: [0/180][7650/10009]	eta 0:44:12 lr 0.049998	data 0.0005 (0.0030)	batch 1.1195 (1.1242)	loss 3.2309 (14.1350)	grad_norm 2.9226 (16.9425)	mem 28740MB
Train: [0/180][7700/10009]	eta 0:43:15 lr 0.049998	data 0.0005 (0.0030)	batch 1.1180 (1.1242)	loss 3.0084 (14.0644)	grad_norm 2.9605 (16.8521)	mem 28740MB
Train: [0/180][7750/10009]	eta 0:42:19 lr 0.049998	data 0.0005 (0.0030)	batch 1.1243 (1.1242)	loss 3.1022 (13.9945)	grad_norm 2.8181 (16.7626)	mem 28740MB
Train: [0/180][7800/10009]	eta 0:41:23 lr 0.049998	data 0.0006 (0.0030)	batch 1.0971 (1.1242)	loss 2.9428 (13.9255)	grad_norm 2.6548 (16.6740)	mem 28740MB
Train: [0/180][7850/10009]	eta 0:40:27 lr 0.049998	data 0.0005 (0.0030)	batch 1.1188 (1.1242)	loss 3.0671 (13.8570)	grad_norm 2.6585 (16.5862)	mem 28740MB
Train: [0/180][7900/10009]	eta 0:39:31 lr 0.049998	data 0.0006 (0.0030)	batch 1.1116 (1.1242)	loss 3.2179 (13.7894)	grad_norm 2.9195 (16.4995)	mem 28740MB
Train: [0/180][7950/10009]	eta 0:38:34 lr 0.049998	data 0.0005 (0.0029)	batch 1.1034 (1.1242)	loss 3.3105 (13.7227)	grad_norm 2.9899 (16.4138)	mem 28740MB
Train: [0/180][8000/10009]	eta 0:37:38 lr 0.049998	data 0.0005 (0.0029)	batch 1.1196 (1.1242)	loss 3.2119 (13.6569)	grad_norm 2.7862 (16.3288)	mem 28740MB
Train: [0/180][8050/10009]	eta 0:36:42 lr 0.049998	data 0.0005 (0.0029)	batch 1.1184 (1.1242)	loss 2.9876 (13.5913)	grad_norm 2.5515 (16.2447)	mem 28740MB
Train: [0/180][8100/10009]	eta 0:35:46 lr 0.049998	data 0.0005 (0.0029)	batch 1.1141 (1.1242)	loss 3.0530 (13.5269)	grad_norm 2.8304 (16.1618)	mem 28740MB
Train: [0/180][8150/10009]	eta 0:34:49 lr 0.049997	data 0.0005 (0.0029)	batch 1.1226 (1.1242)	loss 3.3490 (13.4631)	grad_norm 2.7947 (16.0796)	mem 28740MB
Train: [0/180][8200/10009]	eta 0:33:53 lr 0.049997	data 0.0007 (0.0029)	batch 1.1203 (1.1242)	loss 3.0309 (13.3997)	grad_norm 2.7267 (15.9985)	mem 28740MB
Train: [0/180][8250/10009]	eta 0:32:57 lr 0.049997	data 0.0006 (0.0029)	batch 1.1059 (1.1242)	loss 3.3887 (13.3373)	grad_norm 3.1083 (15.9182)	mem 28740MB
Train: [0/180][8300/10009]	eta 0:32:01 lr 0.049997	data 0.0005 (0.0028)	batch 1.1187 (1.1242)	loss 3.1821 (13.2752)	grad_norm 2.8339 (15.8384)	mem 28740MB
Train: [0/180][8350/10009]	eta 0:31:05 lr 0.049997	data 0.0005 (0.0028)	batch 1.1195 (1.1242)	loss 3.2689 (13.2142)	grad_norm 2.6485 (15.7598)	mem 28740MB
Train: [0/180][8400/10009]	eta 0:30:08 lr 0.049997	data 0.0005 (0.0028)	batch 1.1150 (1.1242)	loss 2.8607 (13.1538)	grad_norm 2.5587 (15.6817)	mem 28740MB
Train: [0/180][8450/10009]	eta 0:29:12 lr 0.049997	data 0.0006 (0.0028)	batch 1.1236 (1.1242)	loss 2.7943 (13.0937)	grad_norm 2.5206 (15.6047)	mem 28740MB
Train: [0/180][8500/10009]	eta 0:28:16 lr 0.049997	data 0.0005 (0.0028)	batch 1.1186 (1.1243)	loss 2.8746 (13.0349)	grad_norm 2.8216 (15.5286)	mem 28740MB
Train: [0/180][8550/10009]	eta 0:27:20 lr 0.049997	data 0.0006 (0.0028)	batch 1.1449 (1.1243)	loss 3.0683 (12.9765)	grad_norm 2.5656 (15.4530)	mem 28740MB
Train: [0/180][8600/10009]	eta 0:26:24 lr 0.049997	data 0.0005 (0.0028)	batch 1.1189 (1.1242)	loss 2.7640 (12.9185)	grad_norm 2.6895 (15.3781)	mem 28740MB
Train: [0/180][8650/10009]	eta 0:25:27 lr 0.049997	data 0.0005 (0.0028)	batch 1.1207 (1.1242)	loss 2.7629 (12.8614)	grad_norm 2.5852 (15.3041)	mem 28740MB
Train: [0/180][8700/10009]	eta 0:24:31 lr 0.049997	data 0.0006 (0.0027)	batch 1.1258 (1.1242)	loss 3.1617 (12.8046)	grad_norm 2.8192 (15.2311)	mem 28740MB
Train: [0/180][8750/10009]	eta 0:23:35 lr 0.049997	data 0.0005 (0.0027)	batch 1.1373 (1.1242)	loss 3.2112 (12.7486)	grad_norm 2.4982 (15.1585)	mem 28740MB
Train: [0/180][8800/10009]	eta 0:22:39 lr 0.049997	data 0.0005 (0.0027)	batch 1.1161 (1.1242)	loss 3.1113 (12.6932)	grad_norm 2.5945 (15.0868)	mem 28740MB
Train: [0/180][8850/10009]	eta 0:21:42 lr 0.049997	data 0.0007 (0.0027)	batch 1.1352 (1.1242)	loss 2.7589 (12.6382)	grad_norm 2.3236 (15.0158)	mem 28740MB
Train: [0/180][8900/10009]	eta 0:20:46 lr 0.049997	data 0.0005 (0.0027)	batch 1.1061 (1.1242)	loss 2.8125 (12.5836)	grad_norm 2.3616 (14.9454)	mem 28740MB
Train: [0/180][8950/10009]	eta 0:19:50 lr 0.049997	data 0.0005 (0.0027)	batch 1.1251 (1.1242)	loss 2.8535 (12.5298)	grad_norm 2.5876 (14.8761)	mem 28740MB
Train: [0/180][9000/10009]	eta 0:18:54 lr 0.049997	data 0.0005 (0.0027)	batch 1.1109 (1.1242)	loss 2.9319 (12.4765)	grad_norm 2.5771 (14.8072)	mem 28740MB
Train: [0/180][9050/10009]	eta 0:17:58 lr 0.049997	data 0.0006 (0.0027)	batch 1.1239 (1.1242)	loss 3.0401 (12.4237)	grad_norm 2.4172 (14.7389)	mem 28740MB
Train: [0/180][9100/10009]	eta 0:17:01 lr 0.049997	data 0.0005 (0.0026)	batch 1.1303 (1.1242)	loss 2.9265 (12.3715)	grad_norm 2.3172 (14.6714)	mem 28740MB
Train: [0/180][9150/10009]	eta 0:16:05 lr 0.049997	data 0.0005 (0.0026)	batch 1.1220 (1.1242)	loss 2.7853 (12.3199)	grad_norm 2.4774 (14.6047)	mem 28740MB
Train: [0/180][9200/10009]	eta 0:15:09 lr 0.049997	data 0.0005 (0.0026)	batch 1.1239 (1.1242)	loss 3.0253 (12.2685)	grad_norm 2.6614 (14.5384)	mem 28740MB
Train: [0/180][9250/10009]	eta 0:14:13 lr 0.049997	data 0.0005 (0.0026)	batch 1.1270 (1.1242)	loss 2.8807 (12.2179)	grad_norm 2.2132 (14.4728)	mem 28740MB
Train: [0/180][9300/10009]	eta 0:13:17 lr 0.049997	data 0.0005 (0.0026)	batch 1.1117 (1.1242)	loss 3.1229 (12.1678)	grad_norm 2.5891 (14.4079)	mem 28740MB
Train: [0/180][9350/10009]	eta 0:12:20 lr 0.049997	data 0.0005 (0.0026)	batch 1.1278 (1.1242)	loss 2.8542 (12.1181)	grad_norm 2.2631 (14.3435)	mem 28740MB
Train: [0/180][9400/10009]	eta 0:11:24 lr 0.049997	data 0.0006 (0.0026)	batch 1.2990 (1.1242)	loss 3.1183 (12.0692)	grad_norm 2.5718 (14.2802)	mem 28740MB
Train: [0/180][9450/10009]	eta 0:10:28 lr 0.049997	data 0.0005 (0.0026)	batch 1.1159 (1.1242)	loss 2.9771 (12.0207)	grad_norm 2.1955 (14.2173)	mem 28740MB
Train: [0/180][9500/10009]	eta 0:09:32 lr 0.049997	data 0.0005 (0.0026)	batch 1.1117 (1.1242)	loss 2.7411 (11.9726)	grad_norm 2.2692 (14.1549)	mem 28740MB
Train: [0/180][9550/10009]	eta 0:08:36 lr 0.049997	data 0.0005 (0.0025)	batch 1.1044 (1.1242)	loss 2.9027 (11.9249)	grad_norm 2.3205 (14.0932)	mem 28740MB
Train: [0/180][9600/10009]	eta 0:07:39 lr 0.049996	data 0.0005 (0.0025)	batch 1.1299 (1.1242)	loss 2.8645 (11.8778)	grad_norm 2.3758 (14.0319)	mem 28740MB
Train: [0/180][9650/10009]	eta 0:06:43 lr 0.049996	data 0.0005 (0.0025)	batch 1.1170 (1.1242)	loss 2.9082 (11.8311)	grad_norm 2.2398 (13.9712)	mem 28740MB
Train: [0/180][9700/10009]	eta 0:05:47 lr 0.049996	data 0.0005 (0.0025)	batch 1.1170 (1.1242)	loss 2.7358 (11.7849)	grad_norm 2.2773 (13.9112)	mem 28740MB
Train: [0/180][9750/10009]	eta 0:04:51 lr 0.049996	data 0.0005 (0.0025)	batch 1.1535 (1.1242)	loss 2.9676 (11.7390)	grad_norm 2.5067 (13.8515)	mem 28740MB
Train: [0/180][9800/10009]	eta 0:03:54 lr 0.049996	data 0.0004 (0.0025)	batch 1.1089 (1.1242)	loss 2.9390 (11.6936)	grad_norm 2.3021 (13.7926)	mem 28740MB
Train: [0/180][9850/10009]	eta 0:02:58 lr 0.049996	data 0.0006 (0.0025)	batch 1.1298 (1.1242)	loss 3.1944 (11.6487)	grad_norm 2.3901 (13.7342)	mem 28740MB
Train: [0/180][9900/10009]	eta 0:02:02 lr 0.049996	data 0.0005 (0.0025)	batch 1.1059 (1.1242)	loss 2.8730 (11.6041)	grad_norm 2.3506 (13.6765)	mem 28740MB
Train: [0/180][9950/10009]	eta 0:01:06 lr 0.049996	data 0.0005 (0.0025)	batch 1.1448 (1.1242)	loss 2.3779 (11.5600)	grad_norm 2.0644 (13.6191)	mem 28740MB
Train: [0/180][10000/10009]	eta 0:00:10 lr 0.049996	data 0.0002 (0.0025)	batch 1.0966 (1.1242)	loss 2.4723 (11.5161)	grad_norm 2.2503 (13.5623)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 0 training takes 3:07:32
Test: [0/391]	Time 13.104 (13.104)	Loss 1.0508 (1.0508)	Acc@1 74.219 (74.219)	Acc@5 92.969 (92.969)	Mem 28740MB
Test: [50/391]	Time 0.285 (0.521)	Loss 0.9194 (1.4256)	Acc@1 75.781 (65.717)	Acc@5 91.406 (86.336)	Mem 28740MB
Test: [100/391]	Time 0.300 (0.396)	Loss 1.4835 (1.4483)	Acc@1 59.375 (63.204)	Acc@5 85.938 (87.028)	Mem 28740MB
Test: [150/391]	Time 0.267 (0.353)	Loss 1.4570 (1.4169)	Acc@1 54.688 (64.197)	Acc@5 89.844 (87.521)	Mem 28740MB
Test: [200/391]	Time 0.289 (0.332)	Loss 1.6581 (1.5732)	Acc@1 56.250 (61.482)	Acc@5 85.938 (85.199)	Mem 28740MB
Test: [250/391]	Time 0.293 (0.319)	Loss 1.8376 (1.6654)	Acc@1 63.281 (59.998)	Acc@5 78.906 (83.696)	Mem 28740MB
Test: [300/391]	Time 0.271 (0.311)	Loss 1.7244 (1.7457)	Acc@1 68.750 (58.656)	Acc@5 81.250 (82.447)	Mem 28740MB
Test: [350/391]	Time 0.283 (0.305)	Loss 1.7898 (1.8081)	Acc@1 61.719 (57.530)	Acc@5 79.688 (81.490)	Mem 28740MB
 * Acc@1 57.912 Acc@5 81.818
Accuracy of the network on the 50000 test images: 57.91%
Max accuracy (after decay): 57.91%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/10009]	eta 2 days, 12:42:50 lr 0.049996	data 20.6083 (20.6083)	batch 21.8374 (21.8374)	loss 2.4098 (2.4098)	grad_norm 2.3526 (2.3526)	mem 28740MB
Train: [1/180][50/10009]	eta 4:13:52 lr 0.049996	data 0.0005 (0.4046)	batch 1.1127 (1.5295)	loss 2.8129 (2.7875)	grad_norm 2.2132 (2.2615)	mem 28740MB
Train: [1/180][100/10009]	eta 3:39:01 lr 0.049996	data 0.0005 (0.2046)	batch 1.1099 (1.3262)	loss 2.6236 (2.7907)	grad_norm 2.0213 (2.2419)	mem 28740MB
Train: [1/180][150/10009]	eta 3:26:48 lr 0.049996	data 0.0005 (0.1370)	batch 1.1181 (1.2585)	loss 2.6751 (2.8012)	grad_norm 2.2983 (2.2560)	mem 28740MB
Train: [1/180][200/10009]	eta 3:20:24 lr 0.049996	data 0.0006 (0.1031)	batch 1.1238 (1.2259)	loss 2.5152 (2.7782)	grad_norm 2.1290 (2.2478)	mem 28740MB
Train: [1/180][250/10009]	eta 3:16:00 lr 0.049996	data 0.0005 (0.0827)	batch 1.1377 (1.2051)	loss 3.0066 (2.7821)	grad_norm 2.1803 (2.2396)	mem 28740MB
Train: [1/180][300/10009]	eta 3:12:47 lr 0.049996	data 0.0005 (0.0690)	batch 1.1194 (1.1914)	loss 2.6702 (2.7843)	grad_norm 2.2252 (2.2394)	mem 28740MB
Train: [1/180][350/10009]	eta 3:10:10 lr 0.049996	data 0.0010 (0.0593)	batch 1.1431 (1.1813)	loss 2.8822 (2.7822)	grad_norm 2.1653 (2.2344)	mem 28740MB
Train: [1/180][400/10009]	eta 3:08:00 lr 0.049996	data 0.0005 (0.0520)	batch 1.1163 (1.1740)	loss 2.3630 (2.7765)	grad_norm 2.1861 (2.2281)	mem 28740MB
Train: [1/180][450/10009]	eta 3:06:09 lr 0.049996	data 0.0006 (0.0463)	batch 1.1037 (1.1685)	loss 2.6809 (2.7772)	grad_norm 2.1821 (2.2257)	mem 28740MB
Train: [1/180][500/10009]	eta 3:04:26 lr 0.049996	data 0.0006 (0.0417)	batch 1.1204 (1.1638)	loss 2.8979 (2.7779)	grad_norm 2.1893 (2.2237)	mem 28740MB
Train: [1/180][550/10009]	eta 3:02:56 lr 0.049996	data 0.0005 (0.0380)	batch 1.1433 (1.1604)	loss 2.7968 (2.7764)	grad_norm 2.1923 (2.2196)	mem 28740MB
Train: [1/180][600/10009]	eta 3:01:31 lr 0.049996	data 0.0004 (0.0349)	batch 1.1437 (1.1575)	loss 2.8213 (2.7711)	grad_norm 2.0006 (2.2146)	mem 28740MB
Train: [1/180][650/10009]	eta 3:00:11 lr 0.049996	data 0.0005 (0.0323)	batch 1.1150 (1.1552)	loss 2.7165 (2.7698)	grad_norm 1.9886 (2.2108)	mem 28740MB
Train: [1/180][700/10009]	eta 2:58:48 lr 0.049996	data 0.0005 (0.0300)	batch 1.1283 (1.1525)	loss 2.5400 (2.7675)	grad_norm 2.0397 (2.2052)	mem 28740MB
Train: [1/180][750/10009]	eta 2:57:34 lr 0.049996	data 0.0005 (0.0280)	batch 1.2921 (1.1507)	loss 2.6537 (2.7685)	grad_norm 2.2671 (2.2023)	mem 28740MB
Train: [1/180][800/10009]	eta 2:56:19 lr 0.049996	data 0.0005 (0.0263)	batch 1.1397 (1.1488)	loss 2.8019 (2.7667)	grad_norm 2.2062 (2.1994)	mem 28740MB
Train: [1/180][850/10009]	eta 2:55:06 lr 0.049996	data 0.0005 (0.0248)	batch 1.1146 (1.1471)	loss 2.8853 (2.7658)	grad_norm 2.0584 (2.1944)	mem 28740MB
Train: [1/180][900/10009]	eta 2:53:56 lr 0.049995	data 0.0009 (0.0235)	batch 1.1273 (1.1458)	loss 2.9753 (2.7656)	grad_norm 2.1261 (2.1913)	mem 28740MB
Train: [1/180][950/10009]	eta 2:52:47 lr 0.049995	data 0.0005 (0.0223)	batch 1.1136 (1.1444)	loss 3.0053 (2.7640)	grad_norm 2.1992 (2.1859)	mem 28740MB
Train: [1/180][1000/10009]	eta 2:51:40 lr 0.049995	data 0.0005 (0.0212)	batch 1.1070 (1.1434)	loss 2.9265 (2.7625)	grad_norm 2.2093 (2.1834)	mem 28740MB
Train: [1/180][1050/10009]	eta 2:50:33 lr 0.049995	data 0.0005 (0.0202)	batch 1.1167 (1.1423)	loss 2.4259 (2.7608)	grad_norm 2.1414 (2.1815)	mem 28740MB
Train: [1/180][1100/10009]	eta 2:49:28 lr 0.049995	data 0.0005 (0.0193)	batch 1.1426 (1.1414)	loss 2.5736 (2.7587)	grad_norm 2.1582 (2.1785)	mem 28740MB
Train: [1/180][1150/10009]	eta 2:48:25 lr 0.049995	data 0.0005 (0.0185)	batch 1.1169 (1.1407)	loss 2.8618 (2.7574)	grad_norm 2.1432 (2.1758)	mem 28740MB
Train: [1/180][1200/10009]	eta 2:47:22 lr 0.049995	data 0.0004 (0.0177)	batch 1.1189 (1.1400)	loss 2.8821 (2.7546)	grad_norm 2.0887 (2.1729)	mem 28740MB
Train: [1/180][1250/10009]	eta 2:46:19 lr 0.049995	data 0.0005 (0.0170)	batch 1.1354 (1.1393)	loss 2.4826 (2.7541)	grad_norm 2.0140 (2.1719)	mem 28740MB
Train: [1/180][1300/10009]	eta 2:45:16 lr 0.049995	data 0.0004 (0.0164)	batch 1.1132 (1.1386)	loss 2.8176 (2.7533)	grad_norm 2.0918 (2.1704)	mem 28740MB
Train: [1/180][1350/10009]	eta 2:44:13 lr 0.049995	data 0.0005 (0.0158)	batch 1.1198 (1.1379)	loss 3.0255 (2.7535)	grad_norm 2.1763 (2.1681)	mem 28740MB
Train: [1/180][1400/10009]	eta 2:43:13 lr 0.049995	data 0.0005 (0.0153)	batch 1.1110 (1.1375)	loss 2.5548 (2.7529)	grad_norm 2.1348 (2.1650)	mem 28740MB
Train: [1/180][1450/10009]	eta 2:42:12 lr 0.049995	data 0.0005 (0.0148)	batch 1.1195 (1.1371)	loss 2.5918 (2.7504)	grad_norm 1.9992 (2.1614)	mem 28740MB
Train: [1/180][1500/10009]	eta 2:41:12 lr 0.049995	data 0.0005 (0.0143)	batch 1.1456 (1.1367)	loss 2.6764 (2.7501)	grad_norm 2.0471 (2.1591)	mem 28740MB
Train: [1/180][1550/10009]	eta 2:40:10 lr 0.049995	data 0.0005 (0.0139)	batch 1.1027 (1.1362)	loss 2.9484 (2.7491)	grad_norm 2.1728 (2.1556)	mem 28740MB
Train: [1/180][1600/10009]	eta 2:39:10 lr 0.049995	data 0.0005 (0.0134)	batch 1.1140 (1.1357)	loss 2.5014 (2.7471)	grad_norm 1.8396 (2.1526)	mem 28740MB
Train: [1/180][1650/10009]	eta 2:38:09 lr 0.049995	data 0.0005 (0.0130)	batch 1.1374 (1.1352)	loss 2.5836 (2.7461)	grad_norm 2.0367 (2.1507)	mem 28740MB
Train: [1/180][1700/10009]	eta 2:37:10 lr 0.049995	data 0.0005 (0.0127)	batch 1.1203 (1.1349)	loss 2.7540 (2.7441)	grad_norm 2.0275 (2.1479)	mem 28740MB
Train: [1/180][1750/10009]	eta 2:36:09 lr 0.049995	data 0.0004 (0.0123)	batch 1.1396 (1.1344)	loss 2.6735 (2.7428)	grad_norm 2.2189 (2.1456)	mem 28740MB
Train: [1/180][1800/10009]	eta 2:35:08 lr 0.049995	data 0.0005 (0.0120)	batch 1.1236 (1.1340)	loss 2.6671 (2.7419)	grad_norm 2.1527 (2.1423)	mem 28740MB
Train: [1/180][1850/10009]	eta 2:34:09 lr 0.049995	data 0.0005 (0.0117)	batch 1.1422 (1.1336)	loss 2.8054 (2.7408)	grad_norm 2.0368 (2.1397)	mem 28740MB
Train: [1/180][1900/10009]	eta 2:33:09 lr 0.049995	data 0.0004 (0.0114)	batch 1.1386 (1.1333)	loss 2.3803 (2.7389)	grad_norm 1.9374 (2.1366)	mem 28740MB
Train: [1/180][1950/10009]	eta 2:32:10 lr 0.049995	data 0.0005 (0.0111)	batch 1.1188 (1.1330)	loss 2.7043 (2.7391)	grad_norm 2.0903 (2.1342)	mem 28740MB
Train: [1/180][2000/10009]	eta 2:31:12 lr 0.049995	data 0.0005 (0.0109)	batch 1.1120 (1.1327)	loss 2.5480 (2.7372)	grad_norm 2.0254 (2.1324)	mem 28740MB
Train: [1/180][2050/10009]	eta 2:30:12 lr 0.049994	data 0.0006 (0.0106)	batch 1.1385 (1.1324)	loss 2.9157 (2.7355)	grad_norm 2.0486 (2.1297)	mem 28740MB
Train: [1/180][2100/10009]	eta 2:29:14 lr 0.049994	data 0.0004 (0.0104)	batch 1.1219 (1.1322)	loss 2.6719 (2.7339)	grad_norm 1.9597 (2.1269)	mem 28740MB
Train: [1/180][2150/10009]	eta 2:28:15 lr 0.049994	data 0.0004 (0.0101)	batch 1.1241 (1.1319)	loss 2.6159 (2.7325)	grad_norm 1.9193 (2.1250)	mem 28740MB
Train: [1/180][2200/10009]	eta 2:27:17 lr 0.049994	data 0.0005 (0.0099)	batch 1.1119 (1.1317)	loss 2.5327 (2.7311)	grad_norm 2.0858 (2.1228)	mem 28740MB
Train: [1/180][2250/10009]	eta 2:26:18 lr 0.049994	data 0.0063 (0.0097)	batch 1.1131 (1.1314)	loss 2.6037 (2.7298)	grad_norm 2.0373 (2.1206)	mem 28740MB
Train: [1/180][2300/10009]	eta 2:25:20 lr 0.049994	data 0.0005 (0.0095)	batch 1.1265 (1.1313)	loss 2.6502 (2.7277)	grad_norm 2.0061 (2.1187)	mem 28740MB
Train: [1/180][2350/10009]	eta 2:24:22 lr 0.049994	data 0.0005 (0.0093)	batch 1.1320 (1.1310)	loss 2.7523 (2.7274)	grad_norm 2.0842 (2.1172)	mem 28740MB
Train: [1/180][2400/10009]	eta 2:23:24 lr 0.049994	data 0.0004 (0.0091)	batch 1.1290 (1.1309)	loss 2.6680 (2.7261)	grad_norm 2.0400 (2.1153)	mem 28740MB
Train: [1/180][2450/10009]	eta 2:22:26 lr 0.049994	data 0.0005 (0.0090)	batch 1.1151 (1.1307)	loss 2.9610 (2.7251)	grad_norm 2.2057 (2.1136)	mem 28740MB
Train: [1/180][2500/10009]	eta 2:21:28 lr 0.049994	data 0.0005 (0.0088)	batch 1.1186 (1.1305)	loss 2.8396 (2.7241)	grad_norm 1.9174 (2.1113)	mem 28740MB
Train: [1/180][2550/10009]	eta 2:20:31 lr 0.049994	data 0.0004 (0.0086)	batch 1.1340 (1.1304)	loss 2.4835 (2.7224)	grad_norm 1.9322 (2.1092)	mem 28740MB
Train: [1/180][2600/10009]	eta 2:19:34 lr 0.049994	data 0.0005 (0.0085)	batch 1.0877 (1.1303)	loss 2.5511 (2.7208)	grad_norm 1.7837 (2.1076)	mem 28740MB
Train: [1/180][2650/10009]	eta 2:18:36 lr 0.049994	data 0.0005 (0.0083)	batch 1.1378 (1.1301)	loss 2.5098 (2.7195)	grad_norm 1.8581 (2.1054)	mem 28740MB
Train: [1/180][2700/10009]	eta 2:17:38 lr 0.049994	data 0.0005 (0.0082)	batch 1.1205 (1.1300)	loss 2.2832 (2.7185)	grad_norm 1.8266 (2.1038)	mem 28740MB
Train: [1/180][2750/10009]	eta 2:16:41 lr 0.049994	data 0.0005 (0.0080)	batch 1.1291 (1.1298)	loss 2.7397 (2.7169)	grad_norm 1.9866 (2.1013)	mem 28740MB
Train: [1/180][2800/10009]	eta 2:15:44 lr 0.049994	data 0.0005 (0.0079)	batch 1.1113 (1.1297)	loss 2.8633 (2.7156)	grad_norm 1.9052 (2.0991)	mem 28740MB
Train: [1/180][2850/10009]	eta 2:14:47 lr 0.049994	data 0.0005 (0.0078)	batch 1.1409 (1.1296)	loss 2.4313 (2.7138)	grad_norm 1.9852 (2.0972)	mem 28740MB
Train: [1/180][2900/10009]	eta 2:13:50 lr 0.049994	data 0.0004 (0.0077)	batch 1.1293 (1.1296)	loss 2.9011 (2.7128)	grad_norm 1.8935 (2.0951)	mem 28740MB
Train: [1/180][2950/10009]	eta 2:12:52 lr 0.049994	data 0.0004 (0.0075)	batch 1.1169 (1.1294)	loss 2.6944 (2.7123)	grad_norm 1.8967 (2.0931)	mem 28740MB
Train: [1/180][3000/10009]	eta 2:11:54 lr 0.049994	data 0.0009 (0.0074)	batch 1.1076 (1.1293)	loss 2.6633 (2.7105)	grad_norm 2.0042 (2.0908)	mem 28740MB
Train: [1/180][3050/10009]	eta 2:10:57 lr 0.049994	data 0.0004 (0.0073)	batch 1.1129 (1.1291)	loss 2.4782 (2.7095)	grad_norm 1.9929 (2.0888)	mem 28740MB
Train: [1/180][3100/10009]	eta 2:10:00 lr 0.049993	data 0.0005 (0.0072)	batch 1.2880 (1.1291)	loss 2.7382 (2.7093)	grad_norm 1.9971 (2.0867)	mem 28740MB
Train: [1/180][3150/10009]	eta 2:09:03 lr 0.049993	data 0.0005 (0.0071)	batch 1.1140 (1.1289)	loss 2.6790 (2.7081)	grad_norm 2.0529 (2.0845)	mem 28740MB
Train: [1/180][3200/10009]	eta 2:08:06 lr 0.049993	data 0.0004 (0.0070)	batch 1.0997 (1.1288)	loss 2.4339 (2.7065)	grad_norm 2.0189 (2.0825)	mem 28740MB
Train: [1/180][3250/10009]	eta 2:07:09 lr 0.049993	data 0.0005 (0.0069)	batch 1.1285 (1.1287)	loss 2.2819 (2.7057)	grad_norm 1.9257 (2.0811)	mem 28740MB
Train: [1/180][3300/10009]	eta 2:06:11 lr 0.049993	data 0.0004 (0.0068)	batch 1.1195 (1.1286)	loss 2.5797 (2.7055)	grad_norm 1.9194 (2.0792)	mem 28740MB
Train: [1/180][3350/10009]	eta 2:05:14 lr 0.049993	data 0.0004 (0.0067)	batch 1.1184 (1.1285)	loss 2.8655 (2.7039)	grad_norm 2.0472 (2.0769)	mem 28740MB
Train: [1/180][3400/10009]	eta 2:04:17 lr 0.049993	data 0.0006 (0.0066)	batch 1.1242 (1.1284)	loss 2.4234 (2.7025)	grad_norm 2.0593 (2.0751)	mem 28740MB
Train: [1/180][3450/10009]	eta 2:03:20 lr 0.049993	data 0.0005 (0.0065)	batch 1.1028 (1.1283)	loss 2.5741 (2.7014)	grad_norm 1.9224 (2.0728)	mem 28740MB
Train: [1/180][3500/10009]	eta 2:02:23 lr 0.049993	data 0.0006 (0.0064)	batch 1.1122 (1.1282)	loss 2.5013 (2.7008)	grad_norm 1.8314 (2.0713)	mem 28740MB
Train: [1/180][3550/10009]	eta 2:01:26 lr 0.049993	data 0.0005 (0.0063)	batch 1.1391 (1.1281)	loss 2.5348 (2.6997)	grad_norm 1.9434 (2.0696)	mem 28740MB
Train: [1/180][3600/10009]	eta 2:00:29 lr 0.049993	data 0.0005 (0.0063)	batch 1.1224 (1.1281)	loss 2.6669 (2.6985)	grad_norm 1.9070 (2.0676)	mem 28740MB
Train: [1/180][3650/10009]	eta 1:59:32 lr 0.049993	data 0.0005 (0.0062)	batch 1.1073 (1.1279)	loss 2.7114 (2.6975)	grad_norm 1.7818 (2.0658)	mem 28740MB
Train: [1/180][3700/10009]	eta 1:58:35 lr 0.049993	data 0.0005 (0.0061)	batch 1.1147 (1.1278)	loss 2.6616 (2.6965)	grad_norm 1.9876 (2.0645)	mem 28740MB
Train: [1/180][3750/10009]	eta 1:57:38 lr 0.049993	data 0.0005 (0.0060)	batch 1.1185 (1.1277)	loss 2.5594 (2.6957)	grad_norm 2.0888 (2.0628)	mem 28740MB
Train: [1/180][3800/10009]	eta 1:56:41 lr 0.049993	data 0.0010 (0.0060)	batch 1.1418 (1.1277)	loss 2.3815 (2.6948)	grad_norm 1.9809 (2.0612)	mem 28740MB
Train: [1/180][3850/10009]	eta 1:55:44 lr 0.049993	data 0.0004 (0.0059)	batch 1.1390 (1.1276)	loss 2.6187 (2.6936)	grad_norm 1.8661 (2.0598)	mem 28740MB
Train: [1/180][3900/10009]	eta 1:54:47 lr 0.049993	data 0.0005 (0.0058)	batch 1.1047 (1.1275)	loss 2.3226 (2.6924)	grad_norm 1.8422 (2.0582)	mem 28740MB
Train: [1/180][3950/10009]	eta 1:53:51 lr 0.049993	data 0.0004 (0.0058)	batch 1.1058 (1.1274)	loss 2.6741 (2.6916)	grad_norm 1.8303 (2.0564)	mem 28740MB
Train: [1/180][4000/10009]	eta 1:52:53 lr 0.049993	data 0.0005 (0.0057)	batch 1.1236 (1.1273)	loss 2.6311 (2.6908)	grad_norm 1.8942 (2.0548)	mem 28740MB
Train: [1/180][4050/10009]	eta 1:51:57 lr 0.049992	data 0.0008 (0.0056)	batch 1.1077 (1.1273)	loss 2.3843 (2.6900)	grad_norm 1.9485 (2.0535)	mem 28740MB
Train: [1/180][4100/10009]	eta 1:51:00 lr 0.049992	data 0.0005 (0.0056)	batch 1.1341 (1.1272)	loss 2.6696 (2.6889)	grad_norm 2.0388 (2.0521)	mem 28740MB
Train: [1/180][4150/10009]	eta 1:50:03 lr 0.049992	data 0.0003 (0.0055)	batch 1.0937 (1.1271)	loss 2.5315 (2.6887)	grad_norm 1.9554 (2.0505)	mem 28740MB
Train: [1/180][4200/10009]	eta 1:49:07 lr 0.049992	data 0.0005 (0.0054)	batch 1.1374 (1.1271)	loss 2.6045 (2.6877)	grad_norm 1.9762 (2.0490)	mem 28740MB
Train: [1/180][4250/10009]	eta 1:48:10 lr 0.049992	data 0.0005 (0.0054)	batch 1.1434 (1.1270)	loss 2.3609 (2.6869)	grad_norm 1.8532 (2.0471)	mem 28740MB
Train: [1/180][4300/10009]	eta 1:47:13 lr 0.049992	data 0.0005 (0.0053)	batch 1.1179 (1.1269)	loss 2.6237 (2.6858)	grad_norm 1.7975 (2.0455)	mem 28740MB
Train: [1/180][4350/10009]	eta 1:46:16 lr 0.049992	data 0.0005 (0.0053)	batch 1.1178 (1.1268)	loss 2.6861 (2.6853)	grad_norm 1.8151 (2.0440)	mem 28740MB
Train: [1/180][4400/10009]	eta 1:45:19 lr 0.049992	data 0.0005 (0.0052)	batch 1.1225 (1.1267)	loss 2.7125 (2.6844)	grad_norm 1.9029 (2.0424)	mem 28740MB
Train: [1/180][4450/10009]	eta 1:44:23 lr 0.049992	data 0.0005 (0.0052)	batch 1.1109 (1.1267)	loss 2.9146 (2.6839)	grad_norm 1.8658 (2.0409)	mem 28740MB
Train: [1/180][4500/10009]	eta 1:43:26 lr 0.049992	data 0.0005 (0.0051)	batch 1.1169 (1.1266)	loss 2.6870 (2.6829)	grad_norm 1.8410 (2.0394)	mem 28740MB
Train: [1/180][4550/10009]	eta 1:42:29 lr 0.049992	data 0.0005 (0.0051)	batch 1.1117 (1.1265)	loss 2.6336 (2.6824)	grad_norm 1.9204 (2.0377)	mem 28740MB
Train: [1/180][4600/10009]	eta 1:41:33 lr 0.049992	data 0.0005 (0.0050)	batch 1.1378 (1.1265)	loss 2.3000 (2.6812)	grad_norm 1.6168 (2.0365)	mem 28740MB
Train: [1/180][4650/10009]	eta 1:40:36 lr 0.049992	data 0.0005 (0.0050)	batch 1.1297 (1.1265)	loss 2.8080 (2.6807)	grad_norm 2.0213 (2.0352)	mem 28740MB
Train: [1/180][4700/10009]	eta 1:39:39 lr 0.049992	data 0.0004 (0.0049)	batch 1.1278 (1.1264)	loss 2.6993 (2.6803)	grad_norm 1.7999 (2.0341)	mem 28740MB
Train: [1/180][4750/10009]	eta 1:38:43 lr 0.049992	data 0.0005 (0.0049)	batch 1.1156 (1.1263)	loss 2.8211 (2.6797)	grad_norm 2.0299 (2.0329)	mem 28740MB
Train: [1/180][4800/10009]	eta 1:37:46 lr 0.049992	data 0.0005 (0.0048)	batch 1.1407 (1.1262)	loss 2.6512 (2.6788)	grad_norm 2.0239 (2.0315)	mem 28740MB
Train: [1/180][4850/10009]	eta 1:36:49 lr 0.049992	data 0.0005 (0.0048)	batch 1.1311 (1.1262)	loss 2.6190 (2.6779)	grad_norm 1.9383 (2.0305)	mem 28740MB
Train: [1/180][4900/10009]	eta 1:35:53 lr 0.049992	data 0.0004 (0.0047)	batch 1.1484 (1.1261)	loss 2.8244 (2.6773)	grad_norm 1.8867 (2.0291)	mem 28740MB
Train: [1/180][4950/10009]	eta 1:34:56 lr 0.049991	data 0.0004 (0.0047)	batch 1.1297 (1.1261)	loss 2.7424 (2.6767)	grad_norm 1.8443 (2.0276)	mem 28740MB
Train: [1/180][5000/10009]	eta 1:34:00 lr 0.049991	data 0.0005 (0.0047)	batch 1.1274 (1.1260)	loss 2.5980 (2.6755)	grad_norm 2.0287 (2.0262)	mem 28740MB
Train: [1/180][5050/10009]	eta 1:33:03 lr 0.049991	data 0.0005 (0.0046)	batch 1.1066 (1.1260)	loss 2.6984 (2.6741)	grad_norm 1.7777 (2.0247)	mem 28740MB
Train: [1/180][5100/10009]	eta 1:32:07 lr 0.049991	data 0.0005 (0.0046)	batch 1.1151 (1.1259)	loss 2.4692 (2.6736)	grad_norm 1.7738 (2.0232)	mem 28740MB
Train: [1/180][5150/10009]	eta 1:31:10 lr 0.049991	data 0.0005 (0.0045)	batch 1.1084 (1.1259)	loss 2.6553 (2.6725)	grad_norm 1.9866 (2.0218)	mem 28740MB
Train: [1/180][5200/10009]	eta 1:30:14 lr 0.049991	data 0.0005 (0.0045)	batch 1.1207 (1.1258)	loss 2.5515 (2.6718)	grad_norm 1.8960 (2.0206)	mem 28740MB
Train: [1/180][5250/10009]	eta 1:29:17 lr 0.049991	data 0.0005 (0.0045)	batch 1.1404 (1.1258)	loss 2.3712 (2.6710)	grad_norm 1.8618 (2.0192)	mem 28740MB
Train: [1/180][5300/10009]	eta 1:28:21 lr 0.049991	data 0.0005 (0.0044)	batch 1.1423 (1.1258)	loss 2.5725 (2.6706)	grad_norm 1.8328 (2.0180)	mem 28740MB
Train: [1/180][5350/10009]	eta 1:27:24 lr 0.049991	data 0.0005 (0.0044)	batch 1.1433 (1.1258)	loss 2.7647 (2.6698)	grad_norm 1.9285 (2.0166)	mem 28740MB
Train: [1/180][5400/10009]	eta 1:26:28 lr 0.049991	data 0.0004 (0.0044)	batch 1.1115 (1.1257)	loss 2.8288 (2.6692)	grad_norm 1.8606 (2.0156)	mem 28740MB
Train: [1/180][5450/10009]	eta 1:25:31 lr 0.049991	data 0.0005 (0.0043)	batch 1.2597 (1.1257)	loss 2.6354 (2.6684)	grad_norm 1.8649 (2.0144)	mem 28740MB
Train: [1/180][5500/10009]	eta 1:24:35 lr 0.049991	data 0.0004 (0.0043)	batch 1.1153 (1.1256)	loss 2.7656 (2.6672)	grad_norm 1.9465 (2.0133)	mem 28740MB
Train: [1/180][5550/10009]	eta 1:23:38 lr 0.049991	data 0.0005 (0.0042)	batch 1.1181 (1.1255)	loss 2.7094 (2.6664)	grad_norm 1.9329 (2.0119)	mem 28740MB
Train: [1/180][5600/10009]	eta 1:22:42 lr 0.049991	data 0.0005 (0.0042)	batch 1.0992 (1.1255)	loss 2.5561 (2.6655)	grad_norm 1.9743 (2.0105)	mem 28740MB
Train: [1/180][5650/10009]	eta 1:21:45 lr 0.049991	data 0.0005 (0.0042)	batch 1.1201 (1.1254)	loss 2.6650 (2.6648)	grad_norm 1.8564 (2.0093)	mem 28740MB
Train: [1/180][5700/10009]	eta 1:20:49 lr 0.049991	data 0.0006 (0.0042)	batch 1.1189 (1.1254)	loss 2.6808 (2.6637)	grad_norm 1.9316 (2.0080)	mem 28740MB
Train: [1/180][5750/10009]	eta 1:19:52 lr 0.049991	data 0.0005 (0.0041)	batch 1.1204 (1.1254)	loss 2.5870 (2.6631)	grad_norm 1.8802 (2.0069)	mem 28740MB
Train: [1/180][5800/10009]	eta 1:18:56 lr 0.049991	data 0.0005 (0.0041)	batch 1.1194 (1.1254)	loss 2.8382 (2.6622)	grad_norm 1.9921 (2.0058)	mem 28740MB
Train: [1/180][5850/10009]	eta 1:18:00 lr 0.049990	data 0.0005 (0.0041)	batch 1.1438 (1.1254)	loss 2.4787 (2.6616)	grad_norm 1.9321 (2.0045)	mem 28740MB
Train: [1/180][5900/10009]	eta 1:17:03 lr 0.049990	data 0.0005 (0.0040)	batch 1.1166 (1.1253)	loss 2.4801 (2.6606)	grad_norm 1.7711 (2.0032)	mem 28740MB
Train: [1/180][5950/10009]	eta 1:16:07 lr 0.049990	data 0.0005 (0.0040)	batch 1.1220 (1.1253)	loss 2.3253 (2.6599)	grad_norm 1.8138 (2.0019)	mem 28740MB
Train: [1/180][6000/10009]	eta 1:15:11 lr 0.049990	data 0.0005 (0.0040)	batch 1.1213 (1.1253)	loss 2.3847 (2.6591)	grad_norm 1.8623 (2.0007)	mem 28740MB
Train: [1/180][6050/10009]	eta 1:14:14 lr 0.049990	data 0.0005 (0.0039)	batch 1.1132 (1.1253)	loss 2.5058 (2.6583)	grad_norm 1.8808 (1.9995)	mem 28740MB
Train: [1/180][6100/10009]	eta 1:13:18 lr 0.049990	data 0.0007 (0.0039)	batch 1.1148 (1.1253)	loss 2.5949 (2.6574)	grad_norm 2.0474 (1.9981)	mem 28740MB
Train: [1/180][6150/10009]	eta 1:12:22 lr 0.049990	data 0.0005 (0.0039)	batch 1.1131 (1.1252)	loss 2.3922 (2.6563)	grad_norm 1.9359 (1.9968)	mem 28740MB
Train: [1/180][6200/10009]	eta 1:11:25 lr 0.049990	data 0.0005 (0.0039)	batch 1.1150 (1.1252)	loss 2.5237 (2.6554)	grad_norm 1.9634 (1.9957)	mem 28740MB
Train: [1/180][6250/10009]	eta 1:10:29 lr 0.049990	data 0.0005 (0.0038)	batch 1.1164 (1.1251)	loss 2.2573 (2.6544)	grad_norm 1.7010 (1.9949)	mem 28740MB
Train: [1/180][6300/10009]	eta 1:09:33 lr 0.049990	data 0.0005 (0.0038)	batch 1.1397 (1.1251)	loss 2.3495 (2.6536)	grad_norm 1.5610 (1.9938)	mem 28740MB
Train: [1/180][6350/10009]	eta 1:08:36 lr 0.049990	data 0.0005 (0.0038)	batch 1.1214 (1.1251)	loss 2.3681 (2.6528)	grad_norm 1.9657 (1.9927)	mem 28740MB
Train: [1/180][6400/10009]	eta 1:07:40 lr 0.049990	data 0.0004 (0.0038)	batch 1.1435 (1.1251)	loss 2.4524 (2.6521)	grad_norm 1.9982 (1.9915)	mem 28740MB
Train: [1/180][6450/10009]	eta 1:06:44 lr 0.049990	data 0.0007 (0.0037)	batch 1.1056 (1.1251)	loss 2.6278 (2.6513)	grad_norm 1.8085 (1.9904)	mem 28740MB
Train: [1/180][6500/10009]	eta 1:05:47 lr 0.049990	data 0.0005 (0.0037)	batch 1.1202 (1.1250)	loss 2.4323 (2.6507)	grad_norm 1.9700 (1.9891)	mem 28740MB
Train: [1/180][6550/10009]	eta 1:04:51 lr 0.049990	data 0.0005 (0.0037)	batch 1.1242 (1.1250)	loss 2.5099 (2.6502)	grad_norm 1.8806 (1.9880)	mem 28740MB
Train: [1/180][6600/10009]	eta 1:03:55 lr 0.049990	data 0.0004 (0.0037)	batch 1.1142 (1.1250)	loss 2.4352 (2.6493)	grad_norm 1.8227 (1.9869)	mem 28740MB
Train: [1/180][6650/10009]	eta 1:02:58 lr 0.049989	data 0.0005 (0.0036)	batch 1.1397 (1.1250)	loss 2.6556 (2.6486)	grad_norm 1.8736 (1.9860)	mem 28740MB
Train: [1/180][6700/10009]	eta 1:02:02 lr 0.049989	data 0.0004 (0.0036)	batch 1.1192 (1.1250)	loss 2.5160 (2.6481)	grad_norm 1.6586 (1.9850)	mem 28740MB
Train: [1/180][6750/10009]	eta 1:01:06 lr 0.049989	data 0.0005 (0.0036)	batch 1.1121 (1.1249)	loss 2.5374 (2.6475)	grad_norm 1.7816 (1.9839)	mem 28740MB
Train: [1/180][6800/10009]	eta 1:00:09 lr 0.049989	data 0.0005 (0.0036)	batch 1.1371 (1.1249)	loss 2.3848 (2.6468)	grad_norm 1.7997 (1.9828)	mem 28740MB
Train: [1/180][6850/10009]	eta 0:59:13 lr 0.049989	data 0.0005 (0.0035)	batch 1.1359 (1.1249)	loss 2.4268 (2.6461)	grad_norm 1.8832 (1.9817)	mem 28740MB
Train: [1/180][6900/10009]	eta 0:58:17 lr 0.049989	data 0.0006 (0.0035)	batch 1.1075 (1.1249)	loss 2.4422 (2.6455)	grad_norm 1.9455 (1.9807)	mem 28740MB
Train: [1/180][6950/10009]	eta 0:57:20 lr 0.049989	data 0.0005 (0.0035)	batch 1.1398 (1.1249)	loss 2.7247 (2.6452)	grad_norm 1.7347 (1.9797)	mem 28740MB
Train: [1/180][7000/10009]	eta 0:56:24 lr 0.049989	data 0.0005 (0.0035)	batch 1.1188 (1.1249)	loss 2.3612 (2.6444)	grad_norm 1.8114 (1.9786)	mem 28740MB
Train: [1/180][7050/10009]	eta 0:55:28 lr 0.049989	data 0.0005 (0.0035)	batch 1.1172 (1.1249)	loss 2.8513 (2.6438)	grad_norm 1.8880 (1.9776)	mem 28740MB
Train: [1/180][7100/10009]	eta 0:54:32 lr 0.049989	data 0.0005 (0.0034)	batch 1.1010 (1.1249)	loss 2.7552 (2.6434)	grad_norm 1.8238 (1.9765)	mem 28740MB
Train: [1/180][7150/10009]	eta 0:53:36 lr 0.049989	data 0.0005 (0.0034)	batch 1.1092 (1.1249)	loss 2.5531 (2.6428)	grad_norm 1.8830 (1.9755)	mem 28740MB
Train: [1/180][7200/10009]	eta 0:52:39 lr 0.049989	data 0.0005 (0.0034)	batch 1.1339 (1.1249)	loss 2.4145 (2.6421)	grad_norm 1.7275 (1.9745)	mem 28740MB
Train: [1/180][7250/10009]	eta 0:51:43 lr 0.049989	data 0.0004 (0.0034)	batch 1.1228 (1.1249)	loss 2.8286 (2.6414)	grad_norm 1.9917 (1.9735)	mem 28740MB
Train: [1/180][7300/10009]	eta 0:50:47 lr 0.049989	data 0.0004 (0.0034)	batch 1.1191 (1.1249)	loss 2.5269 (2.6407)	grad_norm 1.9033 (1.9725)	mem 28740MB
Train: [1/180][7350/10009]	eta 0:49:51 lr 0.049989	data 0.0003 (0.0033)	batch 1.1154 (1.1249)	loss 2.2528 (2.6397)	grad_norm 1.6800 (1.9715)	mem 28740MB
Train: [1/180][7400/10009]	eta 0:48:54 lr 0.049988	data 0.0004 (0.0033)	batch 1.1431 (1.1249)	loss 2.3036 (2.6392)	grad_norm 1.6518 (1.9705)	mem 28740MB
Train: [1/180][7450/10009]	eta 0:47:58 lr 0.049988	data 0.0005 (0.0033)	batch 1.1325 (1.1249)	loss 2.9029 (2.6387)	grad_norm 1.9579 (1.9693)	mem 28740MB
Train: [1/180][7500/10009]	eta 0:47:02 lr 0.049988	data 0.0004 (0.0033)	batch 1.1244 (1.1249)	loss 2.5149 (2.6384)	grad_norm 1.6403 (1.9682)	mem 28740MB
Train: [1/180][7550/10009]	eta 0:46:06 lr 0.049988	data 0.0005 (0.0033)	batch 1.1052 (1.1249)	loss 2.6844 (2.6378)	grad_norm 1.8062 (1.9671)	mem 28740MB
Train: [1/180][7600/10009]	eta 0:45:09 lr 0.049988	data 0.0005 (0.0032)	batch 1.1403 (1.1249)	loss 2.7370 (2.6370)	grad_norm 1.9456 (1.9660)	mem 28740MB
Train: [1/180][7650/10009]	eta 0:44:13 lr 0.049988	data 0.0005 (0.0032)	batch 1.1141 (1.1249)	loss 2.6814 (2.6363)	grad_norm 1.9558 (1.9650)	mem 28740MB
Train: [1/180][7700/10009]	eta 0:43:17 lr 0.049988	data 0.0004 (0.0032)	batch 1.1177 (1.1249)	loss 2.3290 (2.6357)	grad_norm 1.7695 (1.9641)	mem 28740MB
Train: [1/180][7750/10009]	eta 0:42:20 lr 0.049988	data 0.0005 (0.0032)	batch 1.1001 (1.1248)	loss 2.5409 (2.6350)	grad_norm 1.9432 (1.9631)	mem 28740MB
Train: [1/180][7800/10009]	eta 0:41:24 lr 0.049988	data 0.0005 (0.0032)	batch 1.2935 (1.1248)	loss 2.5810 (2.6346)	grad_norm 1.8782 (1.9620)	mem 28740MB
Train: [1/180][7850/10009]	eta 0:40:28 lr 0.049988	data 0.0005 (0.0032)	batch 1.1356 (1.1248)	loss 2.5612 (2.6338)	grad_norm 1.8525 (1.9612)	mem 28740MB
Train: [1/180][7900/10009]	eta 0:39:32 lr 0.049988	data 0.0005 (0.0031)	batch 1.1220 (1.1248)	loss 2.4440 (2.6331)	grad_norm 1.8637 (1.9603)	mem 28740MB
Train: [1/180][7950/10009]	eta 0:38:35 lr 0.049988	data 0.0005 (0.0031)	batch 1.1059 (1.1248)	loss 2.6361 (2.6324)	grad_norm 1.8770 (1.9594)	mem 28740MB
Train: [1/180][8000/10009]	eta 0:37:39 lr 0.049988	data 0.0004 (0.0031)	batch 1.1010 (1.1248)	loss 2.4821 (2.6320)	grad_norm 1.7916 (1.9585)	mem 28740MB
Train: [1/180][8050/10009]	eta 0:36:43 lr 0.049988	data 0.0005 (0.0031)	batch 1.1226 (1.1247)	loss 2.2876 (2.6313)	grad_norm 1.6692 (1.9575)	mem 28740MB
Train: [1/180][8100/10009]	eta 0:35:47 lr 0.049988	data 0.0005 (0.0031)	batch 1.1099 (1.1247)	loss 2.4695 (2.6308)	grad_norm 1.8292 (1.9567)	mem 28740MB
Train: [1/180][8150/10009]	eta 0:34:50 lr 0.049987	data 0.0004 (0.0031)	batch 1.1081 (1.1247)	loss 2.6106 (2.6304)	grad_norm 1.9227 (1.9559)	mem 28740MB
Train: [1/180][8200/10009]	eta 0:33:54 lr 0.049987	data 0.0005 (0.0030)	batch 1.1306 (1.1247)	loss 2.4110 (2.6299)	grad_norm 1.7746 (1.9551)	mem 28740MB
Train: [1/180][8250/10009]	eta 0:32:58 lr 0.049987	data 0.0005 (0.0030)	batch 1.1213 (1.1247)	loss 2.4247 (2.6293)	grad_norm 2.0772 (1.9540)	mem 28740MB
Train: [1/180][8300/10009]	eta 0:32:02 lr 0.049987	data 0.0005 (0.0030)	batch 1.1120 (1.1247)	loss 2.8080 (2.6286)	grad_norm 1.8727 (1.9530)	mem 28740MB
Train: [1/180][8350/10009]	eta 0:31:05 lr 0.049987	data 0.0005 (0.0030)	batch 1.1200 (1.1247)	loss 2.4547 (2.6281)	grad_norm 1.9202 (1.9522)	mem 28740MB
Train: [1/180][8400/10009]	eta 0:30:09 lr 0.049987	data 0.0005 (0.0030)	batch 1.1256 (1.1247)	loss 2.5055 (2.6277)	grad_norm 1.7218 (1.9512)	mem 28740MB
Train: [1/180][8450/10009]	eta 0:29:13 lr 0.049987	data 0.0005 (0.0030)	batch 1.1165 (1.1247)	loss 2.3449 (2.6272)	grad_norm 1.7354 (1.9503)	mem 28740MB
Train: [1/180][8500/10009]	eta 0:28:17 lr 0.049987	data 0.0005 (0.0030)	batch 1.1131 (1.1247)	loss 2.6278 (2.6265)	grad_norm 1.8779 (1.9495)	mem 28740MB
Train: [1/180][8550/10009]	eta 0:27:20 lr 0.049987	data 0.0004 (0.0029)	batch 1.1171 (1.1247)	loss 2.4880 (2.6260)	grad_norm 1.8165 (1.9487)	mem 28740MB
Train: [1/180][8600/10009]	eta 0:26:24 lr 0.049987	data 0.0004 (0.0029)	batch 1.1226 (1.1247)	loss 2.6597 (2.6256)	grad_norm 1.5368 (1.9477)	mem 28740MB
Train: [1/180][8650/10009]	eta 0:25:28 lr 0.049987	data 0.0004 (0.0029)	batch 1.1193 (1.1247)	loss 2.2991 (2.6250)	grad_norm 1.6764 (1.9466)	mem 28740MB
Train: [1/180][8700/10009]	eta 0:24:32 lr 0.049987	data 0.0005 (0.0029)	batch 1.0989 (1.1247)	loss 2.5685 (2.6244)	grad_norm 1.7190 (1.9458)	mem 28740MB
Train: [1/180][8750/10009]	eta 0:23:35 lr 0.049987	data 0.0004 (0.0029)	batch 1.1114 (1.1246)	loss 2.6956 (2.6240)	grad_norm 1.9125 (1.9451)	mem 28740MB
Train: [1/180][8800/10009]	eta 0:22:39 lr 0.049987	data 0.0005 (0.0029)	batch 1.1122 (1.1246)	loss 2.3948 (2.6231)	grad_norm 1.8114 (1.9442)	mem 28740MB
Train: [1/180][8850/10009]	eta 0:21:43 lr 0.049986	data 0.0005 (0.0029)	batch 1.1071 (1.1246)	loss 2.3937 (2.6226)	grad_norm 1.9017 (1.9433)	mem 28740MB
Train: [1/180][8900/10009]	eta 0:20:47 lr 0.049986	data 0.0005 (0.0028)	batch 1.1122 (1.1246)	loss 2.7551 (2.6222)	grad_norm 1.7665 (1.9423)	mem 28740MB
Train: [1/180][8950/10009]	eta 0:19:50 lr 0.049986	data 0.0005 (0.0028)	batch 1.1403 (1.1246)	loss 2.4259 (2.6216)	grad_norm 1.9237 (1.9413)	mem 28740MB
Train: [1/180][9000/10009]	eta 0:18:54 lr 0.049986	data 0.0005 (0.0028)	batch 1.1323 (1.1246)	loss 2.4044 (2.6209)	grad_norm 1.9590 (1.9405)	mem 28740MB
Train: [1/180][9050/10009]	eta 0:17:58 lr 0.049986	data 0.0005 (0.0028)	batch 1.1150 (1.1246)	loss 2.5107 (2.6203)	grad_norm 1.8473 (1.9397)	mem 28740MB
Train: [1/180][9100/10009]	eta 0:17:02 lr 0.049986	data 0.0005 (0.0028)	batch 1.1123 (1.1246)	loss 2.3491 (2.6198)	grad_norm 1.7631 (1.9387)	mem 28740MB
Train: [1/180][9150/10009]	eta 0:16:06 lr 0.049986	data 0.0004 (0.0028)	batch 1.1211 (1.1246)	loss 2.5635 (2.6194)	grad_norm 1.8487 (1.9379)	mem 28740MB
Train: [1/180][9200/10009]	eta 0:15:09 lr 0.049986	data 0.0005 (0.0028)	batch 1.1222 (1.1246)	loss 2.0149 (2.6187)	grad_norm 1.5329 (1.9370)	mem 28740MB
Train: [1/180][9250/10009]	eta 0:14:13 lr 0.049986	data 0.0004 (0.0028)	batch 1.1464 (1.1246)	loss 2.4698 (2.6179)	grad_norm 1.6386 (1.9360)	mem 28740MB
Train: [1/180][9300/10009]	eta 0:13:17 lr 0.049986	data 0.0005 (0.0027)	batch 1.1207 (1.1246)	loss 2.5620 (2.6172)	grad_norm 1.7416 (1.9351)	mem 28740MB
Train: [1/180][9350/10009]	eta 0:12:21 lr 0.049986	data 0.0005 (0.0027)	batch 1.1161 (1.1246)	loss 2.2991 (2.6167)	grad_norm 1.8202 (1.9343)	mem 28740MB
Train: [1/180][9400/10009]	eta 0:11:24 lr 0.049986	data 0.0004 (0.0027)	batch 1.1180 (1.1246)	loss 2.4039 (2.6161)	grad_norm 1.8945 (1.9333)	mem 28740MB
Train: [1/180][9450/10009]	eta 0:10:28 lr 0.049986	data 0.0005 (0.0027)	batch 1.1439 (1.1246)	loss 2.2317 (2.6154)	grad_norm 1.5952 (1.9325)	mem 28740MB
Train: [1/180][9500/10009]	eta 0:09:32 lr 0.049986	data 0.0005 (0.0027)	batch 1.1177 (1.1246)	loss 2.3391 (2.6148)	grad_norm 1.7127 (1.9316)	mem 28740MB
Train: [1/180][9550/10009]	eta 0:08:36 lr 0.049985	data 0.0004 (0.0027)	batch 1.1377 (1.1246)	loss 2.4150 (2.6141)	grad_norm 1.7572 (1.9307)	mem 28740MB
Train: [1/180][9600/10009]	eta 0:07:39 lr 0.049985	data 0.0005 (0.0027)	batch 1.1180 (1.1246)	loss 2.6262 (2.6136)	grad_norm 1.7489 (1.9298)	mem 28740MB
Train: [1/180][9650/10009]	eta 0:06:43 lr 0.049985	data 0.0005 (0.0027)	batch 1.1179 (1.1245)	loss 2.7211 (2.6133)	grad_norm 1.6947 (1.9290)	mem 28740MB
Train: [1/180][9700/10009]	eta 0:05:47 lr 0.049985	data 0.0005 (0.0027)	batch 1.1027 (1.1245)	loss 2.3669 (2.6128)	grad_norm 1.7341 (1.9282)	mem 28740MB
Train: [1/180][9750/10009]	eta 0:04:51 lr 0.049985	data 0.0006 (0.0026)	batch 1.1146 (1.1245)	loss 2.2725 (2.6123)	grad_norm 1.5481 (1.9274)	mem 28740MB
Train: [1/180][9800/10009]	eta 0:03:55 lr 0.049985	data 0.0004 (0.0026)	batch 1.1122 (1.1245)	loss 2.5686 (2.6116)	grad_norm 1.7038 (1.9267)	mem 28740MB
Train: [1/180][9850/10009]	eta 0:02:58 lr 0.049985	data 0.0007 (0.0026)	batch 1.1415 (1.1245)	loss 2.7825 (2.6111)	grad_norm 1.8323 (1.9259)	mem 28740MB
Train: [1/180][9900/10009]	eta 0:02:02 lr 0.049985	data 0.0004 (0.0026)	batch 1.1445 (1.1245)	loss 2.4783 (2.6106)	grad_norm 1.8498 (1.9254)	mem 28740MB
Train: [1/180][9950/10009]	eta 0:01:06 lr 0.049985	data 0.0005 (0.0026)	batch 1.1044 (1.1244)	loss 2.1870 (2.6102)	grad_norm 1.7584 (1.9245)	mem 28740MB
Train: [1/180][10000/10009]	eta 0:00:10 lr 0.049985	data 0.0003 (0.0026)	batch 1.1000 (1.1244)	loss 2.7103 (2.6096)	grad_norm 1.8302 (1.9237)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 1 training takes 3:07:34
Test: [0/391]	Time 13.237 (13.237)	Loss 0.7533 (0.7533)	Acc@1 82.031 (82.031)	Acc@5 93.750 (93.750)	Mem 28740MB
Test: [50/391]	Time 0.255 (0.521)	Loss 0.6513 (1.1287)	Acc@1 85.156 (72.580)	Acc@5 95.312 (90.594)	Mem 28740MB
Test: [100/391]	Time 0.278 (0.394)	Loss 1.0135 (1.1543)	Acc@1 73.438 (70.398)	Acc@5 92.969 (91.197)	Mem 28740MB
Test: [150/391]	Time 0.255 (0.352)	Loss 0.9425 (1.1218)	Acc@1 67.969 (71.420)	Acc@5 95.312 (91.546)	Mem 28740MB
Test: [200/391]	Time 0.255 (0.331)	Loss 1.6023 (1.2629)	Acc@1 57.812 (68.696)	Acc@5 85.156 (89.576)	Mem 28740MB
Test: [250/391]	Time 0.270 (0.318)	Loss 1.1431 (1.3407)	Acc@1 71.875 (67.253)	Acc@5 87.500 (88.253)	Mem 28740MB
Test: [300/391]	Time 0.270 (0.310)	Loss 1.6676 (1.4069)	Acc@1 66.406 (65.942)	Acc@5 82.031 (87.344)	Mem 28740MB
Test: [350/391]	Time 0.270 (0.303)	Loss 1.7063 (1.4618)	Acc@1 64.062 (64.868)	Acc@5 83.594 (86.525)	Mem 28740MB
 * Acc@1 65.016 Acc@5 86.646
Accuracy of the network on the 50000 test images: 65.02%
Max accuracy (after decay): 65.02%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [2/180][0/10009]	eta 2 days, 11:03:06 lr 0.049985	data 20.0833 (20.0833)	batch 21.2395 (21.2395)	loss 2.7904 (2.7904)	grad_norm 1.8659 (1.8659)	mem 28740MB
Train: [2/180][50/10009]	eta 4:11:33 lr 0.049985	data 0.0004 (0.3943)	batch 1.1214 (1.5155)	loss 2.7786 (2.4936)	grad_norm 1.9642 (1.7480)	mem 28740MB
Train: [2/180][100/10009]	eta 3:37:43 lr 0.049985	data 0.0004 (0.1993)	batch 1.1382 (1.3183)	loss 2.6262 (2.4797)	grad_norm 1.8112 (1.7333)	mem 28740MB
Train: [2/180][150/10009]	eta 3:25:53 lr 0.049985	data 0.0005 (0.1335)	batch 1.1111 (1.2530)	loss 2.5512 (2.4706)	grad_norm 1.6975 (1.7341)	mem 28740MB
Train: [2/180][200/10009]	eta 3:19:16 lr 0.049984	data 0.0009 (0.1004)	batch 1.1204 (1.2190)	loss 2.6290 (2.4699)	grad_norm 1.9183 (1.7379)	mem 28740MB
Train: [2/180][250/10009]	eta 3:15:17 lr 0.049984	data 0.0004 (0.0805)	batch 1.1450 (1.2007)	loss 2.2872 (2.4655)	grad_norm 1.6865 (1.7307)	mem 28740MB
Train: [2/180][300/10009]	eta 3:12:09 lr 0.049984	data 0.0005 (0.0672)	batch 1.1106 (1.1875)	loss 2.6705 (2.4718)	grad_norm 1.6626 (1.7339)	mem 28740MB
Train: [2/180][350/10009]	eta 3:09:46 lr 0.049984	data 0.0004 (0.0577)	batch 1.1253 (1.1788)	loss 2.4427 (2.4737)	grad_norm 1.8119 (1.7381)	mem 28740MB
Train: [2/180][400/10009]	eta 3:07:41 lr 0.049984	data 0.0005 (0.0506)	batch 1.1186 (1.1720)	loss 2.5149 (2.4751)	grad_norm 1.7541 (1.7371)	mem 28740MB
Train: [2/180][450/10009]	eta 3:05:55 lr 0.049984	data 0.0004 (0.0450)	batch 1.1351 (1.1671)	loss 2.1601 (2.4697)	grad_norm 1.7580 (1.7349)	mem 28740MB
Train: [2/180][500/10009]	eta 3:04:15 lr 0.049984	data 0.0006 (0.0406)	batch 1.1446 (1.1626)	loss 2.8511 (2.4687)	grad_norm 1.8750 (1.7362)	mem 28740MB
Train: [2/180][550/10009]	eta 3:02:40 lr 0.049984	data 0.0005 (0.0369)	batch 1.1101 (1.1588)	loss 2.6884 (2.4677)	grad_norm 1.8899 (1.7387)	mem 28740MB
Train: [2/180][600/10009]	eta 3:01:13 lr 0.049984	data 0.0005 (0.0339)	batch 1.1239 (1.1556)	loss 2.5177 (2.4686)	grad_norm 1.6169 (1.7383)	mem 28740MB
Train: [2/180][650/10009]	eta 2:59:52 lr 0.049984	data 0.0005 (0.0314)	batch 1.1414 (1.1532)	loss 2.3971 (2.4691)	grad_norm 1.7143 (1.7372)	mem 28740MB
Train: [2/180][700/10009]	eta 2:58:33 lr 0.049984	data 0.0005 (0.0292)	batch 1.1102 (1.1509)	loss 2.4566 (2.4664)	grad_norm 1.6571 (1.7381)	mem 28740MB
Train: [2/180][750/10009]	eta 2:57:16 lr 0.049984	data 0.0005 (0.0272)	batch 1.1180 (1.1488)	loss 2.1783 (2.4637)	grad_norm 1.6534 (1.7388)	mem 28740MB
Train: [2/180][800/10009]	eta 2:56:03 lr 0.049984	data 0.0005 (0.0256)	batch 1.0960 (1.1471)	loss 2.3392 (2.4638)	grad_norm 1.6005 (1.7404)	mem 28740MB
Train: [2/180][850/10009]	eta 2:54:56 lr 0.049983	data 0.0006 (0.0241)	batch 1.1014 (1.1460)	loss 2.5197 (2.4649)	grad_norm 1.7957 (1.7411)	mem 28740MB
Train: [2/180][900/10009]	eta 2:53:47 lr 0.049983	data 0.0004 (0.0228)	batch 1.1442 (1.1448)	loss 2.4885 (2.4656)	grad_norm 1.8138 (1.7412)	mem 28740MB
Train: [2/180][950/10009]	eta 2:52:41 lr 0.049983	data 0.0005 (0.0216)	batch 1.1182 (1.1437)	loss 2.4590 (2.4627)	grad_norm 1.6801 (1.7410)	mem 28740MB
Train: [2/180][1000/10009]	eta 2:51:34 lr 0.049983	data 0.0005 (0.0206)	batch 1.1212 (1.1427)	loss 2.4846 (2.4624)	grad_norm 1.7091 (1.7391)	mem 28740MB
Train: [2/180][1050/10009]	eta 2:50:30 lr 0.049983	data 0.0005 (0.0196)	batch 1.1001 (1.1420)	loss 2.3585 (2.4615)	grad_norm 1.6122 (1.7386)	mem 28740MB
Train: [2/180][1100/10009]	eta 2:49:26 lr 0.049983	data 0.0005 (0.0188)	batch 1.0998 (1.1412)	loss 2.2264 (2.4638)	grad_norm 1.6443 (1.7377)	mem 28740MB
Train: [2/180][1150/10009]	eta 2:48:22 lr 0.049983	data 0.0005 (0.0180)	batch 1.1232 (1.1404)	loss 2.3041 (2.4635)	grad_norm 1.6341 (1.7374)	mem 28740MB
Train: [2/180][1200/10009]	eta 2:47:22 lr 0.049983	data 0.0005 (0.0172)	batch 1.1251 (1.1400)	loss 2.4287 (2.4645)	grad_norm 1.7303 (1.7366)	mem 28740MB
Train: [2/180][1250/10009]	eta 2:46:20 lr 0.049983	data 0.0005 (0.0166)	batch 1.1277 (1.1395)	loss 2.5275 (2.4657)	grad_norm 1.8276 (1.7359)	mem 28740MB
Train: [2/180][1300/10009]	eta 2:45:17 lr 0.049983	data 0.0005 (0.0160)	batch 1.1230 (1.1388)	loss 2.8084 (2.4673)	grad_norm 1.7997 (1.7356)	mem 28740MB
Train: [2/180][1350/10009]	eta 2:44:14 lr 0.049983	data 0.0005 (0.0154)	batch 1.1125 (1.1381)	loss 2.6027 (2.4684)	grad_norm 1.7230 (1.7352)	mem 28740MB
Train: [2/180][1400/10009]	eta 2:43:11 lr 0.049983	data 0.0005 (0.0148)	batch 1.1194 (1.1374)	loss 2.8251 (2.4686)	grad_norm 1.9247 (1.7348)	mem 28740MB
Train: [2/180][1450/10009]	eta 2:42:11 lr 0.049982	data 0.0005 (0.0144)	batch 1.1065 (1.1370)	loss 2.4082 (2.4691)	grad_norm 1.7484 (1.7340)	mem 28740MB
Train: [2/180][1500/10009]	eta 2:41:11 lr 0.049982	data 0.0005 (0.0139)	batch 1.1231 (1.1366)	loss 2.5493 (2.4701)	grad_norm 1.7415 (1.7334)	mem 28740MB
Train: [2/180][1550/10009]	eta 2:40:11 lr 0.049982	data 0.0004 (0.0135)	batch 1.1195 (1.1363)	loss 2.5145 (2.4700)	grad_norm 1.5788 (1.7331)	mem 28740MB
Train: [2/180][1600/10009]	eta 2:39:11 lr 0.049982	data 0.0004 (0.0131)	batch 1.1202 (1.1358)	loss 2.4389 (2.4702)	grad_norm 1.7609 (1.7338)	mem 28740MB
Train: [2/180][1650/10009]	eta 2:38:10 lr 0.049982	data 0.0004 (0.0127)	batch 1.1113 (1.1354)	loss 2.4791 (2.4706)	grad_norm 1.6265 (1.7338)	mem 28740MB
Train: [2/180][1700/10009]	eta 2:37:11 lr 0.049982	data 0.0005 (0.0123)	batch 1.1496 (1.1351)	loss 2.6692 (2.4712)	grad_norm 1.8239 (1.7342)	mem 28740MB
Train: [2/180][1750/10009]	eta 2:36:11 lr 0.049982	data 0.0005 (0.0120)	batch 1.1263 (1.1347)	loss 2.4844 (2.4704)	grad_norm 1.7380 (1.7340)	mem 28740MB
Train: [2/180][1800/10009]	eta 2:35:12 lr 0.049982	data 0.0005 (0.0117)	batch 1.1365 (1.1344)	loss 2.6512 (2.4697)	grad_norm 1.6964 (1.7333)	mem 28740MB
Train: [2/180][1850/10009]	eta 2:34:12 lr 0.049982	data 0.0005 (0.0114)	batch 1.1140 (1.1340)	loss 2.4194 (2.4694)	grad_norm 1.5984 (1.7331)	mem 28740MB
Train: [2/180][1900/10009]	eta 2:33:13 lr 0.049982	data 0.0005 (0.0111)	batch 1.1148 (1.1337)	loss 2.4639 (2.4681)	grad_norm 1.7078 (1.7323)	mem 28740MB
Train: [2/180][1950/10009]	eta 2:32:14 lr 0.049982	data 0.0005 (0.0108)	batch 1.1282 (1.1334)	loss 2.4927 (2.4689)	grad_norm 1.8046 (1.7331)	mem 28740MB
Train: [2/180][2000/10009]	eta 2:31:15 lr 0.049982	data 0.0005 (0.0105)	batch 1.1186 (1.1332)	loss 2.4126 (2.4693)	grad_norm 1.6340 (1.7338)	mem 28740MB
Train: [2/180][2050/10009]	eta 2:30:17 lr 0.049981	data 0.0010 (0.0103)	batch 1.1191 (1.1329)	loss 2.3538 (2.4700)	grad_norm 1.6169 (1.7336)	mem 28740MB
Train: [2/180][2100/10009]	eta 2:29:17 lr 0.049981	data 0.0005 (0.0101)	batch 1.1038 (1.1326)	loss 2.5722 (2.4703)	grad_norm 1.8348 (1.7337)	mem 28740MB
Train: [2/180][2150/10009]	eta 2:28:19 lr 0.049981	data 0.0004 (0.0098)	batch 1.1045 (1.1323)	loss 2.6035 (2.4692)	grad_norm 1.8423 (1.7324)	mem 28740MB
Train: [2/180][2200/10009]	eta 2:27:20 lr 0.049981	data 0.0005 (0.0096)	batch 1.0968 (1.1320)	loss 2.7489 (2.4686)	grad_norm 1.9811 (1.7319)	mem 28740MB
Train: [2/180][2250/10009]	eta 2:26:23 lr 0.049981	data 0.0005 (0.0094)	batch 1.1252 (1.1320)	loss 2.3773 (2.4689)	grad_norm 1.6778 (1.7322)	mem 28740MB
Train: [2/180][2300/10009]	eta 2:25:25 lr 0.049981	data 0.0004 (0.0092)	batch 1.1075 (1.1318)	loss 2.4129 (2.4688)	grad_norm 1.6503 (1.7318)	mem 28740MB
Train: [2/180][2350/10009]	eta 2:24:27 lr 0.049981	data 0.0005 (0.0091)	batch 1.1236 (1.1316)	loss 2.7738 (2.4677)	grad_norm 1.7204 (1.7316)	mem 28740MB
Train: [2/180][2400/10009]	eta 2:23:29 lr 0.049981	data 0.0004 (0.0089)	batch 1.1136 (1.1315)	loss 2.9928 (2.4683)	grad_norm 1.8181 (1.7317)	mem 28740MB
Train: [2/180][2450/10009]	eta 2:22:31 lr 0.049981	data 0.0004 (0.0087)	batch 1.1293 (1.1313)	loss 2.4745 (2.4679)	grad_norm 1.6098 (1.7309)	mem 28740MB
Train: [2/180][2500/10009]	eta 2:21:34 lr 0.049981	data 0.0007 (0.0085)	batch 1.1160 (1.1312)	loss 2.4233 (2.4682)	grad_norm 1.6101 (1.7305)	mem 28740MB
Train: [2/180][2550/10009]	eta 2:20:36 lr 0.049981	data 0.0005 (0.0084)	batch 1.1128 (1.1310)	loss 2.5216 (2.4674)	grad_norm 1.6340 (1.7302)	mem 28740MB
Train: [2/180][2600/10009]	eta 2:19:38 lr 0.049981	data 0.0005 (0.0082)	batch 1.1121 (1.1309)	loss 2.2870 (2.4671)	grad_norm 1.6165 (1.7294)	mem 28740MB
Train: [2/180][2650/10009]	eta 2:18:41 lr 0.049980	data 0.0005 (0.0081)	batch 1.1203 (1.1307)	loss 2.4840 (2.4670)	grad_norm 1.7560 (1.7288)	mem 28740MB
Train: [2/180][2700/10009]	eta 2:17:43 lr 0.049980	data 0.0005 (0.0079)	batch 1.1183 (1.1306)	loss 2.3468 (2.4666)	grad_norm 1.6016 (1.7282)	mem 28740MB
Train: [2/180][2750/10009]	eta 2:16:46 lr 0.049980	data 0.0005 (0.0078)	batch 1.1431 (1.1305)	loss 2.4609 (2.4665)	grad_norm 1.9643 (1.7278)	mem 28740MB
Train: [2/180][2800/10009]	eta 2:15:48 lr 0.049980	data 0.0004 (0.0077)	batch 1.1223 (1.1303)	loss 2.7791 (2.4668)	grad_norm 1.6759 (1.7277)	mem 28740MB
Train: [2/180][2850/10009]	eta 2:14:51 lr 0.049980	data 0.0005 (0.0076)	batch 1.1190 (1.1302)	loss 2.2954 (2.4667)	grad_norm 1.6404 (1.7274)	mem 28740MB
Train: [2/180][2900/10009]	eta 2:13:53 lr 0.049980	data 0.0005 (0.0074)	batch 1.1008 (1.1301)	loss 2.4045 (2.4667)	grad_norm 1.7692 (1.7268)	mem 28740MB
Train: [2/180][2950/10009]	eta 2:12:56 lr 0.049980	data 0.0004 (0.0073)	batch 1.1295 (1.1300)	loss 2.4879 (2.4660)	grad_norm 1.6870 (1.7268)	mem 28740MB
Train: [2/180][3000/10009]	eta 2:11:59 lr 0.049980	data 0.0005 (0.0072)	batch 1.1162 (1.1300)	loss 2.5213 (2.4651)	grad_norm 1.7667 (1.7263)	mem 28740MB
Train: [2/180][3050/10009]	eta 2:11:02 lr 0.049980	data 0.0004 (0.0071)	batch 1.1113 (1.1299)	loss 2.4225 (2.4650)	grad_norm 1.7456 (1.7259)	mem 28740MB
Train: [2/180][3100/10009]	eta 2:10:05 lr 0.049980	data 0.0005 (0.0070)	batch 1.1343 (1.1297)	loss 2.5116 (2.4652)	grad_norm 1.5755 (1.7256)	mem 28740MB
Train: [2/180][3150/10009]	eta 2:09:08 lr 0.049980	data 0.0005 (0.0069)	batch 1.1230 (1.1296)	loss 2.3809 (2.4652)	grad_norm 1.8214 (1.7251)	mem 28740MB
Train: [2/180][3200/10009]	eta 2:08:10 lr 0.049980	data 0.0004 (0.0068)	batch 1.1187 (1.1295)	loss 2.7911 (2.4650)	grad_norm 1.7659 (1.7246)	mem 28740MB
Train: [2/180][3250/10009]	eta 2:07:12 lr 0.049979	data 0.0005 (0.0067)	batch 1.1400 (1.1293)	loss 2.5238 (2.4648)	grad_norm 1.6591 (1.7239)	mem 28740MB
Train: [2/180][3300/10009]	eta 2:06:15 lr 0.049979	data 0.0005 (0.0066)	batch 1.1092 (1.1291)	loss 2.5625 (2.4650)	grad_norm 2.0031 (1.7237)	mem 28740MB
Train: [2/180][3350/10009]	eta 2:05:17 lr 0.049979	data 0.0004 (0.0065)	batch 1.1097 (1.1289)	loss 2.2393 (2.4652)	grad_norm 1.7917 (1.7237)	mem 28740MB
Train: [2/180][3400/10009]	eta 2:04:20 lr 0.049979	data 0.0003 (0.0064)	batch 1.1179 (1.1288)	loss 2.5689 (2.4649)	grad_norm 1.6700 (1.7233)	mem 28740MB
Train: [2/180][3450/10009]	eta 2:03:22 lr 0.049979	data 0.0005 (0.0063)	batch 1.1348 (1.1286)	loss 2.5773 (2.4645)	grad_norm 1.6140 (1.7226)	mem 28740MB
Train: [2/180][3500/10009]	eta 2:02:25 lr 0.049979	data 0.0005 (0.0063)	batch 1.1156 (1.1285)	loss 2.6535 (2.4645)	grad_norm 1.7935 (1.7226)	mem 28740MB
Train: [2/180][3550/10009]	eta 2:01:28 lr 0.049979	data 0.0005 (0.0062)	batch 1.1039 (1.1284)	loss 2.3232 (2.4643)	grad_norm 1.7336 (1.7224)	mem 28740MB
Train: [2/180][3600/10009]	eta 2:00:30 lr 0.049979	data 0.0009 (0.0061)	batch 1.1037 (1.1282)	loss 2.5321 (2.4645)	grad_norm 1.8264 (1.7222)	mem 28740MB
Train: [2/180][3650/10009]	eta 1:59:33 lr 0.049979	data 0.0005 (0.0060)	batch 1.1129 (1.1281)	loss 2.2599 (2.4645)	grad_norm 1.8007 (1.7218)	mem 28740MB
Train: [2/180][3700/10009]	eta 1:58:36 lr 0.049979	data 0.0004 (0.0059)	batch 1.1132 (1.1280)	loss 2.6053 (2.4639)	grad_norm 1.6317 (1.7211)	mem 28740MB
Train: [2/180][3750/10009]	eta 1:57:39 lr 0.049979	data 0.0005 (0.0059)	batch 1.1147 (1.1279)	loss 2.5681 (2.4640)	grad_norm 1.8032 (1.7208)	mem 28740MB
Train: [2/180][3800/10009]	eta 1:56:43 lr 0.049978	data 0.0005 (0.0058)	batch 1.1043 (1.1279)	loss 2.2076 (2.4638)	grad_norm 1.6208 (1.7202)	mem 28740MB
Train: [2/180][3850/10009]	eta 1:55:46 lr 0.049978	data 0.0005 (0.0057)	batch 1.1081 (1.1278)	loss 2.5120 (2.4639)	grad_norm 1.7265 (1.7203)	mem 28740MB
Train: [2/180][3900/10009]	eta 1:54:49 lr 0.049978	data 0.0005 (0.0057)	batch 1.1081 (1.1278)	loss 1.9792 (2.4638)	grad_norm 1.5858 (1.7198)	mem 28740MB
Train: [2/180][3950/10009]	eta 1:53:52 lr 0.049978	data 0.0005 (0.0056)	batch 1.1593 (1.1277)	loss 2.3936 (2.4638)	grad_norm 1.5815 (1.7195)	mem 28740MB
Train: [2/180][4000/10009]	eta 1:52:55 lr 0.049978	data 0.0004 (0.0055)	batch 1.1170 (1.1276)	loss 2.4535 (2.4632)	grad_norm 1.8025 (1.7191)	mem 28740MB
Train: [2/180][4050/10009]	eta 1:51:58 lr 0.049978	data 0.0005 (0.0055)	batch 1.1188 (1.1275)	loss 2.2125 (2.4631)	grad_norm 1.7053 (1.7187)	mem 28740MB
Train: [2/180][4100/10009]	eta 1:51:02 lr 0.049978	data 0.0005 (0.0054)	batch 1.1439 (1.1275)	loss 2.8130 (2.4633)	grad_norm 1.6197 (1.7185)	mem 28740MB
Train: [2/180][4150/10009]	eta 1:50:05 lr 0.049978	data 0.0005 (0.0054)	batch 1.0951 (1.1274)	loss 2.4146 (2.4634)	grad_norm 1.7076 (1.7186)	mem 28740MB
Train: [2/180][4200/10009]	eta 1:49:08 lr 0.049978	data 0.0005 (0.0053)	batch 1.1223 (1.1273)	loss 2.4828 (2.4638)	grad_norm 1.7412 (1.7181)	mem 28740MB
Train: [2/180][4250/10009]	eta 1:48:12 lr 0.049978	data 0.0006 (0.0052)	batch 1.1436 (1.1273)	loss 2.5091 (2.4635)	grad_norm 1.7785 (1.7179)	mem 28740MB
Train: [2/180][4300/10009]	eta 1:47:15 lr 0.049978	data 0.0005 (0.0052)	batch 1.1359 (1.1273)	loss 2.4861 (2.4635)	grad_norm 1.4875 (1.7173)	mem 28740MB
Train: [2/180][4350/10009]	eta 1:46:19 lr 0.049977	data 0.0005 (0.0051)	batch 1.1207 (1.1273)	loss 2.2352 (2.4633)	grad_norm 1.4311 (1.7168)	mem 28740MB
Train: [2/180][4400/10009]	eta 1:45:22 lr 0.049977	data 0.0005 (0.0051)	batch 1.1181 (1.1272)	loss 2.5037 (2.4633)	grad_norm 1.7266 (1.7165)	mem 28740MB
Train: [2/180][4450/10009]	eta 1:44:25 lr 0.049977	data 0.0005 (0.0050)	batch 1.1138 (1.1271)	loss 2.5105 (2.4633)	grad_norm 1.8894 (1.7163)	mem 28740MB
Train: [2/180][4500/10009]	eta 1:43:28 lr 0.049977	data 0.0004 (0.0050)	batch 1.1284 (1.1270)	loss 2.3112 (2.4634)	grad_norm 1.7075 (1.7160)	mem 28740MB
Train: [2/180][4550/10009]	eta 1:42:31 lr 0.049977	data 0.0005 (0.0049)	batch 1.1158 (1.1269)	loss 2.4258 (2.4632)	grad_norm 1.8453 (1.7155)	mem 28740MB
Train: [2/180][4600/10009]	eta 1:41:35 lr 0.049977	data 0.0005 (0.0049)	batch 1.1209 (1.1269)	loss 2.7609 (2.4631)	grad_norm 1.6926 (1.7152)	mem 28740MB
Train: [2/180][4650/10009]	eta 1:40:38 lr 0.049977	data 0.0005 (0.0048)	batch 1.1077 (1.1269)	loss 2.8264 (2.4633)	grad_norm 1.9359 (1.7147)	mem 28740MB
Train: [2/180][4700/10009]	eta 1:39:42 lr 0.049977	data 0.0004 (0.0048)	batch 1.1500 (1.1268)	loss 2.3915 (2.4630)	grad_norm 1.6988 (1.7146)	mem 28740MB
Train: [2/180][4750/10009]	eta 1:38:45 lr 0.049977	data 0.0005 (0.0047)	batch 1.1182 (1.1268)	loss 2.3512 (2.4630)	grad_norm 1.9015 (1.7145)	mem 28740MB
Train: [2/180][4800/10009]	eta 1:37:49 lr 0.049977	data 0.0005 (0.0047)	batch 1.1195 (1.1267)	loss 2.4340 (2.4633)	grad_norm 1.7074 (1.7142)	mem 28740MB
Train: [2/180][4850/10009]	eta 1:36:52 lr 0.049976	data 0.0005 (0.0047)	batch 1.1427 (1.1267)	loss 2.5455 (2.4631)	grad_norm 1.5214 (1.7144)	mem 28740MB
Train: [2/180][4900/10009]	eta 1:35:56 lr 0.049976	data 0.0005 (0.0046)	batch 1.1212 (1.1267)	loss 2.5967 (2.4630)	grad_norm 1.8691 (1.7143)	mem 28740MB
Train: [2/180][4950/10009]	eta 1:34:59 lr 0.049976	data 0.0004 (0.0046)	batch 1.1226 (1.1266)	loss 2.4899 (2.4634)	grad_norm 1.6171 (1.7141)	mem 28740MB
Train: [2/180][5000/10009]	eta 1:34:02 lr 0.049976	data 0.0005 (0.0045)	batch 1.1032 (1.1266)	loss 2.6880 (2.4633)	grad_norm 1.9066 (1.7139)	mem 28740MB
Train: [2/180][5050/10009]	eta 1:33:06 lr 0.049976	data 0.0005 (0.0045)	batch 1.1323 (1.1266)	loss 2.6314 (2.4632)	grad_norm 1.7616 (1.7136)	mem 28740MB
Train: [2/180][5100/10009]	eta 1:32:10 lr 0.049976	data 0.0005 (0.0045)	batch 1.1419 (1.1265)	loss 2.5382 (2.4629)	grad_norm 1.6674 (1.7132)	mem 28740MB
Train: [2/180][5150/10009]	eta 1:31:13 lr 0.049976	data 0.0007 (0.0044)	batch 1.1169 (1.1264)	loss 2.1070 (2.4627)	grad_norm 1.5529 (1.7129)	mem 28740MB
Train: [2/180][5200/10009]	eta 1:30:16 lr 0.049976	data 0.0005 (0.0044)	batch 1.1110 (1.1264)	loss 2.2729 (2.4624)	grad_norm 1.5669 (1.7125)	mem 28740MB
Train: [2/180][5250/10009]	eta 1:29:19 lr 0.049976	data 0.0005 (0.0043)	batch 1.1075 (1.1263)	loss 2.4637 (2.4621)	grad_norm 1.6823 (1.7122)	mem 28740MB
Train: [2/180][5300/10009]	eta 1:28:23 lr 0.049976	data 0.0005 (0.0043)	batch 1.1168 (1.1262)	loss 2.2669 (2.4618)	grad_norm 1.6448 (1.7118)	mem 28740MB
Train: [2/180][5350/10009]	eta 1:27:26 lr 0.049976	data 0.0005 (0.0043)	batch 1.1401 (1.1261)	loss 1.9290 (2.4616)	grad_norm 1.6654 (1.7115)	mem 28740MB
Train: [2/180][5400/10009]	eta 1:26:29 lr 0.049975	data 0.0005 (0.0042)	batch 1.0951 (1.1260)	loss 2.7648 (2.4612)	grad_norm 1.8607 (1.7112)	mem 28740MB
Train: [2/180][5450/10009]	eta 1:25:33 lr 0.049975	data 0.0004 (0.0042)	batch 1.1219 (1.1259)	loss 2.4040 (2.4613)	grad_norm 1.5874 (1.7109)	mem 28740MB
Train: [2/180][5500/10009]	eta 1:24:36 lr 0.049975	data 0.0004 (0.0042)	batch 1.1204 (1.1259)	loss 2.4133 (2.4611)	grad_norm 1.5464 (1.7106)	mem 28740MB
Train: [2/180][5550/10009]	eta 1:23:40 lr 0.049975	data 0.0005 (0.0041)	batch 1.1487 (1.1259)	loss 2.4582 (2.4606)	grad_norm 1.7363 (1.7102)	mem 28740MB
Train: [2/180][5600/10009]	eta 1:22:43 lr 0.049975	data 0.0004 (0.0041)	batch 1.0990 (1.1258)	loss 2.3187 (2.4601)	grad_norm 1.6243 (1.7098)	mem 28740MB
Train: [2/180][5650/10009]	eta 1:21:47 lr 0.049975	data 0.0005 (0.0041)	batch 1.1225 (1.1259)	loss 2.4530 (2.4599)	grad_norm 1.7501 (1.7097)	mem 28740MB
Train: [2/180][5700/10009]	eta 1:20:51 lr 0.049975	data 0.0005 (0.0040)	batch 1.1207 (1.1258)	loss 1.9784 (2.4595)	grad_norm 1.5251 (1.7093)	mem 28740MB
Train: [2/180][5750/10009]	eta 1:19:54 lr 0.049975	data 0.0004 (0.0040)	batch 1.1264 (1.1258)	loss 2.1841 (2.4591)	grad_norm 1.6729 (1.7090)	mem 28740MB
Train: [2/180][5800/10009]	eta 1:18:58 lr 0.049975	data 0.0009 (0.0040)	batch 1.1242 (1.1258)	loss 2.3061 (2.4590)	grad_norm 1.6539 (1.7087)	mem 28740MB
Train: [2/180][5850/10009]	eta 1:18:02 lr 0.049975	data 0.0005 (0.0039)	batch 1.1196 (1.1258)	loss 2.1927 (2.4586)	grad_norm 1.6073 (1.7086)	mem 28740MB
Train: [2/180][5900/10009]	eta 1:17:05 lr 0.049974	data 0.0005 (0.0039)	batch 1.1136 (1.1258)	loss 2.6677 (2.4586)	grad_norm 1.6848 (1.7083)	mem 28740MB
Train: [2/180][5950/10009]	eta 1:16:09 lr 0.049974	data 0.0005 (0.0039)	batch 1.0993 (1.1257)	loss 2.4337 (2.4584)	grad_norm 1.8503 (1.7080)	mem 28740MB
Train: [2/180][6000/10009]	eta 1:15:12 lr 0.049974	data 0.0005 (0.0039)	batch 1.1239 (1.1257)	loss 2.6656 (2.4581)	grad_norm 1.6979 (1.7079)	mem 28740MB
Train: [2/180][6050/10009]	eta 1:14:16 lr 0.049974	data 0.0005 (0.0038)	batch 1.1211 (1.1257)	loss 2.3025 (2.4580)	grad_norm 1.5948 (1.7078)	mem 28740MB
Train: [2/180][6100/10009]	eta 1:13:20 lr 0.049974	data 0.0005 (0.0038)	batch 1.1501 (1.1257)	loss 2.7316 (2.4581)	grad_norm 1.7969 (1.7077)	mem 28740MB
Train: [2/180][6150/10009]	eta 1:12:23 lr 0.049974	data 0.0005 (0.0038)	batch 1.1182 (1.1257)	loss 2.8398 (2.4577)	grad_norm 1.6773 (1.7074)	mem 28740MB
Train: [2/180][6200/10009]	eta 1:11:27 lr 0.049974	data 0.0005 (0.0038)	batch 1.1163 (1.1256)	loss 2.0664 (2.4577)	grad_norm 1.6534 (1.7070)	mem 28740MB
Train: [2/180][6250/10009]	eta 1:10:31 lr 0.049974	data 0.0005 (0.0037)	batch 1.1168 (1.1256)	loss 2.3040 (2.4575)	grad_norm 1.8467 (1.7066)	mem 28740MB
Train: [2/180][6300/10009]	eta 1:09:34 lr 0.049974	data 0.0005 (0.0037)	batch 1.1007 (1.1256)	loss 2.0907 (2.4578)	grad_norm 1.6652 (1.7062)	mem 28740MB
Train: [2/180][6350/10009]	eta 1:08:38 lr 0.049974	data 0.0005 (0.0037)	batch 1.1292 (1.1256)	loss 2.2503 (2.4576)	grad_norm 1.5221 (1.7056)	mem 28740MB
Train: [2/180][6400/10009]	eta 1:07:42 lr 0.049973	data 0.0004 (0.0037)	batch 1.1047 (1.1256)	loss 2.1231 (2.4576)	grad_norm 1.6376 (1.7053)	mem 28740MB
Train: [2/180][6450/10009]	eta 1:06:45 lr 0.049973	data 0.0005 (0.0036)	batch 1.0953 (1.1255)	loss 2.2798 (2.4571)	grad_norm 1.7854 (1.7050)	mem 28740MB
Train: [2/180][6500/10009]	eta 1:05:49 lr 0.049973	data 0.0005 (0.0036)	batch 1.1413 (1.1255)	loss 2.3908 (2.4570)	grad_norm 1.7866 (1.7047)	mem 28740MB
Train: [2/180][6550/10009]	eta 1:04:52 lr 0.049973	data 0.0004 (0.0036)	batch 1.1046 (1.1254)	loss 2.4556 (2.4567)	grad_norm 1.7317 (1.7043)	mem 28740MB
Train: [2/180][6600/10009]	eta 1:03:56 lr 0.049973	data 0.0004 (0.0036)	batch 1.1201 (1.1253)	loss 2.4809 (2.4564)	grad_norm 1.6015 (1.7040)	mem 28740MB
Train: [2/180][6650/10009]	eta 1:02:59 lr 0.049973	data 0.0005 (0.0035)	batch 1.1015 (1.1253)	loss 2.5373 (2.4563)	grad_norm 1.7644 (1.7036)	mem 28740MB
Train: [2/180][6700/10009]	eta 1:02:03 lr 0.049973	data 0.0005 (0.0035)	batch 1.1058 (1.1252)	loss 2.7059 (2.4558)	grad_norm 1.6644 (1.7032)	mem 28740MB
Train: [2/180][6750/10009]	eta 1:01:07 lr 0.049973	data 0.0005 (0.0035)	batch 1.1155 (1.1252)	loss 2.4144 (2.4555)	grad_norm 1.6564 (1.7029)	mem 28740MB
Train: [2/180][6800/10009]	eta 1:00:10 lr 0.049973	data 0.0010 (0.0035)	batch 1.1308 (1.1252)	loss 2.3408 (2.4554)	grad_norm 1.5961 (1.7028)	mem 28740MB
Train: [2/180][6850/10009]	eta 0:59:14 lr 0.049973	data 0.0005 (0.0034)	batch 1.1159 (1.1252)	loss 2.4452 (2.4551)	grad_norm 1.7305 (1.7024)	mem 28740MB
Train: [2/180][6900/10009]	eta 0:58:17 lr 0.049972	data 0.0005 (0.0034)	batch 1.1122 (1.1251)	loss 2.1849 (2.4549)	grad_norm 1.3851 (1.7021)	mem 28740MB
Train: [2/180][6950/10009]	eta 0:57:21 lr 0.049972	data 0.0004 (0.0034)	batch 1.1441 (1.1251)	loss 2.3590 (2.4548)	grad_norm 1.5341 (1.7017)	mem 28740MB
Train: [2/180][7000/10009]	eta 0:56:39 lr 0.049972	data 0.0128 (0.0034)	batch 2.0087 (1.1299)	loss 2.5198 (2.4547)	grad_norm 1.7176 (1.7015)	mem 28740MB
Train: [2/180][7050/10009]	eta 0:56:02 lr 0.049972	data 0.0004 (0.0034)	batch 1.9495 (1.1362)	loss 2.4942 (2.4547)	grad_norm 1.6488 (1.7011)	mem 28740MB
Train: [2/180][7100/10009]	eta 0:55:23 lr 0.049972	data 0.0005 (0.0034)	batch 1.8090 (1.1424)	loss 2.4743 (2.4545)	grad_norm 1.6385 (1.7010)	mem 28740MB
Train: [2/180][7150/10009]	eta 0:54:43 lr 0.049972	data 0.0004 (0.0033)	batch 2.0001 (1.1485)	loss 2.5165 (2.4545)	grad_norm 1.6723 (1.7008)	mem 28740MB
Train: [2/180][7200/10009]	eta 0:54:02 lr 0.049972	data 0.0004 (0.0033)	batch 2.0133 (1.1545)	loss 2.3449 (2.4543)	grad_norm 1.6654 (1.7005)	mem 28740MB
Train: [2/180][7250/10009]	eta 0:53:21 lr 0.049972	data 0.0005 (0.0033)	batch 2.1537 (1.1603)	loss 2.3343 (2.4542)	grad_norm 1.7914 (1.7000)	mem 28740MB
Train: [2/180][7300/10009]	eta 0:52:39 lr 0.049972	data 0.0003 (0.0033)	batch 2.0041 (1.1662)	loss 2.4974 (2.4541)	grad_norm 1.5172 (1.7001)	mem 28740MB
Train: [2/180][7350/10009]	eta 0:51:56 lr 0.049972	data 0.0008 (0.0033)	batch 2.0639 (1.1720)	loss 2.6638 (2.4541)	grad_norm 1.6816 (1.6997)	mem 28740MB
Train: [2/180][7400/10009]	eta 0:51:12 lr 0.049971	data 0.0005 (0.0033)	batch 2.2690 (1.1778)	loss 2.5533 (2.4541)	grad_norm 1.6811 (1.6995)	mem 28740MB
Train: [2/180][7450/10009]	eta 0:50:28 lr 0.049971	data 0.0003 (0.0033)	batch 1.9457 (1.1833)	loss 2.4787 (2.4541)	grad_norm 1.7448 (1.6992)	mem 28740MB
Train: [2/180][7500/10009]	eta 0:49:43 lr 0.049971	data 0.0005 (0.0033)	batch 1.8771 (1.1891)	loss 2.3246 (2.4540)	grad_norm 1.5298 (1.6989)	mem 28740MB
Train: [2/180][7550/10009]	eta 0:48:57 lr 0.049971	data 0.0005 (0.0033)	batch 2.3189 (1.1945)	loss 2.2159 (2.4538)	grad_norm 1.6809 (1.6985)	mem 28740MB
Train: [2/180][7600/10009]	eta 0:48:10 lr 0.049971	data 0.0004 (0.0033)	batch 2.0039 (1.1999)	loss 2.7006 (2.4537)	grad_norm 1.8556 (1.6983)	mem 28740MB
Train: [2/180][7650/10009]	eta 0:47:23 lr 0.049971	data 0.0005 (0.0033)	batch 2.0409 (1.2054)	loss 2.4900 (2.4535)	grad_norm 1.7178 (1.6982)	mem 28740MB
Train: [2/180][7700/10009]	eta 0:46:35 lr 0.049971	data 0.0004 (0.0033)	batch 2.0479 (1.2108)	loss 2.4418 (2.4535)	grad_norm 1.5834 (1.6979)	mem 28740MB
Train: [2/180][7750/10009]	eta 0:45:46 lr 0.049971	data 0.0004 (0.0032)	batch 1.9237 (1.2160)	loss 2.2864 (2.4531)	grad_norm 1.6699 (1.6976)	mem 28740MB
Train: [2/180][7800/10009]	eta 0:44:57 lr 0.049971	data 0.0052 (0.0032)	batch 1.9695 (1.2213)	loss 2.3797 (2.4527)	grad_norm 1.5345 (1.6973)	mem 28740MB
Train: [2/180][7850/10009]	eta 0:44:07 lr 0.049970	data 0.0003 (0.0032)	batch 2.0377 (1.2265)	loss 2.6529 (2.4526)	grad_norm 1.7651 (1.6972)	mem 28740MB
Train: [2/180][7900/10009]	eta 0:43:17 lr 0.049970	data 0.0005 (0.0032)	batch 2.0488 (1.2316)	loss 2.3844 (2.4525)	grad_norm 1.7116 (1.6970)	mem 28740MB
Train: [2/180][7950/10009]	eta 0:42:20 lr 0.049970	data 0.0005 (0.0032)	batch 1.1843 (1.2341)	loss 2.5161 (2.4521)	grad_norm 1.7013 (1.6967)	mem 28740MB
Train: [2/180][8000/10009]	eta 0:41:17 lr 0.049970	data 0.0004 (0.0032)	batch 1.1314 (1.2334)	loss 2.5780 (2.4522)	grad_norm 1.7211 (1.6963)	mem 28740MB
Train: [2/180][8050/10009]	eta 0:40:14 lr 0.049970	data 0.0005 (0.0032)	batch 1.1208 (1.2327)	loss 2.5827 (2.4519)	grad_norm 1.4952 (1.6959)	mem 28740MB
Train: [2/180][8100/10009]	eta 0:39:11 lr 0.049970	data 0.0004 (0.0031)	batch 1.1075 (1.2321)	loss 2.5251 (2.4517)	grad_norm 1.5852 (1.6956)	mem 28740MB
Train: [2/180][8150/10009]	eta 0:38:09 lr 0.049970	data 0.0005 (0.0031)	batch 1.1227 (1.2314)	loss 2.6806 (2.4515)	grad_norm 1.8294 (1.6954)	mem 28740MB
Train: [2/180][8200/10009]	eta 0:37:06 lr 0.049970	data 0.0005 (0.0031)	batch 1.1260 (1.2307)	loss 2.4283 (2.4514)	grad_norm 1.6155 (1.6952)	mem 28740MB
Train: [2/180][8250/10009]	eta 0:36:03 lr 0.049970	data 0.0004 (0.0031)	batch 1.1269 (1.2301)	loss 2.4773 (2.4511)	grad_norm 1.8520 (1.6948)	mem 28740MB
Train: [2/180][8300/10009]	eta 0:35:01 lr 0.049970	data 0.0004 (0.0031)	batch 1.1875 (1.2295)	loss 2.5628 (2.4512)	grad_norm 1.6395 (1.6946)	mem 28740MB
Train: [2/180][8350/10009]	eta 0:33:59 lr 0.049969	data 0.0004 (0.0031)	batch 1.1069 (1.2296)	loss 2.1802 (2.4512)	grad_norm 1.6222 (1.6944)	mem 28740MB
Train: [2/180][8400/10009]	eta 0:32:57 lr 0.049969	data 0.0005 (0.0031)	batch 1.1440 (1.2290)	loss 2.5448 (2.4512)	grad_norm 1.8946 (1.6944)	mem 28740MB
Train: [2/180][8450/10009]	eta 0:31:54 lr 0.049969	data 0.0005 (0.0030)	batch 1.1133 (1.2283)	loss 2.1631 (2.4513)	grad_norm 1.6677 (1.6941)	mem 28740MB
Train: [2/180][8500/10009]	eta 0:30:52 lr 0.049969	data 0.0005 (0.0030)	batch 1.1728 (1.2277)	loss 2.4093 (2.4507)	grad_norm 1.7191 (1.6937)	mem 28740MB
Train: [2/180][8550/10009]	eta 0:29:50 lr 0.049969	data 0.0005 (0.0030)	batch 1.1386 (1.2271)	loss 2.5258 (2.4505)	grad_norm 1.5824 (1.6935)	mem 28740MB
Train: [2/180][8600/10009]	eta 0:28:48 lr 0.049969	data 0.0005 (0.0030)	batch 1.1171 (1.2266)	loss 2.2672 (2.4505)	grad_norm 1.5136 (1.6934)	mem 28740MB
Train: [2/180][8650/10009]	eta 0:27:46 lr 0.049969	data 0.0005 (0.0030)	batch 1.1392 (1.2260)	loss 2.2826 (2.4501)	grad_norm 1.5608 (1.6930)	mem 28740MB
Train: [2/180][8700/10009]	eta 0:26:44 lr 0.049969	data 0.0005 (0.0030)	batch 1.1150 (1.2254)	loss 2.6361 (2.4501)	grad_norm 1.7413 (1.6928)	mem 28740MB
Train: [2/180][8750/10009]	eta 0:25:42 lr 0.049969	data 0.0006 (0.0030)	batch 1.1785 (1.2248)	loss 2.1662 (2.4497)	grad_norm 1.6069 (1.6925)	mem 28740MB
Train: [2/180][8800/10009]	eta 0:24:40 lr 0.049968	data 0.0005 (0.0029)	batch 1.1057 (1.2243)	loss 2.3067 (2.4494)	grad_norm 1.6652 (1.6920)	mem 28740MB
Train: [2/180][8850/10009]	eta 0:23:38 lr 0.049968	data 0.0005 (0.0029)	batch 1.1103 (1.2238)	loss 2.2235 (2.4492)	grad_norm 1.6831 (1.6919)	mem 28740MB
Train: [2/180][8900/10009]	eta 0:22:36 lr 0.049968	data 0.0005 (0.0029)	batch 1.1034 (1.2232)	loss 2.3184 (2.4492)	grad_norm 1.6020 (1.6916)	mem 28740MB
Train: [2/180][8950/10009]	eta 0:21:34 lr 0.049968	data 0.0005 (0.0029)	batch 1.8948 (1.2228)	loss 2.3099 (2.4491)	grad_norm 1.6535 (1.6914)	mem 28740MB
Train: [2/180][9000/10009]	eta 0:20:33 lr 0.049968	data 0.0007 (0.0029)	batch 1.1088 (1.2222)	loss 2.4637 (2.4489)	grad_norm 1.5851 (1.6910)	mem 28740MB
Train: [2/180][9050/10009]	eta 0:19:31 lr 0.049968	data 0.0004 (0.0029)	batch 1.1160 (1.2217)	loss 2.3511 (2.4486)	grad_norm 1.6227 (1.6906)	mem 28740MB
Train: [2/180][9100/10009]	eta 0:18:29 lr 0.049968	data 0.0005 (0.0029)	batch 1.1110 (1.2211)	loss 2.3119 (2.4485)	grad_norm 1.5476 (1.6903)	mem 28740MB
Train: [2/180][9150/10009]	eta 0:17:28 lr 0.049968	data 0.0006 (0.0028)	batch 1.1229 (1.2205)	loss 2.5106 (2.4483)	grad_norm 1.7576 (1.6901)	mem 28740MB
Train: [2/180][9200/10009]	eta 0:16:27 lr 0.049968	data 0.0005 (0.0028)	batch 1.1185 (1.2201)	loss 2.3112 (2.4482)	grad_norm 1.5064 (1.6898)	mem 28740MB
Train: [2/180][9250/10009]	eta 0:15:25 lr 0.049967	data 0.0005 (0.0028)	batch 1.1093 (1.2195)	loss 2.3679 (2.4481)	grad_norm 1.7086 (1.6896)	mem 28740MB
Train: [2/180][9300/10009]	eta 0:14:24 lr 0.049967	data 0.0005 (0.0028)	batch 1.1133 (1.2190)	loss 2.2971 (2.4480)	grad_norm 1.6665 (1.6894)	mem 28740MB
Train: [2/180][9350/10009]	eta 0:13:22 lr 0.049967	data 0.0005 (0.0028)	batch 1.1167 (1.2185)	loss 2.4059 (2.4477)	grad_norm 1.8206 (1.6890)	mem 28740MB
Train: [2/180][9400/10009]	eta 0:12:21 lr 0.049967	data 0.0005 (0.0028)	batch 1.1170 (1.2179)	loss 2.2598 (2.4475)	grad_norm 1.5940 (1.6887)	mem 28740MB
Train: [2/180][9450/10009]	eta 0:11:20 lr 0.049967	data 0.0003 (0.0028)	batch 1.2023 (1.2174)	loss 2.3441 (2.4474)	grad_norm 1.6135 (1.6884)	mem 28740MB
Train: [2/180][9500/10009]	eta 0:10:19 lr 0.049967	data 0.0004 (0.0028)	batch 1.0811 (1.2170)	loss 2.1502 (2.4472)	grad_norm 1.7660 (1.6882)	mem 28740MB
Train: [2/180][9550/10009]	eta 0:09:18 lr 0.049967	data 0.0003 (0.0028)	batch 1.1289 (1.2166)	loss 2.6048 (2.4471)	grad_norm 1.5586 (1.6879)	mem 28740MB
Train: [2/180][9600/10009]	eta 0:08:17 lr 0.049967	data 0.0004 (0.0028)	batch 1.1548 (1.2160)	loss 2.4862 (2.4468)	grad_norm 1.7826 (1.6876)	mem 28740MB
Train: [2/180][9650/10009]	eta 0:07:16 lr 0.049967	data 0.0005 (0.0028)	batch 1.0649 (1.2155)	loss 2.2673 (2.4466)	grad_norm 1.5690 (1.6874)	mem 28740MB
Train: [2/180][9700/10009]	eta 0:06:15 lr 0.049966	data 0.0003 (0.0028)	batch 1.0465 (1.2150)	loss 2.0947 (2.4462)	grad_norm 1.5481 (1.6871)	mem 28740MB
Train: [2/180][9750/10009]	eta 0:05:14 lr 0.049966	data 0.0003 (0.0028)	batch 1.0535 (1.2145)	loss 2.6315 (2.4460)	grad_norm 1.6619 (1.6868)	mem 28740MB
Train: [2/180][9800/10009]	eta 0:04:13 lr 0.049966	data 0.0003 (0.0028)	batch 1.1208 (1.2140)	loss 2.1769 (2.4459)	grad_norm 1.6869 (1.6866)	mem 28740MB
Train: [2/180][9850/10009]	eta 0:03:12 lr 0.049966	data 0.0004 (0.0028)	batch 1.1444 (1.2135)	loss 2.5228 (2.4458)	grad_norm 1.6187 (1.6863)	mem 28740MB
Train: [2/180][9900/10009]	eta 0:02:12 lr 0.049966	data 0.0051 (0.0028)	batch 1.0603 (1.2131)	loss 2.4597 (2.4457)	grad_norm 1.7749 (1.6861)	mem 28740MB
Train: [2/180][9950/10009]	eta 0:01:11 lr 0.049966	data 0.0055 (0.0028)	batch 1.1070 (1.2125)	loss 2.5973 (2.4457)	grad_norm 1.7186 (1.6860)	mem 28740MB
Train: [2/180][10000/10009]	eta 0:00:10 lr 0.049966	data 0.0002 (0.0028)	batch 1.1173 (1.2120)	loss 2.3230 (2.4455)	grad_norm 1.6361 (1.6858)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 2 training takes 3:22:10
Test: [0/391]	Time 11.952 (11.952)	Loss 0.6921 (0.6921)	Acc@1 85.938 (85.938)	Acc@5 94.531 (94.531)	Mem 28740MB
Test: [50/391]	Time 0.278 (0.511)	Loss 0.6050 (1.0660)	Acc@1 85.938 (73.882)	Acc@5 93.750 (91.008)	Mem 28740MB
Test: [100/391]	Time 0.265 (0.400)	Loss 1.0320 (1.1236)	Acc@1 73.438 (71.434)	Acc@5 93.750 (91.213)	Mem 28740MB
Test: [150/391]	Time 0.265 (0.362)	Loss 1.0471 (1.1002)	Acc@1 66.406 (72.092)	Acc@5 96.094 (91.686)	Mem 28740MB
Test: [200/391]	Time 0.262 (0.343)	Loss 1.5712 (1.2387)	Acc@1 61.719 (69.426)	Acc@5 85.938 (89.817)	Mem 28740MB
Test: [250/391]	Time 0.256 (0.330)	Loss 1.0146 (1.3103)	Acc@1 73.438 (68.152)	Acc@5 90.625 (88.670)	Mem 28740MB
Test: [300/391]	Time 0.321 (0.325)	Loss 1.4666 (1.3738)	Acc@1 71.875 (67.001)	Acc@5 84.375 (87.702)	Mem 28740MB
Test: [350/391]	Time 0.266 (0.319)	Loss 1.4747 (1.4225)	Acc@1 65.625 (66.075)	Acc@5 85.156 (87.055)	Mem 28740MB
 * Acc@1 66.212 Acc@5 87.156
Accuracy of the network on the 50000 test images: 66.21%
Max accuracy (after decay): 66.21%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [3/180][0/10009]	eta 2 days, 9:52:35 lr 0.049966	data 19.3636 (19.3636)	batch 20.8168 (20.8168)	loss 2.4412 (2.4412)	grad_norm 1.5592 (1.5592)	mem 28740MB
Train: [3/180][50/10009]	eta 4:10:30 lr 0.049966	data 0.0007 (0.3810)	batch 1.1014 (1.5092)	loss 2.4564 (2.4047)	grad_norm 1.6516 (1.6025)	mem 28740MB
Train: [3/180][100/10009]	eta 3:37:38 lr 0.049966	data 0.0005 (0.1927)	batch 1.1326 (1.3178)	loss 2.6447 (2.3902)	grad_norm 1.6345 (1.5911)	mem 28740MB
Train: [3/180][150/10009]	eta 3:26:05 lr 0.049965	data 0.0005 (0.1291)	batch 1.1325 (1.2543)	loss 2.3335 (2.3883)	grad_norm 1.5800 (1.5913)	mem 28740MB
Train: [3/180][200/10009]	eta 3:19:15 lr 0.049965	data 0.0003 (0.0974)	batch 1.0610 (1.2188)	loss 2.3387 (2.3909)	grad_norm 1.4127 (1.5963)	mem 28740MB
Train: [3/180][250/10009]	eta 3:15:26 lr 0.049965	data 0.0003 (0.0785)	batch 1.1325 (1.2016)	loss 2.4730 (2.3979)	grad_norm 1.6044 (1.6016)	mem 28740MB
Train: [3/180][300/10009]	eta 3:12:33 lr 0.049965	data 0.0133 (0.0662)	batch 1.1354 (1.1900)	loss 2.5806 (2.4014)	grad_norm 1.6077 (1.6072)	mem 28740MB
Train: [3/180][350/10009]	eta 3:09:58 lr 0.049965	data 0.0003 (0.0571)	batch 1.2068 (1.1801)	loss 2.3236 (2.4027)	grad_norm 1.7221 (1.6098)	mem 28740MB
Train: [3/180][400/10009]	eta 3:07:42 lr 0.049965	data 0.0005 (0.0504)	batch 1.0510 (1.1721)	loss 2.4096 (2.4057)	grad_norm 1.5773 (1.6128)	mem 28740MB
Train: [3/180][450/10009]	eta 3:05:39 lr 0.049965	data 0.0043 (0.0451)	batch 1.0863 (1.1654)	loss 2.3290 (2.4057)	grad_norm 1.5123 (1.6141)	mem 28740MB
Train: [3/180][500/10009]	eta 3:03:48 lr 0.049965	data 0.0011 (0.0409)	batch 1.1634 (1.1598)	loss 2.0947 (2.4042)	grad_norm 1.5279 (1.6143)	mem 28740MB
Train: [3/180][550/10009]	eta 3:02:01 lr 0.049964	data 0.0006 (0.0374)	batch 1.1454 (1.1546)	loss 2.3831 (2.4035)	grad_norm 1.6293 (1.6141)	mem 28740MB
Train: [3/180][600/10009]	eta 3:00:28 lr 0.049964	data 0.0004 (0.0345)	batch 1.0661 (1.1508)	loss 2.4074 (2.4034)	grad_norm 1.6767 (1.6153)	mem 28740MB
Train: [3/180][650/10009]	eta 2:59:09 lr 0.049964	data 0.0003 (0.0321)	batch 1.0688 (1.1485)	loss 2.5824 (2.4052)	grad_norm 1.5897 (1.6162)	mem 28740MB
Train: [3/180][700/10009]	eta 2:57:39 lr 0.049964	data 0.0071 (0.0299)	batch 1.0674 (1.1451)	loss 2.2865 (2.4040)	grad_norm 1.6101 (1.6159)	mem 28740MB
Train: [3/180][750/10009]	eta 2:56:22 lr 0.049964	data 0.0004 (0.0281)	batch 1.0919 (1.1429)	loss 2.4060 (2.4037)	grad_norm 1.7624 (1.6161)	mem 28740MB
Train: [3/180][800/10009]	eta 2:55:09 lr 0.049964	data 0.0024 (0.0265)	batch 1.1022 (1.1412)	loss 2.3655 (2.4044)	grad_norm 1.6481 (1.6176)	mem 28740MB
Train: [3/180][850/10009]	eta 2:53:48 lr 0.049964	data 0.0003 (0.0251)	batch 1.0466 (1.1386)	loss 2.3629 (2.4040)	grad_norm 1.6111 (1.6167)	mem 28740MB
Train: [3/180][900/10009]	eta 2:52:33 lr 0.049964	data 0.0050 (0.0238)	batch 1.0548 (1.1366)	loss 2.2109 (2.4008)	grad_norm 1.5023 (1.6151)	mem 28740MB
Train: [3/180][950/10009]	eta 2:51:28 lr 0.049964	data 0.0004 (0.0226)	batch 1.1034 (1.1357)	loss 2.6257 (2.4001)	grad_norm 1.4928 (1.6142)	mem 28740MB
Train: [3/180][1000/10009]	eta 2:50:24 lr 0.049963	data 0.0005 (0.0215)	batch 1.1182 (1.1350)	loss 2.1964 (2.4007)	grad_norm 1.6344 (1.6139)	mem 28740MB
Train: [3/180][1050/10009]	eta 2:49:19 lr 0.049963	data 0.0005 (0.0205)	batch 1.1114 (1.1340)	loss 2.3098 (2.4011)	grad_norm 1.5446 (1.6136)	mem 28740MB
Train: [3/180][1100/10009]	eta 2:48:15 lr 0.049963	data 0.0005 (0.0196)	batch 1.1221 (1.1332)	loss 2.4450 (2.3999)	grad_norm 1.5372 (1.6146)	mem 28740MB
Train: [3/180][1150/10009]	eta 2:47:14 lr 0.049963	data 0.0005 (0.0187)	batch 1.1234 (1.1327)	loss 2.3925 (2.4001)	grad_norm 1.5884 (1.6144)	mem 28740MB
Train: [3/180][1200/10009]	eta 2:46:11 lr 0.049963	data 0.0006 (0.0180)	batch 1.1118 (1.1320)	loss 2.3944 (2.3999)	grad_norm 1.5473 (1.6142)	mem 28740MB
Train: [3/180][1250/10009]	eta 2:45:09 lr 0.049963	data 0.0005 (0.0173)	batch 1.1182 (1.1314)	loss 2.4958 (2.3997)	grad_norm 1.6459 (1.6139)	mem 28740MB
Train: [3/180][1300/10009]	eta 2:44:07 lr 0.049963	data 0.0005 (0.0166)	batch 1.1218 (1.1307)	loss 2.2927 (2.4001)	grad_norm 1.5572 (1.6148)	mem 28740MB
Train: [3/180][1350/10009]	eta 2:43:07 lr 0.049963	data 0.0005 (0.0160)	batch 1.1121 (1.1304)	loss 2.3625 (2.3992)	grad_norm 1.4306 (1.6161)	mem 28740MB
Train: [3/180][1400/10009]	eta 2:42:08 lr 0.049962	data 0.0005 (0.0155)	batch 1.1354 (1.1301)	loss 2.3573 (2.3997)	grad_norm 1.6760 (1.6155)	mem 28740MB
Train: [3/180][1450/10009]	eta 2:41:08 lr 0.049962	data 0.0005 (0.0150)	batch 1.1203 (1.1297)	loss 2.1642 (2.3980)	grad_norm 1.6045 (1.6155)	mem 28740MB
Train: [3/180][1500/10009]	eta 2:40:10 lr 0.049962	data 0.0005 (0.0145)	batch 1.1157 (1.1295)	loss 2.5122 (2.3986)	grad_norm 1.7470 (1.6159)	mem 28740MB
Train: [3/180][1550/10009]	eta 2:39:11 lr 0.049962	data 0.0005 (0.0140)	batch 1.0996 (1.1292)	loss 2.3385 (2.3972)	grad_norm 1.4507 (1.6157)	mem 28740MB
Train: [3/180][1600/10009]	eta 2:38:13 lr 0.049962	data 0.0005 (0.0136)	batch 1.1219 (1.1289)	loss 2.3267 (2.3964)	grad_norm 1.6038 (1.6155)	mem 28740MB
Train: [3/180][1650/10009]	eta 2:37:14 lr 0.049962	data 0.0005 (0.0132)	batch 1.1003 (1.1286)	loss 2.1539 (2.3945)	grad_norm 1.7338 (1.6144)	mem 28740MB
Train: [3/180][1700/10009]	eta 2:36:15 lr 0.049962	data 0.0004 (0.0128)	batch 1.1224 (1.1284)	loss 2.5458 (2.3956)	grad_norm 1.6231 (1.6150)	mem 28740MB
Train: [3/180][1750/10009]	eta 2:35:17 lr 0.049962	data 0.0005 (0.0125)	batch 1.1194 (1.1282)	loss 2.4105 (2.3951)	grad_norm 1.7550 (1.6156)	mem 28740MB
Train: [3/180][1800/10009]	eta 2:34:19 lr 0.049962	data 0.0005 (0.0122)	batch 1.1079 (1.1279)	loss 2.4251 (2.3943)	grad_norm 1.6394 (1.6150)	mem 28740MB
Train: [3/180][1850/10009]	eta 2:33:22 lr 0.049961	data 0.0005 (0.0118)	batch 1.1414 (1.1279)	loss 2.1960 (2.3937)	grad_norm 1.5215 (1.6143)	mem 28740MB
Train: [3/180][1900/10009]	eta 2:32:24 lr 0.049961	data 0.0005 (0.0115)	batch 1.1287 (1.1277)	loss 2.3260 (2.3946)	grad_norm 1.6229 (1.6137)	mem 28740MB
Train: [3/180][1950/10009]	eta 2:31:26 lr 0.049961	data 0.0005 (0.0113)	batch 1.0970 (1.1275)	loss 2.1676 (2.3933)	grad_norm 1.4696 (1.6128)	mem 28740MB
Train: [3/180][2000/10009]	eta 2:30:28 lr 0.049961	data 0.0004 (0.0110)	batch 1.1191 (1.1273)	loss 2.3455 (2.3925)	grad_norm 1.5050 (1.6128)	mem 28740MB
Train: [3/180][2050/10009]	eta 2:29:30 lr 0.049961	data 0.0006 (0.0107)	batch 1.1041 (1.1271)	loss 2.5961 (2.3923)	grad_norm 1.5796 (1.6130)	mem 28740MB
Train: [3/180][2100/10009]	eta 2:28:33 lr 0.049961	data 0.0005 (0.0105)	batch 1.1355 (1.1270)	loss 2.4496 (2.3918)	grad_norm 1.6572 (1.6125)	mem 28740MB
Train: [3/180][2150/10009]	eta 2:27:35 lr 0.049961	data 0.0005 (0.0103)	batch 1.1216 (1.1268)	loss 2.2527 (2.3915)	grad_norm 1.5637 (1.6124)	mem 28740MB
Train: [3/180][2200/10009]	eta 2:26:38 lr 0.049961	data 0.0005 (0.0100)	batch 1.1431 (1.1267)	loss 2.6142 (2.3917)	grad_norm 1.6608 (1.6124)	mem 28740MB
Train: [3/180][2250/10009]	eta 2:25:41 lr 0.049960	data 0.0004 (0.0098)	batch 1.1373 (1.1266)	loss 2.1061 (2.3917)	grad_norm 1.4038 (1.6114)	mem 28740MB
Train: [3/180][2300/10009]	eta 2:24:44 lr 0.049960	data 0.0005 (0.0096)	batch 1.1117 (1.1265)	loss 2.2931 (2.3909)	grad_norm 1.5342 (1.6107)	mem 28740MB
Train: [3/180][2350/10009]	eta 2:23:46 lr 0.049960	data 0.0005 (0.0094)	batch 1.1315 (1.1264)	loss 2.2137 (2.3911)	grad_norm 1.7821 (1.6107)	mem 28740MB
Train: [3/180][2400/10009]	eta 2:22:49 lr 0.049960	data 0.0005 (0.0092)	batch 1.1397 (1.1263)	loss 2.2943 (2.3916)	grad_norm 1.4801 (1.6105)	mem 28740MB
Train: [3/180][2450/10009]	eta 2:21:51 lr 0.049960	data 0.0005 (0.0091)	batch 1.1119 (1.1260)	loss 2.8410 (2.3917)	grad_norm 1.6511 (1.6108)	mem 28740MB
Train: [3/180][2500/10009]	eta 2:20:54 lr 0.049960	data 0.0006 (0.0089)	batch 1.1215 (1.1259)	loss 2.5378 (2.3918)	grad_norm 1.6209 (1.6109)	mem 28740MB
Train: [3/180][2550/10009]	eta 2:19:56 lr 0.049960	data 0.0005 (0.0087)	batch 1.1147 (1.1257)	loss 2.4740 (2.3918)	grad_norm 1.6385 (1.6107)	mem 28740MB
Train: [3/180][2600/10009]	eta 2:18:59 lr 0.049960	data 0.0005 (0.0086)	batch 1.1011 (1.1256)	loss 2.4506 (2.3921)	grad_norm 1.5942 (1.6105)	mem 28740MB
Train: [3/180][2650/10009]	eta 2:18:02 lr 0.049959	data 0.0005 (0.0084)	batch 1.1048 (1.1255)	loss 2.3863 (2.3915)	grad_norm 1.6011 (1.6107)	mem 28740MB
Train: [3/180][2700/10009]	eta 2:17:05 lr 0.049959	data 0.0005 (0.0083)	batch 1.1118 (1.1253)	loss 2.3873 (2.3928)	grad_norm 1.6556 (1.6107)	mem 28740MB
Train: [3/180][2750/10009]	eta 2:16:07 lr 0.049959	data 0.0005 (0.0081)	batch 1.1172 (1.1252)	loss 2.4366 (2.3922)	grad_norm 1.5903 (1.6109)	mem 28740MB
Train: [3/180][2800/10009]	eta 2:15:10 lr 0.049959	data 0.0005 (0.0080)	batch 1.1129 (1.1251)	loss 2.3761 (2.3923)	grad_norm 1.5728 (1.6109)	mem 28740MB
Train: [3/180][2850/10009]	eta 2:14:13 lr 0.049959	data 0.0004 (0.0079)	batch 1.1160 (1.1249)	loss 2.6901 (2.3930)	grad_norm 1.6780 (1.6109)	mem 28740MB
Train: [3/180][2900/10009]	eta 2:13:16 lr 0.049959	data 0.0005 (0.0077)	batch 1.1265 (1.1248)	loss 2.4862 (2.3921)	grad_norm 1.6221 (1.6105)	mem 28740MB
Train: [3/180][2950/10009]	eta 2:12:19 lr 0.049959	data 0.0005 (0.0076)	batch 1.1101 (1.1247)	loss 2.7003 (2.3922)	grad_norm 1.6791 (1.6105)	mem 28740MB
Train: [3/180][3000/10009]	eta 2:11:22 lr 0.049959	data 0.0005 (0.0075)	batch 1.1186 (1.1246)	loss 2.1888 (2.3922)	grad_norm 1.5614 (1.6104)	mem 28740MB
Train: [3/180][3050/10009]	eta 2:10:26 lr 0.049958	data 0.0004 (0.0074)	batch 1.0963 (1.1246)	loss 2.4191 (2.3925)	grad_norm 1.5732 (1.6101)	mem 28740MB
Train: [3/180][3100/10009]	eta 2:09:29 lr 0.049958	data 0.0006 (0.0073)	batch 1.1135 (1.1245)	loss 2.3203 (2.3918)	grad_norm 1.4978 (1.6098)	mem 28740MB
Train: [3/180][3150/10009]	eta 2:08:32 lr 0.049958	data 0.0004 (0.0072)	batch 1.1026 (1.1245)	loss 2.2090 (2.3915)	grad_norm 1.4466 (1.6095)	mem 28740MB
Train: [3/180][3200/10009]	eta 2:07:36 lr 0.049958	data 0.0005 (0.0071)	batch 1.1227 (1.1244)	loss 2.2360 (2.3911)	grad_norm 1.5166 (1.6092)	mem 28740MB
Train: [3/180][3250/10009]	eta 2:06:39 lr 0.049958	data 0.0005 (0.0070)	batch 1.1247 (1.1244)	loss 2.4294 (2.3914)	grad_norm 1.6107 (1.6090)	mem 28740MB
Train: [3/180][3300/10009]	eta 2:05:43 lr 0.049958	data 0.0005 (0.0069)	batch 1.1170 (1.1243)	loss 2.4429 (2.3920)	grad_norm 1.4704 (1.6094)	mem 28740MB
Train: [3/180][3350/10009]	eta 2:04:46 lr 0.049958	data 0.0005 (0.0068)	batch 1.1380 (1.1243)	loss 2.4345 (2.3917)	grad_norm 1.6482 (1.6092)	mem 28740MB
Train: [3/180][3400/10009]	eta 2:03:50 lr 0.049958	data 0.0005 (0.0067)	batch 1.1005 (1.1242)	loss 2.2936 (2.3913)	grad_norm 1.5450 (1.6090)	mem 28740MB
Train: [3/180][3450/10009]	eta 2:02:53 lr 0.049957	data 0.0005 (0.0066)	batch 1.1063 (1.1241)	loss 2.2335 (2.3912)	grad_norm 1.6769 (1.6092)	mem 28740MB
Train: [3/180][3500/10009]	eta 2:01:56 lr 0.049957	data 0.0004 (0.0065)	batch 1.1399 (1.1241)	loss 2.3831 (2.3912)	grad_norm 1.5453 (1.6088)	mem 28740MB
Train: [3/180][3550/10009]	eta 2:00:59 lr 0.049957	data 0.0005 (0.0064)	batch 1.1158 (1.1240)	loss 2.4299 (2.3909)	grad_norm 1.4473 (1.6086)	mem 28740MB
Train: [3/180][3600/10009]	eta 2:00:03 lr 0.049957	data 0.0005 (0.0063)	batch 1.1293 (1.1240)	loss 2.4636 (2.3911)	grad_norm 1.5420 (1.6084)	mem 28740MB
Train: [3/180][3650/10009]	eta 1:59:07 lr 0.049957	data 0.0005 (0.0063)	batch 1.1412 (1.1240)	loss 2.5165 (2.3914)	grad_norm 1.5530 (1.6085)	mem 28740MB
Train: [3/180][3700/10009]	eta 1:58:10 lr 0.049957	data 0.0005 (0.0062)	batch 1.1155 (1.1239)	loss 2.4660 (2.3907)	grad_norm 1.4810 (1.6082)	mem 28740MB
Train: [3/180][3750/10009]	eta 1:57:14 lr 0.049957	data 0.0005 (0.0061)	batch 1.1387 (1.1239)	loss 2.7714 (2.3908)	grad_norm 1.4978 (1.6079)	mem 28740MB
Train: [3/180][3800/10009]	eta 1:56:17 lr 0.049957	data 0.0005 (0.0060)	batch 1.1168 (1.1238)	loss 2.7273 (2.3904)	grad_norm 1.6089 (1.6075)	mem 28740MB
Train: [3/180][3850/10009]	eta 1:55:21 lr 0.049956	data 0.0005 (0.0060)	batch 1.1141 (1.1238)	loss 2.5237 (2.3898)	grad_norm 1.6316 (1.6073)	mem 28740MB
Train: [3/180][3900/10009]	eta 1:54:24 lr 0.049956	data 0.0004 (0.0059)	batch 1.1130 (1.1237)	loss 2.2728 (2.3904)	grad_norm 1.6403 (1.6071)	mem 28740MB
Train: [3/180][3950/10009]	eta 1:53:28 lr 0.049956	data 0.0004 (0.0058)	batch 1.1090 (1.1237)	loss 2.3537 (2.3903)	grad_norm 1.6674 (1.6070)	mem 28740MB
Train: [3/180][4000/10009]	eta 1:52:31 lr 0.049956	data 0.0005 (0.0058)	batch 1.0979 (1.1236)	loss 2.4309 (2.3901)	grad_norm 1.6428 (1.6071)	mem 28740MB
Train: [3/180][4050/10009]	eta 1:51:35 lr 0.049956	data 0.0005 (0.0057)	batch 1.1013 (1.1236)	loss 2.2299 (2.3900)	grad_norm 1.5951 (1.6069)	mem 28740MB
Train: [3/180][4100/10009]	eta 1:50:39 lr 0.049956	data 0.0004 (0.0056)	batch 1.1128 (1.1236)	loss 2.3673 (2.3904)	grad_norm 1.4333 (1.6068)	mem 28740MB
Train: [3/180][4150/10009]	eta 1:49:42 lr 0.049956	data 0.0004 (0.0056)	batch 1.1376 (1.1235)	loss 2.5343 (2.3904)	grad_norm 1.6640 (1.6066)	mem 28740MB
Train: [3/180][4200/10009]	eta 1:48:46 lr 0.049955	data 0.0005 (0.0055)	batch 1.1326 (1.1235)	loss 2.5572 (2.3903)	grad_norm 1.4943 (1.6065)	mem 28740MB
Train: [3/180][4250/10009]	eta 1:47:50 lr 0.049955	data 0.0005 (0.0055)	batch 1.1179 (1.1235)	loss 2.5446 (2.3904)	grad_norm 1.5431 (1.6063)	mem 28740MB
Train: [3/180][4300/10009]	eta 1:46:54 lr 0.049955	data 0.0004 (0.0054)	batch 1.1085 (1.1235)	loss 2.2798 (2.3899)	grad_norm 1.5565 (1.6058)	mem 28740MB
Train: [3/180][4350/10009]	eta 1:45:57 lr 0.049955	data 0.0005 (0.0053)	batch 1.1198 (1.1235)	loss 2.6263 (2.3899)	grad_norm 1.7781 (1.6056)	mem 28740MB
Train: [3/180][4400/10009]	eta 1:45:01 lr 0.049955	data 0.0004 (0.0053)	batch 1.0970 (1.1234)	loss 2.3847 (2.3899)	grad_norm 1.4282 (1.6054)	mem 28740MB
Train: [3/180][4450/10009]	eta 1:44:04 lr 0.049955	data 0.0005 (0.0052)	batch 1.1202 (1.1234)	loss 2.5014 (2.3897)	grad_norm 1.6118 (1.6050)	mem 28740MB
Train: [3/180][4500/10009]	eta 1:43:08 lr 0.049955	data 0.0005 (0.0052)	batch 1.1184 (1.1233)	loss 2.1613 (2.3897)	grad_norm 1.4233 (1.6049)	mem 28740MB
Train: [3/180][4550/10009]	eta 1:42:12 lr 0.049955	data 0.0006 (0.0051)	batch 1.0998 (1.1233)	loss 2.3518 (2.3896)	grad_norm 1.5500 (1.6046)	mem 28740MB
Train: [3/180][4600/10009]	eta 1:41:15 lr 0.049954	data 0.0004 (0.0051)	batch 1.1193 (1.1232)	loss 2.6423 (2.3894)	grad_norm 1.7304 (1.6043)	mem 28740MB
Train: [3/180][4650/10009]	eta 1:40:18 lr 0.049954	data 0.0005 (0.0050)	batch 1.1060 (1.1231)	loss 2.5182 (2.3894)	grad_norm 1.6327 (1.6041)	mem 28740MB
Train: [3/180][4700/10009]	eta 1:39:21 lr 0.049954	data 0.0006 (0.0050)	batch 1.1128 (1.1230)	loss 2.6261 (2.3891)	grad_norm 1.5810 (1.6040)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.5
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.4999999999999998e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 5
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.5
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.4999999999999998e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 5
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.25
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.25e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.2499999999999999e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 5
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 22:45:43 lr 0.050000	data 14.7488 (14.7488)	batch 16.8192 (16.8192)	loss 36.9385 (36.9385)	grad_norm 122.9316 (122.9316)	mem 24443MB
Train: [0/180][50/10009]	eta 1:57:46 lr 0.050000	data 0.0003 (0.2895)	batch 0.3732 (0.7096)	loss 4.6250 (5.5847)	grad_norm 3.8407 (7.3097)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 23:54:31 lr 0.050000	data 14.7577 (14.7577)	batch 17.2317 (17.2317)	loss 36.9385 (36.9385)	grad_norm 122.9316 (122.9316)	mem 24443MB
Train: [0/180][50/10009]	eta 1:58:53 lr 0.050000	data 0.0003 (0.2897)	batch 0.3676 (0.7163)	loss 4.5526 (5.6319)	grad_norm 3.9844 (7.3347)	mem 24443MB
Train: [0/180][100/10009]	eta 1:30:43 lr 0.050000	data 0.0004 (0.1464)	batch 0.3842 (0.5493)	loss 4.1793 (5.0188)	grad_norm 3.6887 (5.6249)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 23:03:39 lr 0.050000	data 14.9623 (14.9623)	batch 16.9267 (16.9267)	loss 36.9385 (36.9385)	grad_norm 122.9316 (122.9316)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 23:15:59 lr 0.050000	data 14.7314 (14.7314)	batch 17.0006 (17.0006)	loss 36.9385 (36.9385)	grad_norm 122.9316 (122.9316)	mem 24443MB
Train: [0/180][50/10009]	eta 1:56:41 lr 0.050000	data 0.0002 (0.2892)	batch 0.3754 (0.7030)	loss 4.5796 (5.6834)	grad_norm 3.7707 (7.1707)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 23:10:31 lr 0.050000	data 14.8385 (14.8385)	batch 16.9679 (16.9679)	loss 36.9385 (36.9385)	grad_norm 122.9310 (122.9310)	mem 24443MB
Train: [0/180][50/10009]	eta 1:55:50 lr 0.050000	data 0.0003 (0.2913)	batch 0.3706 (0.6979)	loss 4.9988 (5.3308)	grad_norm 4.7606 (7.4331)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 22:30:00 lr 0.050000	data 14.6222 (14.6222)	batch 16.7250 (16.7250)	loss 36.9385 (36.9385)	grad_norm 122.9310 (122.9310)	mem 24443MB
Train: [0/180][50/10009]	eta 1:56:19 lr 0.050000	data 0.0004 (0.2870)	batch 0.3721 (0.7008)	loss 4.9724 (5.4157)	grad_norm 4.5855 (7.5587)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 1 day, 23:05:51 lr 0.050000	data 14.6730 (14.6730)	batch 16.9399 (16.9399)	loss 36.9385 (36.9385)	grad_norm 122.9310 (122.9310)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: ''
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/2502]	eta 15:39:43 lr 0.200000	data 16.5876 (16.5876)	batch 22.5354 (22.5354)	loss 6.9401 (6.9401)	grad_norm 0.9547 (0.9547)	mem 55534MB
Train: [0/180][50/2502]	eta 1:40:28 lr 0.200000	data 0.0003 (0.3256)	batch 2.0824 (2.4585)	loss 6.8260 (6.9248)	grad_norm 1.0075 (0.9344)	mem 55534MB
Train: [0/180][100/2502]	eta 1:30:34 lr 0.200000	data 0.0004 (0.1646)	batch 2.0268 (2.2624)	loss 6.5916 (6.8125)	grad_norm 1.0493 (0.9930)	mem 55534MB
Train: [0/180][150/2502]	eta 1:26:07 lr 0.200000	data 0.0003 (0.1102)	batch 2.0819 (2.1970)	loss 6.3200 (6.7076)	grad_norm 0.8996 (1.0017)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: ''
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/2502]	eta 15:20:30 lr 0.200000	data 16.5489 (16.5489)	batch 22.0744 (22.0744)	loss 6.9401 (6.9401)	grad_norm 0.9547 (0.9547)	mem 55534MB
Train: [0/180][50/2502]	eta 1:40:34 lr 0.200000	data 0.0004 (0.3249)	batch 2.0444 (2.4611)	loss 6.8393 (6.9234)	grad_norm 0.9854 (0.9394)	mem 55534MB
Train: [0/180][100/2502]	eta 1:30:33 lr 0.200000	data 0.0003 (0.1642)	batch 2.0738 (2.2622)	loss 6.5782 (6.8098)	grad_norm 1.0489 (1.0013)	mem 55534MB
Train: [0/180][150/2502]	eta 1:26:12 lr 0.200000	data 0.0003 (0.1100)	batch 2.0723 (2.1993)	loss 6.3195 (6.7078)	grad_norm 0.9105 (1.0034)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Test: [0/98]	Time 14.862 (14.862)	Loss 0.6144 (0.6144)	Acc@1 85.742 (85.742)	Acc@5 96.875 (96.875)	Mem 8207MB
Test: [50/98]	Time 0.991 (0.611)	Loss 1.1414 (0.8720)	Acc@1 71.875 (79.511)	Acc@5 92.383 (94.723)	Mem 8208MB
 * Acc@1 76.532 Acc@5 92.984
Accuracy of the teacher network on the 50000 test images: 76.5%
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/2502]	eta 14:40:52 lr 0.200000	data 16.2390 (16.2390)	batch 21.1241 (21.1241)	loss 31.4576 (31.4576)	grad_norm 87.3511 (87.3511)	mem 55575MB
Train: [0/180][50/2502]	eta 1:47:07 lr 0.200000	data 0.0003 (0.3188)	batch 2.3014 (2.6213)	loss 88.1244 (52.1506)	grad_norm 33.1158 (45.9550)	mem 55575MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/2502]	eta 15:12:58 lr 0.200000	data 16.4843 (16.4843)	batch 21.8939 (21.8939)	loss 31.5457 (31.5457)	grad_norm 87.3852 (87.3852)	mem 55534MB
Train: [0/180][50/2502]	eta 1:40:06 lr 0.200000	data 0.0003 (0.3236)	batch 2.0676 (2.4495)	loss 86.0418 (58.3659)	grad_norm 51.5688 (51.0982)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: ''
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Test: [0/98]	Time 14.806 (14.806)	Loss 0.6144 (0.6144)	Acc@1 85.742 (85.742)	Acc@5 96.875 (96.875)	Mem 8207MB
Test: [50/98]	Time 0.866 (0.605)	Loss 1.1414 (0.8720)	Acc@1 71.875 (79.511)	Acc@5 92.383 (94.723)	Mem 8208MB
 * Acc@1 76.532 Acc@5 92.984
Accuracy of the teacher network on the 50000 test images: 76.5%
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/2502]	eta 14:04:25 lr 0.200000	data 16.0056 (16.0056)	batch 20.2499 (20.2499)	loss 6.9334 (6.9334)	grad_norm 0.7443 (0.7443)	mem 55575MB
Train: [0/180][50/2502]	eta 1:46:06 lr 0.200000	data 0.0004 (0.3142)	batch 2.2364 (2.5963)	loss 6.7420 (6.8723)	grad_norm 0.9363 (0.8243)	mem 55575MB
Train: [0/180][100/2502]	eta 1:37:01 lr 0.200000	data 0.0003 (0.1588)	batch 2.3029 (2.4234)	loss 6.4340 (6.7129)	grad_norm 1.0057 (0.9395)	mem 55575MB
Train: [0/180][150/2502]	eta 1:32:36 lr 0.200000	data 0.0003 (0.1064)	batch 2.2472 (2.3624)	loss 6.1838 (6.5803)	grad_norm 0.9503 (0.9728)	mem 55575MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.2
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 2
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: ''
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.1
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 1 day, 3:07:56 lr 0.100000	data 15.9598 (15.9598)	batch 19.5197 (19.5197)	loss 6.9619 (6.9619)	grad_norm 1.3009 (1.3009)	mem 55534MB
Train: [0/180][50/5004]	eta 1:51:24 lr 0.100000	data 0.0004 (0.3133)	batch 0.9827 (1.3492)	loss 6.9374 (6.9780)	grad_norm 1.2348 (1.2814)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.1
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10002
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][100/5004]	eta 1:35:05 lr 0.100000	data 0.0004 (0.1584)	batch 0.9834 (1.1635)	loss 6.8511 (6.9453)	grad_norm 1.3324 (1.2851)	mem 55534MB
Train: [0/180][0/5004]	eta 1 day, 3:00:01 lr 0.100000	data 15.9122 (15.9122)	batch 19.4247 (19.4247)	loss 27.0408 (27.0408)	grad_norm 71.8521 (71.8521)	mem 55534MB
Train: [0/180][150/5004]	eta 1:29:06 lr 0.100000	data 0.0004 (0.1061)	batch 0.9540 (1.1015)	loss 6.6881 (6.8926)	grad_norm 1.2950 (1.3035)	mem 55534MB
Train: [0/180][50/5004]	eta 1:51:33 lr 0.100000	data 0.0004 (0.3124)	batch 1.0037 (1.3511)	loss 34.2554 (22.4824)	grad_norm 41.2359 (33.8811)	mem 55534MB
Train: [0/180][200/5004]	eta 1:25:40 lr 0.100000	data 0.0004 (0.0798)	batch 0.9703 (1.0701)	loss 6.6553 (6.8418)	grad_norm 1.2938 (1.3099)	mem 55534MB
Train: [0/180][100/5004]	eta 1:35:16 lr 0.100000	data 0.0004 (0.1579)	batch 0.9859 (1.1657)	loss 41.1032 (31.3250)	grad_norm 86.3668 (40.8507)	mem 55534MB
Train: [0/180][250/5004]	eta 1:23:18 lr 0.100000	data 0.0004 (0.0640)	batch 0.9839 (1.0513)	loss 6.5236 (6.7908)	grad_norm 1.2990 (1.3112)	mem 55534MB
Train: [0/180][150/5004]	eta 1:29:17 lr 0.100000	data 0.0004 (0.1059)	batch 1.0656 (1.1038)	loss 70.1004 (41.0416)	grad_norm 55.1861 (44.0927)	mem 55534MB
Train: [0/180][300/5004]	eta 1:21:26 lr 0.100000	data 0.0004 (0.0534)	batch 0.9604 (1.0388)	loss 6.5115 (6.7420)	grad_norm 1.3453 (1.3152)	mem 55534MB
Train: [0/180][200/5004]	eta 1:25:54 lr 0.100000	data 0.0004 (0.0796)	batch 0.9836 (1.0729)	loss 78.2642 (49.6818)	grad_norm 32.7172 (47.1435)	mem 55534MB
Train: [0/180][350/5004]	eta 1:19:54 lr 0.100000	data 0.0003 (0.0458)	batch 0.9757 (1.0301)	loss 6.3991 (6.6967)	grad_norm 1.2959 (1.3143)	mem 55534MB
Train: [0/180][250/5004]	eta 1:23:30 lr 0.100000	data 0.0003 (0.0638)	batch 0.9682 (1.0539)	loss 79.5968 (54.7374)	grad_norm 32.9056 (47.8240)	mem 55534MB
Train: [0/180][400/5004]	eta 1:18:33 lr 0.100000	data 0.0003 (0.0402)	batch 0.9735 (1.0238)	loss 6.3221 (6.6526)	grad_norm 1.2880 (1.3122)	mem 55534MB
Train: [0/180][300/5004]	eta 1:21:38 lr 0.100000	data 0.0004 (0.0533)	batch 0.9656 (1.0414)	loss 76.2676 (57.6766)	grad_norm 30.1115 (48.4336)	mem 55534MB
Train: [0/180][450/5004]	eta 1:17:20 lr 0.100000	data 0.0004 (0.0358)	batch 0.9818 (1.0189)	loss 6.2261 (6.6093)	grad_norm 1.2234 (1.3082)	mem 55534MB
Train: [0/180][350/5004]	eta 1:20:07 lr 0.100000	data 0.0004 (0.0457)	batch 1.0010 (1.0330)	loss 62.8108 (59.9178)	grad_norm 94.6123 (48.3793)	mem 55534MB
Train: [0/180][500/5004]	eta 1:16:10 lr 0.100000	data 0.0003 (0.0322)	batch 0.9804 (1.0149)	loss 6.1183 (6.5674)	grad_norm 1.2538 (1.3041)	mem 55534MB
Train: [0/180][400/5004]	eta 1:18:44 lr 0.100000	data 0.0004 (0.0401)	batch 0.9830 (1.0261)	loss 75.8169 (61.9063)	grad_norm 39.2474 (48.2210)	mem 55534MB
Train: [0/180][550/5004]	eta 1:15:05 lr 0.100000	data 0.0003 (0.0293)	batch 0.9566 (1.0116)	loss 6.0479 (6.5290)	grad_norm 1.2595 (1.3005)	mem 55534MB
Train: [0/180][450/5004]	eta 1:17:30 lr 0.100000	data 0.0003 (0.0357)	batch 0.9858 (1.0211)	loss 73.8471 (63.0088)	grad_norm 25.9999 (48.0245)	mem 55534MB
Train: [0/180][600/5004]	eta 1:14:04 lr 0.100000	data 0.0003 (0.0269)	batch 0.9519 (1.0091)	loss 6.1297 (6.4915)	grad_norm 1.2441 (1.2986)	mem 55534MB
Train: [0/180][500/5004]	eta 1:16:20 lr 0.100000	data 0.0004 (0.0322)	batch 0.9904 (1.0170)	loss 59.2055 (63.3293)	grad_norm 53.9885 (47.6746)	mem 55534MB
Train: [0/180][650/5004]	eta 1:13:03 lr 0.100000	data 0.0003 (0.0249)	batch 0.9754 (1.0067)	loss 6.0899 (6.4554)	grad_norm 1.2701 (1.2957)	mem 55534MB
Train: [0/180][550/5004]	eta 1:15:14 lr 0.100000	data 0.0004 (0.0293)	batch 0.9630 (1.0136)	loss 61.8126 (63.3726)	grad_norm 31.7234 (47.0604)	mem 55534MB
Train: [0/180][700/5004]	eta 1:12:04 lr 0.100000	data 0.0004 (0.0231)	batch 0.9816 (1.0048)	loss 6.0212 (6.4210)	grad_norm 1.2167 (1.2939)	mem 55534MB
Train: [0/180][600/5004]	eta 1:14:11 lr 0.100000	data 0.0004 (0.0269)	batch 0.9640 (1.0109)	loss 46.4443 (63.1178)	grad_norm 55.5685 (46.2169)	mem 55534MB
Train: [0/180][750/5004]	eta 1:11:07 lr 0.100000	data 0.0003 (0.0216)	batch 0.9955 (1.0031)	loss 5.9900 (6.3896)	grad_norm 1.2405 (1.2914)	mem 55534MB
Train: [0/180][650/5004]	eta 1:13:11 lr 0.100000	data 0.0003 (0.0248)	batch 0.9994 (1.0086)	loss 63.1421 (62.8432)	grad_norm 20.2414 (45.5406)	mem 55534MB
Train: [0/180][800/5004]	eta 1:10:11 lr 0.100000	data 0.0003 (0.0203)	batch 0.9862 (1.0017)	loss 5.7916 (6.3578)	grad_norm 1.2485 (1.2896)	mem 55534MB
Train: [0/180][700/5004]	eta 1:12:11 lr 0.100000	data 0.0002 (0.0231)	batch 0.9698 (1.0064)	loss 57.5933 (62.1039)	grad_norm 23.6743 (44.7469)	mem 55534MB
Train: [0/180][850/5004]	eta 1:09:15 lr 0.100000	data 0.0004 (0.0191)	batch 0.9703 (1.0003)	loss 5.8230 (6.3279)	grad_norm 1.2367 (1.2881)	mem 55534MB
Train: [0/180][750/5004]	eta 1:11:14 lr 0.100000	data 0.0003 (0.0216)	batch 1.0088 (1.0048)	loss 51.0203 (61.3636)	grad_norm 38.4959 (44.1525)	mem 55534MB
Train: [0/180][900/5004]	eta 1:08:20 lr 0.100000	data 0.0004 (0.0181)	batch 1.0000 (0.9992)	loss 5.7021 (6.2983)	grad_norm 1.2404 (1.2868)	mem 55534MB
Train: [0/180][800/5004]	eta 1:10:17 lr 0.100000	data 0.0003 (0.0202)	batch 0.9602 (1.0032)	loss 59.1164 (60.7658)	grad_norm 22.3549 (43.6905)	mem 55534MB
Train: [0/180][950/5004]	eta 1:07:27 lr 0.100000	data 0.0003 (0.0171)	batch 0.9867 (0.9983)	loss 5.9063 (6.2702)	grad_norm 1.2761 (1.2864)	mem 55534MB
Train: [0/180][850/5004]	eta 1:09:21 lr 0.100000	data 0.0004 (0.0191)	batch 0.9901 (1.0018)	loss 51.7307 (60.1472)	grad_norm 61.4704 (43.0487)	mem 55534MB
Train: [0/180][1000/5004]	eta 1:06:33 lr 0.100000	data 0.0004 (0.0163)	batch 0.9656 (0.9974)	loss 5.8558 (6.2412)	grad_norm 1.3149 (1.2856)	mem 55534MB
Train: [0/180][900/5004]	eta 1:08:26 lr 0.100000	data 0.0003 (0.0180)	batch 0.9894 (1.0007)	loss 49.8351 (59.4858)	grad_norm 28.4806 (42.5638)	mem 55534MB
Train: [0/180][1050/5004]	eta 1:05:40 lr 0.100000	data 0.0004 (0.0155)	batch 0.9933 (0.9965)	loss 5.6075 (6.2138)	grad_norm 1.2956 (1.2853)	mem 55534MB
Train: [0/180][950/5004]	eta 1:07:32 lr 0.100000	data 0.0003 (0.0171)	batch 0.9802 (0.9997)	loss 42.4748 (58.8759)	grad_norm 25.9651 (41.9874)	mem 55534MB
Train: [0/180][1100/5004]	eta 1:04:47 lr 0.100000	data 0.0004 (0.0149)	batch 0.9918 (0.9958)	loss 5.5826 (6.1878)	grad_norm 1.3139 (1.2851)	mem 55534MB
Train: [0/180][1000/5004]	eta 1:06:38 lr 0.100000	data 0.0003 (0.0163)	batch 0.9694 (0.9985)	loss 53.8127 (58.3170)	grad_norm 27.6398 (41.6540)	mem 55534MB
Train: [0/180][1150/5004]	eta 1:03:55 lr 0.100000	data 0.0004 (0.0142)	batch 0.9817 (0.9953)	loss 5.4173 (6.1625)	grad_norm 1.3102 (1.2847)	mem 55534MB
Train: [0/180][1050/5004]	eta 1:05:44 lr 0.100000	data 0.0004 (0.0155)	batch 0.9742 (0.9975)	loss 52.1852 (57.8708)	grad_norm 28.8924 (41.3159)	mem 55534MB
Train: [0/180][1200/5004]	eta 1:03:03 lr 0.100000	data 0.0005 (0.0137)	batch 0.9631 (0.9946)	loss 5.4692 (6.1374)	grad_norm 1.2222 (1.2848)	mem 55534MB
Train: [0/180][1100/5004]	eta 1:04:50 lr 0.100000	data 0.0004 (0.0148)	batch 1.0004 (0.9965)	loss 41.1232 (57.3802)	grad_norm 34.6368 (40.8995)	mem 55534MB
Train: [0/180][1250/5004]	eta 1:02:11 lr 0.100000	data 0.0004 (0.0131)	batch 0.9777 (0.9941)	loss 5.4380 (6.1130)	grad_norm 1.3820 (1.2844)	mem 55534MB
Train: [0/180][1150/5004]	eta 1:03:57 lr 0.100000	data 0.0004 (0.0142)	batch 0.9858 (0.9957)	loss 41.0511 (56.8091)	grad_norm 20.6236 (40.4216)	mem 55534MB
Train: [0/180][1300/5004]	eta 1:01:20 lr 0.099999	data 0.0003 (0.0126)	batch 0.9692 (0.9936)	loss 5.5990 (6.0891)	grad_norm 1.2779 (1.2844)	mem 55534MB
Train: [0/180][1200/5004]	eta 1:03:05 lr 0.100000	data 0.0005 (0.0136)	batch 0.9731 (0.9953)	loss 43.7267 (56.2694)	grad_norm 14.1076 (39.9847)	mem 55534MB
Train: [0/180][1350/5004]	eta 1:00:28 lr 0.099999	data 0.0003 (0.0122)	batch 1.0051 (0.9931)	loss 5.5393 (6.0656)	grad_norm 1.2929 (1.2845)	mem 55534MB
Train: [0/180][1250/5004]	eta 1:02:13 lr 0.100000	data 0.0003 (0.0131)	batch 0.9641 (0.9946)	loss 40.0453 (55.7246)	grad_norm 38.9178 (39.5313)	mem 55534MB
Train: [0/180][1400/5004]	eta 0:59:37 lr 0.099999	data 0.0004 (0.0118)	batch 0.9683 (0.9927)	loss 5.5246 (6.0423)	grad_norm 1.2698 (1.2848)	mem 55534MB
Train: [0/180][1300/5004]	eta 1:01:21 lr 0.099999	data 0.0005 (0.0126)	batch 0.9680 (0.9938)	loss 35.8272 (55.1925)	grad_norm 28.3046 (39.1383)	mem 55534MB
Train: [0/180][1450/5004]	eta 0:58:46 lr 0.099999	data 0.0004 (0.0114)	batch 0.9938 (0.9923)	loss 5.2983 (6.0199)	grad_norm 1.2881 (1.2848)	mem 55534MB
Train: [0/180][1350/5004]	eta 1:00:29 lr 0.099999	data 0.0002 (0.0122)	batch 0.9876 (0.9934)	loss 43.5424 (54.6177)	grad_norm 22.1614 (38.7073)	mem 55534MB
Train: [0/180][1500/5004]	eta 0:57:55 lr 0.099999	data 0.0004 (0.0110)	batch 0.9881 (0.9918)	loss 5.2734 (5.9976)	grad_norm 1.2940 (1.2851)	mem 55534MB
Train: [0/180][1400/5004]	eta 0:59:38 lr 0.099999	data 0.0003 (0.0117)	batch 1.0088 (0.9929)	loss 39.1248 (54.0100)	grad_norm 33.7949 (38.1859)	mem 55534MB
Train: [0/180][1550/5004]	eta 0:57:04 lr 0.099999	data 0.0004 (0.0107)	batch 0.9738 (0.9914)	loss 5.3480 (5.9764)	grad_norm 1.2868 (1.2852)	mem 55534MB
Train: [0/180][1450/5004]	eta 0:58:47 lr 0.099999	data 0.0004 (0.0113)	batch 0.9595 (0.9925)	loss 36.3572 (53.3504)	grad_norm 15.9080 (37.6698)	mem 55534MB
Train: [0/180][1600/5004]	eta 0:56:13 lr 0.099999	data 0.0004 (0.0103)	batch 1.0062 (0.9910)	loss 5.3061 (5.9553)	grad_norm 1.3207 (1.2856)	mem 55534MB
Train: [0/180][1500/5004]	eta 0:57:56 lr 0.099999	data 0.0004 (0.0110)	batch 0.9585 (0.9920)	loss 33.6121 (52.7397)	grad_norm 33.6960 (37.2458)	mem 55534MB
Train: [0/180][1650/5004]	eta 0:55:22 lr 0.099999	data 0.0003 (0.0100)	batch 0.9722 (0.9906)	loss 5.2187 (5.9338)	grad_norm 1.2904 (1.2856)	mem 55534MB
Train: [0/180][1550/5004]	eta 0:57:05 lr 0.099999	data 0.0004 (0.0106)	batch 1.0732 (0.9918)	loss 31.1765 (52.1771)	grad_norm 14.2868 (36.8268)	mem 55534MB
Train: [0/180][1700/5004]	eta 0:54:31 lr 0.099999	data 0.0003 (0.0097)	batch 0.9780 (0.9903)	loss 5.1904 (5.9131)	grad_norm 1.2564 (1.2861)	mem 55534MB
Train: [0/180][1600/5004]	eta 0:56:14 lr 0.099999	data 0.0003 (0.0103)	batch 0.9932 (0.9913)	loss 32.7934 (51.6078)	grad_norm 11.3894 (36.3788)	mem 55534MB
Train: [0/180][1750/5004]	eta 0:53:41 lr 0.099999	data 0.0004 (0.0095)	batch 0.9760 (0.9900)	loss 5.1304 (5.8935)	grad_norm 1.2534 (1.2863)	mem 55534MB
Train: [0/180][1650/5004]	eta 0:55:23 lr 0.099999	data 0.0004 (0.0100)	batch 0.9859 (0.9910)	loss 31.6991 (51.0396)	grad_norm 14.3224 (35.9507)	mem 55534MB
Train: [0/180][1800/5004]	eta 0:52:51 lr 0.099999	data 0.0004 (0.0092)	batch 0.9943 (0.9897)	loss 5.0999 (5.8741)	grad_norm 1.2996 (1.2866)	mem 55534MB
Train: [0/180][1700/5004]	eta 0:54:33 lr 0.099999	data 0.0004 (0.0097)	batch 0.9963 (0.9908)	loss 37.1037 (50.4845)	grad_norm 50.2629 (35.5220)	mem 55534MB
Train: [0/180][1850/5004]	eta 0:52:01 lr 0.099999	data 0.0005 (0.0090)	batch 0.9721 (0.9896)	loss 5.2025 (5.8546)	grad_norm 1.3321 (1.2867)	mem 55534MB
Train: [0/180][1750/5004]	eta 0:53:42 lr 0.099999	data 0.0003 (0.0095)	batch 0.9826 (0.9905)	loss 31.3526 (49.9449)	grad_norm 16.8550 (35.0973)	mem 55534MB
Train: [0/180][1900/5004]	eta 0:51:10 lr 0.099999	data 0.0003 (0.0088)	batch 0.9575 (0.9893)	loss 5.0594 (5.8354)	grad_norm 1.3142 (1.2870)	mem 55534MB
Train: [0/180][1800/5004]	eta 0:52:52 lr 0.099999	data 0.0003 (0.0092)	batch 0.9959 (0.9902)	loss 29.8504 (49.3854)	grad_norm 20.6061 (34.6806)	mem 55534MB
Train: [0/180][1950/5004]	eta 0:50:20 lr 0.099999	data 0.0003 (0.0086)	batch 1.0467 (0.9891)	loss 5.0711 (5.8163)	grad_norm 1.2978 (1.2873)	mem 55534MB
Train: [0/180][1850/5004]	eta 0:52:02 lr 0.099999	data 0.0005 (0.0090)	batch 0.9906 (0.9900)	loss 28.0194 (48.8393)	grad_norm 19.4959 (34.2896)	mem 55534MB
Train: [0/180][2000/5004]	eta 0:49:30 lr 0.099999	data 0.0003 (0.0083)	batch 0.9773 (0.9888)	loss 5.0627 (5.7982)	grad_norm 1.2503 (1.2877)	mem 55534MB
Train: [0/180][1900/5004]	eta 0:51:12 lr 0.099999	data 0.0003 (0.0088)	batch 0.9539 (0.9898)	loss 26.8187 (48.2960)	grad_norm 26.0708 (33.8764)	mem 55534MB
Train: [0/180][2050/5004]	eta 0:48:40 lr 0.099999	data 0.0004 (0.0082)	batch 0.9765 (0.9886)	loss 5.1169 (5.7799)	grad_norm 1.2968 (1.2882)	mem 55534MB
Train: [0/180][1950/5004]	eta 0:50:21 lr 0.099999	data 0.0003 (0.0085)	batch 0.9750 (0.9895)	loss 23.0710 (47.7642)	grad_norm 11.0034 (33.4850)	mem 55534MB
Train: [0/180][2100/5004]	eta 0:47:50 lr 0.099999	data 0.0003 (0.0080)	batch 0.9939 (0.9885)	loss 4.9725 (5.7618)	grad_norm 1.2758 (1.2885)	mem 55534MB
Train: [0/180][2000/5004]	eta 0:49:31 lr 0.099999	data 0.0004 (0.0083)	batch 0.9598 (0.9893)	loss 28.3560 (47.2329)	grad_norm 17.1240 (33.1135)	mem 55534MB
Train: [0/180][2150/5004]	eta 0:47:00 lr 0.099999	data 0.0004 (0.0078)	batch 0.9701 (0.9883)	loss 5.1526 (5.7438)	grad_norm 1.3067 (1.2889)	mem 55534MB
Train: [0/180][2050/5004]	eta 0:48:41 lr 0.099999	data 0.0004 (0.0081)	batch 0.9594 (0.9891)	loss 25.6023 (46.6911)	grad_norm 9.1418 (32.7132)	mem 55534MB
Train: [0/180][2200/5004]	eta 0:46:10 lr 0.099999	data 0.0004 (0.0076)	batch 0.9669 (0.9881)	loss 5.0319 (5.7263)	grad_norm 1.2858 (1.2895)	mem 55534MB
Train: [0/180][2100/5004]	eta 0:47:51 lr 0.099999	data 0.0004 (0.0080)	batch 0.9614 (0.9889)	loss 27.6786 (46.1848)	grad_norm 12.8786 (32.3400)	mem 55534MB
Train: [0/180][2250/5004]	eta 0:45:20 lr 0.099998	data 0.0004 (0.0075)	batch 0.9629 (0.9879)	loss 4.9766 (5.7088)	grad_norm 1.3134 (1.2897)	mem 55534MB
Train: [0/180][2150/5004]	eta 0:47:01 lr 0.099999	data 0.0003 (0.0078)	batch 0.9948 (0.9887)	loss 25.5740 (45.6586)	grad_norm 26.3765 (32.0011)	mem 55534MB
Train: [0/180][2300/5004]	eta 0:44:31 lr 0.099998	data 0.0004 (0.0073)	batch 0.9997 (0.9878)	loss 4.8615 (5.6914)	grad_norm 1.2650 (1.2899)	mem 55534MB
Train: [0/180][2200/5004]	eta 0:46:11 lr 0.099999	data 0.0004 (0.0076)	batch 1.0035 (0.9885)	loss 23.7700 (45.1372)	grad_norm 26.4502 (31.6138)	mem 55534MB
Train: [0/180][2350/5004]	eta 0:43:41 lr 0.099998	data 0.0002 (0.0072)	batch 0.9863 (0.9877)	loss 4.7216 (5.6746)	grad_norm 1.3033 (1.2903)	mem 55534MB
Train: [0/180][2250/5004]	eta 0:45:21 lr 0.099998	data 0.0002 (0.0075)	batch 0.9572 (0.9883)	loss 20.9670 (44.6292)	grad_norm 13.8670 (31.2662)	mem 55534MB
Train: [0/180][2400/5004]	eta 0:42:51 lr 0.099998	data 0.0003 (0.0070)	batch 0.9906 (0.9876)	loss 4.8970 (5.6574)	grad_norm 1.3412 (1.2908)	mem 55534MB
Train: [0/180][2300/5004]	eta 0:44:31 lr 0.099998	data 0.0003 (0.0073)	batch 0.9727 (0.9881)	loss 23.9902 (44.1328)	grad_norm 10.8684 (30.8771)	mem 55534MB
Train: [0/180][2450/5004]	eta 0:42:01 lr 0.099998	data 0.0004 (0.0069)	batch 0.9865 (0.9874)	loss 4.8517 (5.6402)	grad_norm 1.2933 (1.2911)	mem 55534MB
Train: [0/180][2350/5004]	eta 0:43:42 lr 0.099998	data 0.0006 (0.0072)	batch 1.0202 (0.9880)	loss 17.3692 (43.6061)	grad_norm 8.0031 (30.5125)	mem 55534MB
Train: [0/180][2500/5004]	eta 0:41:12 lr 0.099998	data 0.0003 (0.0068)	batch 1.0060 (0.9873)	loss 4.8956 (5.6234)	grad_norm 1.3325 (1.2914)	mem 55534MB
Train: [0/180][2400/5004]	eta 0:42:52 lr 0.099998	data 0.0004 (0.0070)	batch 0.9815 (0.9878)	loss 17.5639 (43.1123)	grad_norm 10.4540 (30.1665)	mem 55534MB
Train: [0/180][2550/5004]	eta 0:40:22 lr 0.099998	data 0.0004 (0.0066)	batch 0.9591 (0.9871)	loss 4.6216 (5.6064)	grad_norm 1.3302 (1.2917)	mem 55534MB
Train: [0/180][2450/5004]	eta 0:42:02 lr 0.099998	data 0.0004 (0.0069)	batch 0.9960 (0.9876)	loss 18.3913 (42.6205)	grad_norm 9.2210 (29.8267)	mem 55534MB
Train: [0/180][2600/5004]	eta 0:39:32 lr 0.099998	data 0.0003 (0.0065)	batch 0.9869 (0.9870)	loss 4.7778 (5.5901)	grad_norm 1.3488 (1.2922)	mem 55534MB
Train: [0/180][2500/5004]	eta 0:41:12 lr 0.099998	data 0.0003 (0.0067)	batch 0.9803 (0.9875)	loss 17.7832 (42.1363)	grad_norm 14.9142 (29.4696)	mem 55534MB
Train: [0/180][2650/5004]	eta 0:38:43 lr 0.099998	data 0.0005 (0.0064)	batch 0.9913 (0.9868)	loss 4.7412 (5.5743)	grad_norm 1.3114 (1.2926)	mem 55534MB
Train: [0/180][2550/5004]	eta 0:40:22 lr 0.099998	data 0.0004 (0.0066)	batch 0.9694 (0.9874)	loss 14.5717 (41.6636)	grad_norm 10.4748 (29.1205)	mem 55534MB
Train: [0/180][2700/5004]	eta 0:37:53 lr 0.099998	data 0.0004 (0.0063)	batch 0.9542 (0.9867)	loss 4.7967 (5.5582)	grad_norm 1.3168 (1.2929)	mem 55534MB
Train: [0/180][2600/5004]	eta 0:39:33 lr 0.099998	data 0.0003 (0.0065)	batch 1.0028 (0.9872)	loss 14.5854 (41.1907)	grad_norm 10.4621 (28.7756)	mem 55534MB
Train: [0/180][2750/5004]	eta 0:37:03 lr 0.099998	data 0.0004 (0.0062)	batch 0.9662 (0.9866)	loss 4.6423 (5.5423)	grad_norm 1.3366 (1.2935)	mem 55534MB
Train: [0/180][2650/5004]	eta 0:38:43 lr 0.099998	data 0.0004 (0.0064)	batch 0.9823 (0.9871)	loss 15.3923 (40.7181)	grad_norm 11.3608 (28.4286)	mem 55534MB
Train: [0/180][2800/5004]	eta 0:36:14 lr 0.099998	data 0.0004 (0.0061)	batch 0.9896 (0.9864)	loss 4.6965 (5.5263)	grad_norm 1.3160 (1.2940)	mem 55534MB
Train: [0/180][2700/5004]	eta 0:37:53 lr 0.099998	data 0.0003 (0.0063)	batch 0.9896 (0.9869)	loss 15.8329 (40.2522)	grad_norm 9.5117 (28.0915)	mem 55534MB
Train: [0/180][2850/5004]	eta 0:35:24 lr 0.099998	data 0.0004 (0.0060)	batch 1.0205 (0.9863)	loss 4.6371 (5.5112)	grad_norm 1.3449 (1.2944)	mem 55534MB
Train: [0/180][2750/5004]	eta 0:37:04 lr 0.099998	data 0.0004 (0.0062)	batch 0.9916 (0.9868)	loss 14.2905 (39.7909)	grad_norm 6.8449 (27.7515)	mem 55534MB
Train: [0/180][2900/5004]	eta 0:34:35 lr 0.099997	data 0.0003 (0.0059)	batch 1.0124 (0.9863)	loss 4.5418 (5.4960)	grad_norm 1.2652 (1.2948)	mem 55534MB
Train: [0/180][2800/5004]	eta 0:36:14 lr 0.099998	data 0.0003 (0.0061)	batch 0.9819 (0.9867)	loss 15.7727 (39.3305)	grad_norm 9.4104 (27.4221)	mem 55534MB
Train: [0/180][2950/5004]	eta 0:33:45 lr 0.099997	data 0.0004 (0.0058)	batch 0.9859 (0.9861)	loss 4.5974 (5.4809)	grad_norm 1.3432 (1.2949)	mem 55534MB
Train: [0/180][2850/5004]	eta 0:35:24 lr 0.099998	data 0.0003 (0.0060)	batch 0.9700 (0.9865)	loss 13.7813 (38.8814)	grad_norm 7.5180 (27.1020)	mem 55534MB
Train: [0/180][3000/5004]	eta 0:32:56 lr 0.099997	data 0.0004 (0.0057)	batch 0.9887 (0.9861)	loss 4.5411 (5.4657)	grad_norm 1.3095 (1.2952)	mem 55534MB
Train: [0/180][2900/5004]	eta 0:34:35 lr 0.099997	data 0.0003 (0.0059)	batch 0.9689 (0.9864)	loss 12.8836 (38.4292)	grad_norm 11.0993 (26.7710)	mem 55534MB
Train: [0/180][3050/5004]	eta 0:32:06 lr 0.099997	data 0.0003 (0.0056)	batch 0.9617 (0.9860)	loss 4.5860 (5.4509)	grad_norm 1.3083 (1.2956)	mem 55534MB
Train: [0/180][2950/5004]	eta 0:33:45 lr 0.099997	data 0.0003 (0.0058)	batch 0.9829 (0.9863)	loss 11.7733 (37.9824)	grad_norm 6.9073 (26.4431)	mem 55534MB
Train: [0/180][3100/5004]	eta 0:31:17 lr 0.099997	data 0.0003 (0.0055)	batch 1.0061 (0.9859)	loss 4.4841 (5.4356)	grad_norm 1.3051 (1.2958)	mem 55534MB
Train: [0/180][3000/5004]	eta 0:32:56 lr 0.099997	data 0.0004 (0.0057)	batch 0.9663 (0.9862)	loss 11.9982 (37.5426)	grad_norm 5.9588 (26.1294)	mem 55534MB
Train: [0/180][3150/5004]	eta 0:30:27 lr 0.099997	data 0.0003 (0.0054)	batch 0.9727 (0.9859)	loss 4.4392 (5.4206)	grad_norm 1.2772 (1.2960)	mem 55534MB
Train: [0/180][3050/5004]	eta 0:32:06 lr 0.099997	data 0.0003 (0.0056)	batch 0.9734 (0.9861)	loss 10.2927 (37.1092)	grad_norm 5.1353 (25.8114)	mem 55534MB
Train: [0/180][3200/5004]	eta 0:29:38 lr 0.099997	data 0.0003 (0.0054)	batch 0.9803 (0.9857)	loss 4.5701 (5.4059)	grad_norm 1.3282 (1.2963)	mem 55534MB
Train: [0/180][3100/5004]	eta 0:31:17 lr 0.099997	data 0.0004 (0.0055)	batch 1.0072 (0.9860)	loss 10.9117 (36.6832)	grad_norm 4.4890 (25.5011)	mem 55534MB
Train: [0/180][3250/5004]	eta 0:28:48 lr 0.099997	data 0.0003 (0.0053)	batch 0.9580 (0.9856)	loss 4.4013 (5.3916)	grad_norm 1.3083 (1.2967)	mem 55534MB
Train: [0/180][3150/5004]	eta 0:30:28 lr 0.099997	data 0.0003 (0.0054)	batch 0.9816 (0.9860)	loss 9.4543 (36.2572)	grad_norm 4.9384 (25.1857)	mem 55534MB
Train: [0/180][3300/5004]	eta 0:27:59 lr 0.099997	data 0.0005 (0.0052)	batch 0.9954 (0.9855)	loss 4.2776 (5.3770)	grad_norm 1.3167 (1.2968)	mem 55534MB
Train: [0/180][3200/5004]	eta 0:29:38 lr 0.099997	data 0.0004 (0.0053)	batch 0.9800 (0.9859)	loss 9.0197 (35.8402)	grad_norm 5.4741 (24.8753)	mem 55534MB
Train: [0/180][3350/5004]	eta 0:27:09 lr 0.099997	data 0.0004 (0.0051)	batch 0.9835 (0.9855)	loss 4.5249 (5.3625)	grad_norm 1.3285 (1.2970)	mem 55534MB
Train: [0/180][3250/5004]	eta 0:28:49 lr 0.099997	data 0.0004 (0.0053)	batch 0.9712 (0.9858)	loss 8.4203 (35.4221)	grad_norm 3.6556 (24.5645)	mem 55534MB
Train: [0/180][3400/5004]	eta 0:26:20 lr 0.099996	data 0.0004 (0.0051)	batch 0.9796 (0.9854)	loss 4.5003 (5.3481)	grad_norm 1.3384 (1.2971)	mem 55534MB
Train: [0/180][3300/5004]	eta 0:27:59 lr 0.099997	data 0.0002 (0.0052)	batch 0.9893 (0.9857)	loss 7.8524 (35.0049)	grad_norm 3.5023 (24.2547)	mem 55534MB
Train: [0/180][3450/5004]	eta 0:25:31 lr 0.099996	data 0.0003 (0.0050)	batch 1.0068 (0.9853)	loss 4.3128 (5.3344)	grad_norm 1.2578 (1.2971)	mem 55534MB
Train: [0/180][3350/5004]	eta 0:27:10 lr 0.099997	data 0.0003 (0.0051)	batch 0.9972 (0.9856)	loss 7.4433 (34.5933)	grad_norm 3.7647 (23.9506)	mem 55534MB
Train: [0/180][3500/5004]	eta 0:24:41 lr 0.099996	data 0.0004 (0.0049)	batch 1.0100 (0.9853)	loss 4.6453 (5.3206)	grad_norm 1.3182 (1.2970)	mem 55534MB
Train: [0/180][3400/5004]	eta 0:26:20 lr 0.099996	data 0.0003 (0.0051)	batch 0.9943 (0.9855)	loss 7.4907 (34.1899)	grad_norm 3.4582 (23.6521)	mem 55534MB
Train: [0/180][3550/5004]	eta 0:23:52 lr 0.099996	data 0.0004 (0.0049)	batch 0.9922 (0.9853)	loss 4.3910 (5.3070)	grad_norm 1.2395 (1.2972)	mem 55534MB
Train: [0/180][3450/5004]	eta 0:25:31 lr 0.099996	data 0.0003 (0.0050)	batch 1.0110 (0.9855)	loss 7.0463 (33.7945)	grad_norm 3.1117 (23.3604)	mem 55534MB
Train: [0/180][3600/5004]	eta 0:23:03 lr 0.099996	data 0.0003 (0.0048)	batch 0.9622 (0.9852)	loss 4.2749 (5.2932)	grad_norm 1.2934 (1.2973)	mem 55534MB
Train: [0/180][3500/5004]	eta 0:24:42 lr 0.099996	data 0.0005 (0.0049)	batch 0.9894 (0.9854)	loss 6.5148 (33.4059)	grad_norm 3.0029 (23.0716)	mem 55534MB
Train: [0/180][3650/5004]	eta 0:22:13 lr 0.099996	data 0.0003 (0.0047)	batch 0.9795 (0.9851)	loss 4.2466 (5.2796)	grad_norm 1.3051 (1.2975)	mem 55534MB
Train: [0/180][3550/5004]	eta 0:23:52 lr 0.099996	data 0.0003 (0.0049)	batch 0.9696 (0.9854)	loss 6.6735 (33.0257)	grad_norm 3.1456 (22.7899)	mem 55534MB
Train: [0/180][3700/5004]	eta 0:21:24 lr 0.099996	data 0.0003 (0.0047)	batch 0.9812 (0.9851)	loss 4.2403 (5.2661)	grad_norm 1.3302 (1.2977)	mem 55534MB
Train: [0/180][3600/5004]	eta 0:23:03 lr 0.099996	data 0.0003 (0.0048)	batch 0.9656 (0.9853)	loss 5.9745 (32.6547)	grad_norm 2.5945 (22.5150)	mem 55534MB
Train: [0/180][3750/5004]	eta 0:20:35 lr 0.099996	data 0.0003 (0.0046)	batch 0.9489 (0.9850)	loss 4.2880 (5.2525)	grad_norm 1.3571 (1.2979)	mem 55534MB
Train: [0/180][3650/5004]	eta 0:22:14 lr 0.099996	data 0.0004 (0.0047)	batch 0.9800 (0.9853)	loss 6.2727 (32.2917)	grad_norm 3.1045 (22.2463)	mem 55534MB
Train: [0/180][3800/5004]	eta 0:19:45 lr 0.099996	data 0.0004 (0.0046)	batch 0.9891 (0.9849)	loss 4.1449 (5.2391)	grad_norm 1.2554 (1.2980)	mem 55534MB
Train: [0/180][3700/5004]	eta 0:21:24 lr 0.099996	data 0.0003 (0.0047)	batch 0.9600 (0.9853)	loss 5.9845 (31.9372)	grad_norm 2.6265 (21.9831)	mem 55534MB
Train: [0/180][3850/5004]	eta 0:18:56 lr 0.099995	data 0.0005 (0.0045)	batch 0.9598 (0.9849)	loss 4.0448 (5.2259)	grad_norm 1.3044 (1.2980)	mem 55534MB
Train: [0/180][3750/5004]	eta 0:20:35 lr 0.099996	data 0.0003 (0.0046)	batch 0.9630 (0.9852)	loss 5.7377 (31.5907)	grad_norm 2.6673 (21.7268)	mem 55534MB
Train: [0/180][3900/5004]	eta 0:18:07 lr 0.099995	data 0.0002 (0.0045)	batch 0.9860 (0.9848)	loss 4.1484 (5.2131)	grad_norm 1.2800 (1.2979)	mem 55534MB
Train: [0/180][3800/5004]	eta 0:19:46 lr 0.099996	data 0.0004 (0.0046)	batch 0.9620 (0.9851)	loss 5.5844 (31.2526)	grad_norm 2.5138 (21.4767)	mem 55534MB
Train: [0/180][3950/5004]	eta 0:17:17 lr 0.099995	data 0.0003 (0.0044)	batch 0.9529 (0.9848)	loss 4.2054 (5.2001)	grad_norm 1.3149 (1.2980)	mem 55534MB
Train: [0/180][3850/5004]	eta 0:18:56 lr 0.099995	data 0.0003 (0.0045)	batch 0.9810 (0.9851)	loss 5.5938 (30.9223)	grad_norm 2.7573 (21.2337)	mem 55534MB
Train: [0/180][4000/5004]	eta 0:16:28 lr 0.099995	data 0.0003 (0.0044)	batch 0.9925 (0.9847)	loss 4.2365 (5.1875)	grad_norm 1.2643 (1.2979)	mem 55534MB
Train: [0/180][3900/5004]	eta 0:18:07 lr 0.099995	data 0.0003 (0.0045)	batch 0.9887 (0.9850)	loss 5.5778 (30.5986)	grad_norm 2.5748 (20.9952)	mem 55534MB
Train: [0/180][4050/5004]	eta 0:15:39 lr 0.099995	data 0.0005 (0.0043)	batch 1.0163 (0.9847)	loss 4.1963 (5.1747)	grad_norm 1.3498 (1.2979)	mem 55534MB
Train: [0/180][3950/5004]	eta 0:17:18 lr 0.099995	data 0.0003 (0.0044)	batch 0.9626 (0.9850)	loss 5.5230 (30.2813)	grad_norm 2.8449 (20.7619)	mem 55534MB
Train: [0/180][4100/5004]	eta 0:14:50 lr 0.099995	data 0.0005 (0.0043)	batch 0.9797 (0.9847)	loss 4.0856 (5.1617)	grad_norm 1.3565 (1.2978)	mem 55534MB
Train: [0/180][4000/5004]	eta 0:16:28 lr 0.099995	data 0.0003 (0.0044)	batch 0.9966 (0.9849)	loss 5.4800 (29.9716)	grad_norm 2.5543 (20.5337)	mem 55534MB
Train: [0/180][4150/5004]	eta 0:14:00 lr 0.099995	data 0.0004 (0.0042)	batch 0.9609 (0.9846)	loss 4.0731 (5.1489)	grad_norm 1.3150 (1.2979)	mem 55534MB
Train: [0/180][4050/5004]	eta 0:15:39 lr 0.099995	data 0.0004 (0.0043)	batch 0.9805 (0.9849)	loss 5.3761 (29.6687)	grad_norm 2.5420 (20.3122)	mem 55534MB
Train: [0/180][4200/5004]	eta 0:13:11 lr 0.099995	data 0.0005 (0.0042)	batch 0.9895 (0.9845)	loss 4.2010 (5.1364)	grad_norm 1.3007 (1.2980)	mem 55534MB
Train: [0/180][4100/5004]	eta 0:14:50 lr 0.099995	data 0.0003 (0.0043)	batch 0.9857 (0.9848)	loss 5.1590 (29.3728)	grad_norm 2.4403 (20.0957)	mem 55534MB
Train: [0/180][4250/5004]	eta 0:12:22 lr 0.099995	data 0.0003 (0.0041)	batch 1.0091 (0.9845)	loss 4.0507 (5.1237)	grad_norm 1.3317 (1.2979)	mem 55534MB
Train: [0/180][4150/5004]	eta 0:14:00 lr 0.099995	data 0.0004 (0.0042)	batch 1.0475 (0.9848)	loss 5.0151 (29.0829)	grad_norm 2.4311 (19.8838)	mem 55534MB
Train: [0/180][4300/5004]	eta 0:11:33 lr 0.099994	data 0.0004 (0.0041)	batch 0.9767 (0.9845)	loss 4.0486 (5.1112)	grad_norm 1.2971 (1.2980)	mem 55534MB
Train: [0/180][4200/5004]	eta 0:13:11 lr 0.099995	data 0.0004 (0.0042)	batch 0.9955 (0.9847)	loss 5.2095 (28.7987)	grad_norm 2.4989 (19.6762)	mem 55534MB
Train: [0/180][4350/5004]	eta 0:10:43 lr 0.099994	data 0.0003 (0.0041)	batch 0.9726 (0.9844)	loss 4.1554 (5.0990)	grad_norm 1.2740 (1.2981)	mem 55534MB
Train: [0/180][4250/5004]	eta 0:12:22 lr 0.099995	data 0.0004 (0.0041)	batch 0.9989 (0.9847)	loss 5.1354 (28.5197)	grad_norm 2.3735 (19.4732)	mem 55534MB
Train: [0/180][4400/5004]	eta 0:09:54 lr 0.099994	data 0.0003 (0.0040)	batch 0.9512 (0.9844)	loss 4.3412 (5.0868)	grad_norm 1.3198 (1.2982)	mem 55534MB
Train: [0/180][4300/5004]	eta 0:11:33 lr 0.099994	data 0.0003 (0.0041)	batch 0.9691 (0.9846)	loss 5.1054 (28.2471)	grad_norm 2.4949 (19.2749)	mem 55534MB
Train: [0/180][4450/5004]	eta 0:09:05 lr 0.099994	data 0.0003 (0.0040)	batch 1.0059 (0.9844)	loss 3.9510 (5.0745)	grad_norm 1.2925 (1.2981)	mem 55534MB
Train: [0/180][4350/5004]	eta 0:10:43 lr 0.099994	data 0.0003 (0.0040)	batch 0.9805 (0.9846)	loss 5.0044 (27.9796)	grad_norm 2.3035 (19.0809)	mem 55534MB
Train: [0/180][4500/5004]	eta 0:08:16 lr 0.099994	data 0.0002 (0.0039)	batch 0.9825 (0.9843)	loss 4.0263 (5.0621)	grad_norm 1.2810 (1.2982)	mem 55534MB
Train: [0/180][4400/5004]	eta 0:09:54 lr 0.099994	data 0.0002 (0.0040)	batch 0.9633 (0.9846)	loss 5.1437 (27.7174)	grad_norm 2.2589 (18.8906)	mem 55534MB
Train: [0/180][4550/5004]	eta 0:07:26 lr 0.099994	data 0.0004 (0.0039)	batch 0.9778 (0.9843)	loss 3.8828 (5.0498)	grad_norm 1.3539 (1.2982)	mem 55534MB
Train: [0/180][4450/5004]	eta 0:09:05 lr 0.099994	data 0.0003 (0.0040)	batch 0.9972 (0.9845)	loss 4.8064 (27.4605)	grad_norm 2.4293 (18.7049)	mem 55534MB
Train: [0/180][4600/5004]	eta 0:06:37 lr 0.099994	data 0.0003 (0.0039)	batch 0.9974 (0.9842)	loss 3.9060 (5.0375)	grad_norm 1.2562 (1.2982)	mem 55534MB
Train: [0/180][4500/5004]	eta 0:08:16 lr 0.099994	data 0.0003 (0.0039)	batch 0.9932 (0.9845)	loss 4.7793 (27.2083)	grad_norm 2.3934 (18.5230)	mem 55534MB
Train: [0/180][4650/5004]	eta 0:05:48 lr 0.099993	data 0.0004 (0.0038)	batch 0.9866 (0.9842)	loss 3.9587 (5.0252)	grad_norm 1.2868 (1.2980)	mem 55534MB
Train: [0/180][4550/5004]	eta 0:07:26 lr 0.099994	data 0.0004 (0.0039)	batch 0.9839 (0.9844)	loss 4.4211 (26.9606)	grad_norm 2.1384 (18.3443)	mem 55534MB
Train: [0/180][4700/5004]	eta 0:04:59 lr 0.099993	data 0.0003 (0.0038)	batch 0.9581 (0.9841)	loss 3.9173 (5.0132)	grad_norm 1.3039 (1.2980)	mem 55534MB
Train: [0/180][4600/5004]	eta 0:06:37 lr 0.099994	data 0.0003 (0.0038)	batch 0.9881 (0.9844)	loss 4.7159 (26.7184)	grad_norm 2.2646 (18.1697)	mem 55534MB
Train: [0/180][4750/5004]	eta 0:04:09 lr 0.099993	data 0.0004 (0.0037)	batch 0.9664 (0.9841)	loss 3.9592 (5.0012)	grad_norm 1.2843 (1.2979)	mem 55534MB
Train: [0/180][4650/5004]	eta 0:05:48 lr 0.099993	data 0.0004 (0.0038)	batch 0.9726 (0.9843)	loss 4.6763 (26.4802)	grad_norm 2.1621 (17.9982)	mem 55534MB
Train: [0/180][4800/5004]	eta 0:03:20 lr 0.099993	data 0.0002 (0.0037)	batch 0.9933 (0.9840)	loss 3.8995 (4.9892)	grad_norm 1.3203 (1.2978)	mem 55534MB
Train: [0/180][4700/5004]	eta 0:04:59 lr 0.099993	data 0.0004 (0.0038)	batch 0.9838 (0.9842)	loss 4.5107 (26.2473)	grad_norm 2.1563 (17.8305)	mem 55534MB
Train: [0/180][4850/5004]	eta 0:02:31 lr 0.099993	data 0.0003 (0.0037)	batch 0.9780 (0.9840)	loss 3.9074 (4.9774)	grad_norm 1.2838 (1.2976)	mem 55534MB
Train: [0/180][4750/5004]	eta 0:04:09 lr 0.099993	data 0.0004 (0.0037)	batch 0.9571 (0.9842)	loss 4.5846 (26.0182)	grad_norm 2.2213 (17.6659)	mem 55534MB
Train: [0/180][4900/5004]	eta 0:01:42 lr 0.099993	data 0.0003 (0.0036)	batch 0.9780 (0.9839)	loss 3.9448 (4.9653)	grad_norm 1.3116 (1.2975)	mem 55534MB
Train: [0/180][4800/5004]	eta 0:03:20 lr 0.099993	data 0.0004 (0.0037)	batch 0.9773 (0.9841)	loss 4.5550 (25.7937)	grad_norm 2.2664 (17.5049)	mem 55534MB
Train: [0/180][4950/5004]	eta 0:00:53 lr 0.099993	data 0.0003 (0.0036)	batch 0.9677 (0.9838)	loss 3.7905 (4.9537)	grad_norm 1.2632 (1.2976)	mem 55534MB
Train: [0/180][4850/5004]	eta 0:02:31 lr 0.099993	data 0.0004 (0.0037)	batch 0.9666 (0.9841)	loss 4.6067 (25.5733)	grad_norm 2.2380 (17.3465)	mem 55534MB
Train: [0/180][5000/5004]	eta 0:00:03 lr 0.099992	data 0.0001 (0.0036)	batch 0.9567 (0.9837)	loss 3.7022 (4.9418)	grad_norm 1.3322 (1.2975)	mem 55534MB
Current slope: None 	
EPOCH 0 training takes 1:22:03
Test: [0/196]	Time 10.801 (10.801)	Loss 2.4006 (2.4006)	Acc@1 45.703 (45.703)	Acc@5 78.906 (78.906)	Mem 55534MB
Train: [0/180][4900/5004]	eta 0:01:42 lr 0.099993	data 0.0003 (0.0036)	batch 1.0062 (0.9841)	loss 4.3947 (25.3560)	grad_norm 2.0557 (17.1911)	mem 55534MB
Test: [50/196]	Time 0.219 (0.538)	Loss 3.0164 (3.5106)	Acc@1 31.250 (23.491)	Acc@5 56.641 (49.939)	Mem 55534MB
Test: [100/196]	Time 0.327 (0.451)	Loss 5.2033 (3.6950)	Acc@1 10.156 (22.687)	Acc@5 19.141 (47.718)	Mem 55534MB
Train: [0/180][4950/5004]	eta 0:00:53 lr 0.099993	data 0.0003 (0.0036)	batch 0.9569 (0.9840)	loss 4.3250 (25.1437)	grad_norm 2.2259 (17.0391)	mem 55534MB
Test: [150/196]	Time 0.967 (0.419)	Loss 4.6998 (3.8766)	Acc@1 16.406 (21.673)	Acc@5 26.562 (45.194)	Mem 55534MB
 * Acc@1 22.200 Acc@5 45.448
Accuracy of the network on the 50000 test images: 22.20%
Max accuracy (after decay): 22.20%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/5004]	eta 1 day, 0:06:29 lr 0.099992	data 16.0606 (16.0606)	batch 17.3441 (17.3441)	loss 3.6064 (3.6064)	grad_norm 1.2557 (1.2557)	mem 55534MB
Train: [0/180][5000/5004]	eta 0:00:03 lr 0.099992	data 0.0002 (0.0036)	batch 0.9652 (0.9840)	loss 4.0134 (24.9347)	grad_norm 2.0413 (16.8898)	mem 55534MB
Current slope: None 	
EPOCH 0 training takes 1:22:04
Test: [0/196]	Time 10.573 (10.573)	Loss 2.5346 (2.5346)	Acc@1 38.281 (38.281)	Acc@5 71.875 (71.875)	Mem 55534MB
Test: [50/196]	Time 0.220 (0.534)	Loss 3.3495 (3.5361)	Acc@1 19.141 (27.076)	Acc@5 53.516 (54.052)	Mem 55534MB
Train: [1/180][50/5004]	eta 1:47:31 lr 0.099992	data 0.0004 (0.3153)	batch 0.9610 (1.3023)	loss 3.7426 (3.7236)	grad_norm 1.2638 (1.2934)	mem 55534MB
Test: [100/196]	Time 0.657 (0.451)	Loss 3.9560 (3.6586)	Acc@1 18.359 (26.392)	Acc@5 43.750 (52.715)	Mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.1
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Test: [150/196]	Time 0.919 (0.416)	Loss 4.0426 (3.8077)	Acc@1 26.562 (25.422)	Acc@5 44.922 (50.251)	Mem 55534MB
Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
 * Acc@1 25.428 Acc@5 50.070
Accuracy of the network on the 50000 test images: 25.43%
Max accuracy (after decay): 25.43%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Test: [0/196]	Time 14.648 (14.648)	Loss 0.5683 (0.5683)	Acc@1 87.500 (87.500)	Acc@5 98.438 (98.438)	Mem 8207MB
Test: [50/196]	Time 0.123 (0.575)	Loss 0.5182 (0.7565)	Acc@1 88.281 (82.498)	Acc@5 97.656 (95.895)	Mem 8208MB
Train: [1/180][0/5004]	eta 1 day, 0:02:43 lr 0.099992	data 15.8791 (15.8791)	batch 17.2988 (17.2988)	loss 3.9804 (3.9804)	grad_norm 2.0582 (2.0582)	mem 55534MB
Test: [100/196]	Time 1.159 (0.476)	Loss 1.3065 (0.8709)	Acc@1 67.578 (79.544)	Acc@5 92.188 (94.744)	Mem 8208MB
Test: [150/196]	Time 0.141 (0.426)	Loss 1.1012 (0.9696)	Acc@1 79.688 (77.468)	Acc@5 90.234 (93.460)	Mem 8208MB
Train: [1/180][50/5004]	eta 1:47:18 lr 0.099992	data 0.0003 (0.3117)	batch 0.9618 (1.2997)	loss 4.2627 (4.1687)	grad_norm 2.1541 (2.0910)	mem 55534MB
 * Acc@1 76.532 Acc@5 92.984
Accuracy of the teacher network on the 50000 test images: 76.5%
Successfully build teacher model
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 1 day, 2:16:25 lr 0.100000	data 15.8810 (15.8810)	batch 18.9020 (18.9020)	loss 26.2091 (26.2091)	grad_norm 72.3561 (72.3561)	mem 55575MB
Train: [1/180][100/5004]	eta 1:33:01 lr 0.099992	data 0.0004 (0.1576)	batch 0.9672 (1.1381)	loss 4.0996 (4.1654)	grad_norm 2.0253 (2.0647)	mem 55534MB
Train: [0/180][50/5004]	eta 1:57:13 lr 0.100000	data 0.0003 (0.3117)	batch 1.0621 (1.4198)	loss 22.5271 (19.7691)	grad_norm 31.7692 (27.9449)	mem 55575MB
Train: [1/180][150/5004]	eta 1:27:48 lr 0.099992	data 0.0004 (0.1056)	batch 1.0731 (1.0854)	loss 4.0784 (4.1472)	grad_norm 2.0622 (2.0692)	mem 55534MB
Train: [0/180][100/5004]	eta 1:41:35 lr 0.100000	data 0.0003 (0.1576)	batch 1.0546 (1.2430)	loss 38.6796 (27.0513)	grad_norm 40.9492 (34.6973)	mem 55575MB
Train: [1/180][200/5004]	eta 1:24:41 lr 0.099992	data 0.0004 (0.0794)	batch 0.9697 (1.0577)	loss 3.9120 (4.1241)	grad_norm 2.0243 (2.0562)	mem 55534MB
Train: [0/180][150/5004]	eta 1:35:49 lr 0.100000	data 0.0004 (0.1055)	batch 1.0582 (1.1844)	loss 61.8629 (34.9596)	grad_norm 70.1112 (38.3476)	mem 55575MB
Train: [1/180][250/5004]	eta 1:22:32 lr 0.099992	data 0.0004 (0.0637)	batch 0.9669 (1.0418)	loss 3.9790 (4.1083)	grad_norm 1.9471 (2.0486)	mem 55534MB
Train: [0/180][200/5004]	eta 1:32:28 lr 0.100000	data 0.0003 (0.0794)	batch 1.0732 (1.1549)	loss 73.3253 (41.8968)	grad_norm 58.6324 (40.5939)	mem 55575MB
Train: [1/180][300/5004]	eta 1:20:50 lr 0.099991	data 0.0004 (0.0532)	batch 0.9804 (1.0311)	loss 4.1320 (4.0880)	grad_norm 2.0578 (2.0419)	mem 55534MB
Train: [0/180][250/5004]	eta 1:30:07 lr 0.100000	data 0.0003 (0.0636)	batch 1.0652 (1.1376)	loss 80.2450 (47.0114)	grad_norm 39.7013 (42.3532)	mem 55575MB
Train: [1/180][350/5004]	eta 1:19:22 lr 0.099991	data 0.0004 (0.0456)	batch 0.9774 (1.0234)	loss 3.8814 (4.0814)	grad_norm 2.0218 (2.0381)	mem 55534MB
Train: [0/180][300/5004]	eta 1:28:15 lr 0.100000	data 0.0003 (0.0531)	batch 1.0829 (1.1257)	loss 71.5105 (51.0995)	grad_norm 51.2736 (43.2652)	mem 55575MB
Train: [1/180][400/5004]	eta 1:18:06 lr 0.099991	data 0.0004 (0.0400)	batch 0.9889 (1.0180)	loss 4.0693 (4.0713)	grad_norm 2.0794 (2.0328)	mem 55534MB
Train: [1/180][450/5004]	eta 1:16:56 lr 0.099991	data 0.0003 (0.0356)	batch 0.9992 (1.0137)	loss 4.0505 (4.0566)	grad_norm 1.9855 (2.0286)	mem 55534MB
Train: [0/180][350/5004]	eta 1:26:39 lr 0.100000	data 0.0003 (0.0456)	batch 1.0718 (1.1172)	loss 83.9449 (54.5011)	grad_norm 55.8863 (44.8112)	mem 55575MB
Train: [1/180][500/5004]	eta 1:15:47 lr 0.099991	data 0.0004 (0.0321)	batch 0.9738 (1.0096)	loss 4.0947 (4.0452)	grad_norm 2.0310 (2.0244)	mem 55534MB
Train: [0/180][400/5004]	eta 1:25:15 lr 0.100000	data 0.0004 (0.0400)	batch 1.0706 (1.1111)	loss 62.2726 (56.6653)	grad_norm 71.9242 (45.2674)	mem 55575MB
Train: [1/180][550/5004]	eta 1:14:42 lr 0.099991	data 0.0003 (0.0292)	batch 0.9915 (1.0064)	loss 3.8486 (4.0275)	grad_norm 1.9742 (2.0192)	mem 55534MB
Train: [0/180][450/5004]	eta 1:23:57 lr 0.100000	data 0.0003 (0.0356)	batch 1.0527 (1.1062)	loss 62.5876 (57.9165)	grad_norm 19.3477 (45.4011)	mem 55575MB
Train: [1/180][600/5004]	eta 1:13:39 lr 0.099990	data 0.0002 (0.0268)	batch 0.9635 (1.0036)	loss 4.1740 (4.0129)	grad_norm 1.9996 (2.0113)	mem 55534MB
Train: [0/180][500/5004]	eta 1:22:42 lr 0.100000	data 0.0004 (0.0320)	batch 1.0506 (1.1018)	loss 76.4206 (59.2916)	grad_norm 22.7121 (45.9415)	mem 55575MB
Train: [1/180][650/5004]	eta 1:12:41 lr 0.099990	data 0.0004 (0.0248)	batch 0.9785 (1.0017)	loss 3.7153 (4.0016)	grad_norm 1.9819 (2.0068)	mem 55534MB
Train: [0/180][550/5004]	eta 1:21:31 lr 0.100000	data 0.0003 (0.0292)	batch 1.0619 (1.0982)	loss 60.2212 (59.5555)	grad_norm 46.2363 (44.9004)	mem 55575MB
Train: [1/180][700/5004]	eta 1:11:43 lr 0.099990	data 0.0003 (0.0230)	batch 0.9952 (0.9999)	loss 3.7600 (3.9870)	grad_norm 1.9536 (2.0014)	mem 55534MB
Train: [0/180][600/5004]	eta 1:20:25 lr 0.100000	data 0.0003 (0.0268)	batch 1.0656 (1.0957)	loss 48.6494 (59.0538)	grad_norm 22.2538 (43.9205)	mem 55575MB
Train: [1/180][750/5004]	eta 1:10:47 lr 0.099990	data 0.0004 (0.0215)	batch 0.9886 (0.9985)	loss 3.8697 (3.9731)	grad_norm 1.9569 (1.9973)	mem 55534MB
Train: [0/180][650/5004]	eta 1:19:20 lr 0.100000	data 0.0003 (0.0247)	batch 1.0525 (1.0933)	loss 51.5084 (58.1804)	grad_norm 24.8237 (43.1741)	mem 55575MB
Train: [1/180][800/5004]	eta 1:09:52 lr 0.099990	data 0.0004 (0.0202)	batch 0.9690 (0.9972)	loss 3.6217 (3.9577)	grad_norm 1.8029 (1.9912)	mem 55534MB
Train: [0/180][700/5004]	eta 1:18:18 lr 0.100000	data 0.0003 (0.0230)	batch 1.0724 (1.0916)	loss 48.0033 (57.4527)	grad_norm 16.2940 (42.6247)	mem 55575MB
Train: [1/180][850/5004]	eta 1:08:57 lr 0.099990	data 0.0003 (0.0190)	batch 0.9513 (0.9961)	loss 3.5943 (3.9459)	grad_norm 1.8335 (1.9870)	mem 55534MB
Train: [0/180][750/5004]	eta 1:17:16 lr 0.100000	data 0.0003 (0.0215)	batch 1.0479 (1.0898)	loss 55.9854 (56.9516)	grad_norm 24.5989 (42.0820)	mem 55575MB
Train: [1/180][900/5004]	eta 1:08:04 lr 0.099989	data 0.0004 (0.0180)	batch 0.9703 (0.9952)	loss 3.6491 (3.9338)	grad_norm 1.8928 (1.9823)	mem 55534MB
Train: [0/180][800/5004]	eta 1:16:14 lr 0.100000	data 0.0004 (0.0202)	batch 1.0623 (1.0881)	loss 47.9327 (56.4254)	grad_norm 21.3778 (41.4488)	mem 55575MB
Train: [1/180][950/5004]	eta 1:07:10 lr 0.099989	data 0.0003 (0.0171)	batch 0.9678 (0.9942)	loss 3.4983 (3.9204)	grad_norm 1.8242 (1.9770)	mem 55534MB
Train: [0/180][850/5004]	eta 1:15:14 lr 0.100000	data 0.0003 (0.0190)	batch 1.0626 (1.0867)	loss 46.6126 (55.9657)	grad_norm 22.2543 (40.9862)	mem 55575MB
Train: [1/180][1000/5004]	eta 1:06:18 lr 0.099989	data 0.0004 (0.0162)	batch 0.9879 (0.9935)	loss 3.6407 (3.9070)	grad_norm 1.8772 (1.9716)	mem 55534MB
Train: [1/180][1050/5004]	eta 1:05:25 lr 0.099989	data 0.0004 (0.0155)	batch 0.9627 (0.9927)	loss 3.4847 (3.8929)	grad_norm 1.8829 (1.9670)	mem 55534MB
Train: [0/180][900/5004]	eta 1:14:15 lr 0.100000	data 0.0004 (0.0180)	batch 1.0711 (1.0856)	loss 42.5558 (55.4541)	grad_norm 15.4612 (40.4324)	mem 55575MB
Train: [1/180][1100/5004]	eta 1:04:32 lr 0.099989	data 0.0003 (0.0148)	batch 0.9836 (0.9920)	loss 3.6294 (3.8791)	grad_norm 1.8351 (1.9621)	mem 55534MB
Train: [0/180][950/5004]	eta 1:13:16 lr 0.100000	data 0.0003 (0.0171)	batch 1.0779 (1.0846)	loss 42.8237 (54.9263)	grad_norm 29.9889 (40.0896)	mem 55575MB
Train: [1/180][1150/5004]	eta 1:03:40 lr 0.099988	data 0.0003 (0.0142)	batch 0.9747 (0.9912)	loss 3.5397 (3.8667)	grad_norm 1.8681 (1.9575)	mem 55534MB
Train: [0/180][1000/5004]	eta 1:12:19 lr 0.100000	data 0.0003 (0.0162)	batch 1.0548 (1.0837)	loss 43.9060 (54.4448)	grad_norm 47.7326 (39.6275)	mem 55575MB
Train: [1/180][1200/5004]	eta 1:02:48 lr 0.099988	data 0.0004 (0.0136)	batch 0.9705 (0.9907)	loss 3.4028 (3.8542)	grad_norm 1.8128 (1.9532)	mem 55534MB
Train: [0/180][1050/5004]	eta 1:11:21 lr 0.100000	data 0.0003 (0.0155)	batch 1.0525 (1.0828)	loss 42.8964 (53.8110)	grad_norm 27.7499 (39.0902)	mem 55575MB
Train: [1/180][1250/5004]	eta 1:01:57 lr 0.099988	data 0.0004 (0.0131)	batch 0.9936 (0.9901)	loss 3.1505 (3.8412)	grad_norm 1.7651 (1.9486)	mem 55534MB
Train: [0/180][1100/5004]	eta 1:10:24 lr 0.100000	data 0.0003 (0.0148)	batch 1.0541 (1.0822)	loss 38.2874 (53.2890)	grad_norm 17.5518 (38.6073)	mem 55575MB
Train: [1/180][1300/5004]	eta 1:01:05 lr 0.099988	data 0.0004 (0.0126)	batch 1.0004 (0.9897)	loss 3.4714 (3.8280)	grad_norm 1.8263 (1.9441)	mem 55534MB
Train: [0/180][1150/5004]	eta 1:09:27 lr 0.100000	data 0.0003 (0.0142)	batch 1.0626 (1.0813)	loss 38.2309 (52.6491)	grad_norm 20.1132 (37.9750)	mem 55575MB
Train: [1/180][1350/5004]	eta 1:00:14 lr 0.099988	data 0.0004 (0.0121)	batch 0.9736 (0.9893)	loss 3.1887 (3.8165)	grad_norm 1.7622 (1.9403)	mem 55534MB
Train: [0/180][1200/5004]	eta 1:08:30 lr 0.100000	data 0.0003 (0.0136)	batch 1.0614 (1.0807)	loss 42.5403 (52.1241)	grad_norm 19.4974 (37.5877)	mem 55575MB
Train: [1/180][1400/5004]	eta 0:59:23 lr 0.099988	data 0.0003 (0.0117)	batch 0.9953 (0.9888)	loss 3.5355 (3.8032)	grad_norm 1.8875 (1.9358)	mem 55534MB
Train: [0/180][1250/5004]	eta 1:07:34 lr 0.100000	data 0.0003 (0.0131)	batch 1.0781 (1.0801)	loss 41.1039 (51.5715)	grad_norm 18.0117 (37.1818)	mem 55575MB
Train: [1/180][1450/5004]	eta 0:58:32 lr 0.099987	data 0.0004 (0.0113)	batch 0.9803 (0.9883)	loss 3.7404 (3.7910)	grad_norm 1.8752 (1.9315)	mem 55534MB
Train: [0/180][1300/5004]	eta 1:06:38 lr 0.099999	data 0.0003 (0.0126)	batch 1.0649 (1.0796)	loss 36.8757 (51.0260)	grad_norm 21.8820 (36.7440)	mem 55575MB
Train: [1/180][1500/5004]	eta 0:57:42 lr 0.099987	data 0.0003 (0.0109)	batch 0.9652 (0.9880)	loss 3.5024 (3.7799)	grad_norm 1.8011 (1.9279)	mem 55534MB
Train: [0/180][1350/5004]	eta 1:05:42 lr 0.099999	data 0.0004 (0.0121)	batch 1.0653 (1.0790)	loss 36.8155 (50.5246)	grad_norm 15.6997 (36.2979)	mem 55575MB
Train: [1/180][1550/5004]	eta 0:56:51 lr 0.099987	data 0.0002 (0.0106)	batch 0.9692 (0.9877)	loss 3.5159 (3.7686)	grad_norm 1.8108 (1.9237)	mem 55534MB
Train: [0/180][1400/5004]	eta 1:04:47 lr 0.099999	data 0.0003 (0.0117)	batch 1.0640 (1.0786)	loss 38.5617 (49.9879)	grad_norm 17.9525 (35.9236)	mem 55575MB
Train: [1/180][1600/5004]	eta 0:56:00 lr 0.099987	data 0.0003 (0.0103)	batch 0.9627 (0.9873)	loss 3.2435 (3.7565)	grad_norm 1.7303 (1.9198)	mem 55534MB
Train: [1/180][1650/5004]	eta 0:55:10 lr 0.099987	data 0.0003 (0.0100)	batch 0.9968 (0.9870)	loss 3.4013 (3.7464)	grad_norm 1.7191 (1.9163)	mem 55534MB
Train: [0/180][1450/5004]	eta 1:03:51 lr 0.099999	data 0.0004 (0.0113)	batch 1.0750 (1.0781)	loss 29.9714 (49.4313)	grad_norm 16.0442 (35.5061)	mem 55575MB
Train: [1/180][1700/5004]	eta 0:54:20 lr 0.099986	data 0.0003 (0.0097)	batch 0.9628 (0.9867)	loss 3.3147 (3.7346)	grad_norm 1.7520 (1.9127)	mem 55534MB
Train: [0/180][1500/5004]	eta 1:02:56 lr 0.099999	data 0.0003 (0.0110)	batch 1.0523 (1.0776)	loss 38.0946 (48.9428)	grad_norm 31.5906 (35.1412)	mem 55575MB
Train: [1/180][1750/5004]	eta 0:53:29 lr 0.099986	data 0.0003 (0.0094)	batch 0.9915 (0.9864)	loss 3.2403 (3.7235)	grad_norm 1.7869 (1.9084)	mem 55534MB
Train: [0/180][1550/5004]	eta 1:02:01 lr 0.099999	data 0.0003 (0.0106)	batch 1.0794 (1.0774)	loss 37.2600 (48.4188)	grad_norm 25.1042 (34.6697)	mem 55575MB
Train: [1/180][1800/5004]	eta 0:52:39 lr 0.099986	data 0.0003 (0.0092)	batch 0.9702 (0.9861)	loss 3.3071 (3.7136)	grad_norm 1.7698 (1.9051)	mem 55534MB
Train: [0/180][1600/5004]	eta 1:01:06 lr 0.099999	data 0.0004 (0.0103)	batch 1.0711 (1.0770)	loss 28.0917 (47.8760)	grad_norm 13.8209 (34.2805)	mem 55575MB
Train: [1/180][1850/5004]	eta 0:51:49 lr 0.099986	data 0.0004 (0.0089)	batch 1.0068 (0.9859)	loss 3.3659 (3.7032)	grad_norm 1.7470 (1.9014)	mem 55534MB
Train: [0/180][1650/5004]	eta 1:00:11 lr 0.099999	data 0.0003 (0.0100)	batch 1.0628 (1.0767)	loss 31.1256 (47.3345)	grad_norm 23.8926 (33.8018)	mem 55575MB
Train: [1/180][1900/5004]	eta 0:50:59 lr 0.099986	data 0.0005 (0.0087)	batch 1.0008 (0.9856)	loss 3.1069 (3.6924)	grad_norm 1.7203 (1.8980)	mem 55534MB
Train: [0/180][1700/5004]	eta 0:59:16 lr 0.099999	data 0.0004 (0.0097)	batch 1.0554 (1.0764)	loss 30.7147 (46.7794)	grad_norm 15.7123 (33.3653)	mem 55575MB
Train: [1/180][1950/5004]	eta 0:50:09 lr 0.099985	data 0.0003 (0.0085)	batch 0.9584 (0.9854)	loss 3.4748 (3.6813)	grad_norm 1.7981 (1.8945)	mem 55534MB
Train: [0/180][1750/5004]	eta 0:58:21 lr 0.099999	data 0.0004 (0.0094)	batch 1.0764 (1.0761)	loss 26.2609 (46.2637)	grad_norm 20.2067 (33.0350)	mem 55575MB
Train: [1/180][2000/5004]	eta 0:49:19 lr 0.099985	data 0.0004 (0.0083)	batch 0.9647 (0.9852)	loss 3.3752 (3.6706)	grad_norm 1.7792 (1.8917)	mem 55534MB
Train: [0/180][1800/5004]	eta 0:57:26 lr 0.099999	data 0.0005 (0.0092)	batch 1.0494 (1.0757)	loss 25.0274 (45.7556)	grad_norm 9.5161 (32.6393)	mem 55575MB
Train: [1/180][2050/5004]	eta 0:48:29 lr 0.099985	data 0.0003 (0.0081)	batch 0.9570 (0.9849)	loss 3.1214 (3.6602)	grad_norm 1.7586 (1.8880)	mem 55534MB
Train: [0/180][1850/5004]	eta 0:56:32 lr 0.099999	data 0.0005 (0.0089)	batch 1.0545 (1.0755)	loss 25.1015 (45.2596)	grad_norm 14.3355 (32.2367)	mem 55575MB
Train: [1/180][2100/5004]	eta 0:47:39 lr 0.099985	data 0.0004 (0.0079)	batch 0.9771 (0.9847)	loss 3.1153 (3.6503)	grad_norm 1.6245 (1.8845)	mem 55534MB
Train: [0/180][1900/5004]	eta 0:55:37 lr 0.099999	data 0.0003 (0.0087)	batch 1.0649 (1.0752)	loss 23.9317 (44.7278)	grad_norm 11.8829 (31.8098)	mem 55575MB
Train: [1/180][2150/5004]	eta 0:46:49 lr 0.099984	data 0.0004 (0.0077)	batch 0.9824 (0.9846)	loss 3.3873 (3.6402)	grad_norm 1.8130 (1.8810)	mem 55534MB
Train: [0/180][1950/5004]	eta 0:54:42 lr 0.099999	data 0.0004 (0.0085)	batch 1.0555 (1.0749)	loss 24.3928 (44.1939)	grad_norm 20.7811 (31.4404)	mem 55575MB
Train: [1/180][2200/5004]	eta 0:46:00 lr 0.099984	data 0.0004 (0.0076)	batch 0.9704 (0.9844)	loss 3.2265 (3.6306)	grad_norm 1.7182 (1.8780)	mem 55534MB
Train: [1/180][2250/5004]	eta 0:45:10 lr 0.099984	data 0.0004 (0.0074)	batch 0.9648 (0.9843)	loss 3.2358 (3.6208)	grad_norm 1.7272 (1.8751)	mem 55534MB
Train: [0/180][2000/5004]	eta 0:53:48 lr 0.099999	data 0.0003 (0.0083)	batch 1.0515 (1.0746)	loss 24.0522 (43.6915)	grad_norm 12.4629 (31.0944)	mem 55575MB
Train: [1/180][2300/5004]	eta 0:44:21 lr 0.099984	data 0.0004 (0.0073)	batch 0.9638 (0.9841)	loss 3.1164 (3.6104)	grad_norm 1.6867 (1.8719)	mem 55534MB
Train: [0/180][2050/5004]	eta 0:52:53 lr 0.099999	data 0.0004 (0.0081)	batch 1.0645 (1.0744)	loss 20.1827 (43.1957)	grad_norm 9.8385 (30.7199)	mem 55575MB
Train: [1/180][2350/5004]	eta 0:43:31 lr 0.099984	data 0.0004 (0.0071)	batch 0.9802 (0.9841)	loss 3.1763 (3.6008)	grad_norm 1.6993 (1.8687)	mem 55534MB
Train: [0/180][2100/5004]	eta 0:51:59 lr 0.099999	data 0.0004 (0.0079)	batch 1.0678 (1.0742)	loss 22.3539 (42.6849)	grad_norm 15.3152 (30.3477)	mem 55575MB
Train: [1/180][2400/5004]	eta 0:42:42 lr 0.099983	data 0.0004 (0.0070)	batch 0.9712 (0.9839)	loss 3.1357 (3.5909)	grad_norm 1.7510 (1.8657)	mem 55534MB
Train: [0/180][2150/5004]	eta 0:51:05 lr 0.099999	data 0.0003 (0.0077)	batch 1.0728 (1.0739)	loss 19.3326 (42.1788)	grad_norm 13.6133 (29.9671)	mem 55575MB
Train: [1/180][2450/5004]	eta 0:41:52 lr 0.099983	data 0.0004 (0.0069)	batch 0.9792 (0.9838)	loss 3.4084 (3.5812)	grad_norm 1.7659 (1.8625)	mem 55534MB
Train: [0/180][2200/5004]	eta 0:50:10 lr 0.099999	data 0.0003 (0.0076)	batch 1.0618 (1.0738)	loss 17.8397 (41.6694)	grad_norm 10.8893 (29.5682)	mem 55575MB
Train: [1/180][2500/5004]	eta 0:41:03 lr 0.099983	data 0.0004 (0.0067)	batch 1.0109 (0.9837)	loss 3.1901 (3.5720)	grad_norm 1.7227 (1.8599)	mem 55534MB
Train: [0/180][2250/5004]	eta 0:49:17 lr 0.099998	data 0.0004 (0.0074)	batch 1.0908 (1.0737)	loss 16.8791 (41.1710)	grad_norm 10.0485 (29.1952)	mem 55575MB
Train: [1/180][2550/5004]	eta 0:40:13 lr 0.099983	data 0.0004 (0.0066)	batch 0.9613 (0.9836)	loss 2.9623 (3.5624)	grad_norm 1.7072 (1.8570)	mem 55534MB
Train: [0/180][2300/5004]	eta 0:48:22 lr 0.099998	data 0.0004 (0.0073)	batch 1.0503 (1.0736)	loss 19.3710 (40.6760)	grad_norm 11.3128 (28.8318)	mem 55575MB
Train: [1/180][2600/5004]	eta 0:39:24 lr 0.099982	data 0.0003 (0.0065)	batch 0.9668 (0.9834)	loss 3.1169 (3.5527)	grad_norm 1.6915 (1.8540)	mem 55534MB
Train: [0/180][2350/5004]	eta 0:47:28 lr 0.099998	data 0.0003 (0.0071)	batch 1.0480 (1.0734)	loss 15.9174 (40.1835)	grad_norm 11.9711 (28.4566)	mem 55575MB
Train: [1/180][2650/5004]	eta 0:38:34 lr 0.099982	data 0.0003 (0.0064)	batch 0.9718 (0.9833)	loss 3.1605 (3.5434)	grad_norm 1.6573 (1.8512)	mem 55534MB
Train: [0/180][2400/5004]	eta 0:46:34 lr 0.099998	data 0.0003 (0.0070)	batch 1.0966 (1.0732)	loss 15.4912 (39.6981)	grad_norm 8.7265 (28.0961)	mem 55575MB
Train: [1/180][2700/5004]	eta 0:37:45 lr 0.099982	data 0.0004 (0.0063)	batch 0.9626 (0.9832)	loss 3.2186 (3.5343)	grad_norm 1.6990 (1.8486)	mem 55534MB
Train: [0/180][2450/5004]	eta 0:45:40 lr 0.099998	data 0.0003 (0.0068)	batch 1.0682 (1.0731)	loss 16.5207 (39.2230)	grad_norm 13.4605 (27.7493)	mem 55575MB
Train: [1/180][2750/5004]	eta 0:36:55 lr 0.099982	data 0.0004 (0.0061)	batch 1.0558 (0.9831)	loss 3.2036 (3.5249)	grad_norm 1.6842 (1.8458)	mem 55534MB
Train: [0/180][2500/5004]	eta 0:44:46 lr 0.099998	data 0.0004 (0.0067)	batch 1.0537 (1.0729)	loss 16.0480 (38.7617)	grad_norm 7.3470 (27.3946)	mem 55575MB
Train: [1/180][2800/5004]	eta 0:36:06 lr 0.099981	data 0.0004 (0.0060)	batch 0.9655 (0.9830)	loss 3.2812 (3.5161)	grad_norm 1.7554 (1.8433)	mem 55534MB
Train: [1/180][2850/5004]	eta 0:35:17 lr 0.099981	data 0.0003 (0.0059)	batch 0.9743 (0.9828)	loss 2.9675 (3.5070)	grad_norm 1.6597 (1.8407)	mem 55534MB
Train: [0/180][2550/5004]	eta 0:43:52 lr 0.099998	data 0.0003 (0.0066)	batch 1.0610 (1.0728)	loss 15.4257 (38.2957)	grad_norm 9.3289 (27.0508)	mem 55575MB
Train: [1/180][2900/5004]	eta 0:34:27 lr 0.099981	data 0.0003 (0.0058)	batch 0.9853 (0.9828)	loss 2.8448 (3.4984)	grad_norm 1.6503 (1.8388)	mem 55534MB
Train: [0/180][2600/5004]	eta 0:42:58 lr 0.099998	data 0.0003 (0.0065)	batch 1.0627 (1.0726)	loss 13.5093 (37.8324)	grad_norm 10.4093 (26.7064)	mem 55575MB
Train: [1/180][2950/5004]	eta 0:33:38 lr 0.099981	data 0.0003 (0.0058)	batch 0.9808 (0.9826)	loss 2.8201 (3.4892)	grad_norm 1.6803 (1.8364)	mem 55534MB
Train: [0/180][2650/5004]	eta 0:42:04 lr 0.099998	data 0.0005 (0.0064)	batch 1.0528 (1.0725)	loss 13.4525 (37.3760)	grad_norm 7.5050 (26.3752)	mem 55575MB
Train: [1/180][3000/5004]	eta 0:32:48 lr 0.099981	data 0.0004 (0.0057)	batch 0.9632 (0.9825)	loss 2.9245 (3.4804)	grad_norm 1.6716 (1.8339)	mem 55534MB
Train: [0/180][2700/5004]	eta 0:41:10 lr 0.099998	data 0.0003 (0.0062)	batch 1.0464 (1.0724)	loss 13.9153 (36.9371)	grad_norm 15.5460 (26.0565)	mem 55575MB
Train: [1/180][3050/5004]	eta 0:31:59 lr 0.099980	data 0.0004 (0.0056)	batch 0.9762 (0.9825)	loss 3.0836 (3.4715)	grad_norm 1.6890 (1.8315)	mem 55534MB
Train: [0/180][2750/5004]	eta 0:40:16 lr 0.099998	data 0.0003 (0.0061)	batch 1.0610 (1.0722)	loss 13.0722 (36.4950)	grad_norm 6.9716 (25.7399)	mem 55575MB
Train: [1/180][3100/5004]	eta 0:31:10 lr 0.099980	data 0.0003 (0.0055)	batch 0.9799 (0.9824)	loss 2.7007 (3.4623)	grad_norm 1.6070 (1.8289)	mem 55534MB
Train: [0/180][2800/5004]	eta 0:39:22 lr 0.099998	data 0.0005 (0.0060)	batch 1.0800 (1.0720)	loss 12.3718 (36.0592)	grad_norm 8.1529 (25.4202)	mem 55575MB
Train: [1/180][3150/5004]	eta 0:30:21 lr 0.099980	data 0.0003 (0.0054)	batch 0.9912 (0.9823)	loss 2.8131 (3.4529)	grad_norm 1.6807 (1.8266)	mem 55534MB
Train: [0/180][2850/5004]	eta 0:38:28 lr 0.099998	data 0.0003 (0.0059)	batch 1.0586 (1.0719)	loss 12.3531 (35.6349)	grad_norm 4.8452 (25.1188)	mem 55575MB
Train: [1/180][3200/5004]	eta 0:29:32 lr 0.099980	data 0.0004 (0.0053)	batch 0.9884 (0.9823)	loss 2.8178 (3.4442)	grad_norm 1.6872 (1.8243)	mem 55534MB
Train: [0/180][2900/5004]	eta 0:37:34 lr 0.099997	data 0.0003 (0.0058)	batch 1.0787 (1.0717)	loss 11.1734 (35.2075)	grad_norm 15.5614 (24.8091)	mem 55575MB
Train: [1/180][3250/5004]	eta 0:28:42 lr 0.099979	data 0.0003 (0.0053)	batch 0.9879 (0.9821)	loss 2.7800 (3.4359)	grad_norm 1.6553 (1.8222)	mem 55534MB
Train: [0/180][2950/5004]	eta 0:36:41 lr 0.099997	data 0.0003 (0.0057)	batch 1.0621 (1.0716)	loss 9.5434 (34.7867)	grad_norm 5.4386 (24.5063)	mem 55575MB
Train: [1/180][3300/5004]	eta 0:27:53 lr 0.099979	data 0.0004 (0.0052)	batch 0.9615 (0.9821)	loss 2.8018 (3.4275)	grad_norm 1.7134 (1.8200)	mem 55534MB
Train: [0/180][3000/5004]	eta 0:35:47 lr 0.099997	data 0.0003 (0.0056)	batch 1.0547 (1.0715)	loss 9.3180 (34.3666)	grad_norm 5.8778 (24.2020)	mem 55575MB
Train: [1/180][3350/5004]	eta 0:27:04 lr 0.099979	data 0.0003 (0.0051)	batch 0.9978 (0.9820)	loss 2.8520 (3.4193)	grad_norm 1.6502 (1.8178)	mem 55534MB
Train: [0/180][3050/5004]	eta 0:34:53 lr 0.099997	data 0.0004 (0.0056)	batch 1.0563 (1.0714)	loss 7.9477 (33.9459)	grad_norm 4.2626 (23.8956)	mem 55575MB
Train: [1/180][3400/5004]	eta 0:26:15 lr 0.099979	data 0.0003 (0.0050)	batch 0.9614 (0.9819)	loss 2.7661 (3.4110)	grad_norm 1.6042 (1.8155)	mem 55534MB
Train: [1/180][3450/5004]	eta 0:25:25 lr 0.099978	data 0.0003 (0.0050)	batch 0.9552 (0.9819)	loss 2.9062 (3.4025)	grad_norm 1.6683 (1.8134)	mem 55534MB
Train: [0/180][3100/5004]	eta 0:33:59 lr 0.099997	data 0.0004 (0.0055)	batch 1.1517 (1.0713)	loss 8.1621 (33.5305)	grad_norm 4.3995 (23.5940)	mem 55575MB
Train: [1/180][3500/5004]	eta 0:24:36 lr 0.099978	data 0.0003 (0.0049)	batch 0.9710 (0.9818)	loss 2.6969 (3.3939)	grad_norm 1.6286 (1.8112)	mem 55534MB
Train: [0/180][3150/5004]	eta 0:33:05 lr 0.099997	data 0.0003 (0.0054)	batch 1.0620 (1.0712)	loss 6.9235 (33.1145)	grad_norm 4.5621 (23.2937)	mem 55575MB
Train: [1/180][3550/5004]	eta 0:23:47 lr 0.099978	data 0.0004 (0.0048)	batch 0.9854 (0.9817)	loss 2.7934 (3.3858)	grad_norm 1.6812 (1.8092)	mem 55534MB
Train: [0/180][3200/5004]	eta 0:32:12 lr 0.099997	data 0.0004 (0.0053)	batch 1.0728 (1.0711)	loss 7.1686 (32.7136)	grad_norm 4.4213 (23.0042)	mem 55575MB
Train: [1/180][3600/5004]	eta 0:22:58 lr 0.099977	data 0.0003 (0.0048)	batch 0.9589 (0.9816)	loss 2.6076 (3.3780)	grad_norm 1.5951 (1.8074)	mem 55534MB
Train: [0/180][3250/5004]	eta 0:31:18 lr 0.099997	data 0.0004 (0.0052)	batch 1.0547 (1.0710)	loss 6.0016 (32.3108)	grad_norm 3.9627 (22.7139)	mem 55575MB
Train: [1/180][3650/5004]	eta 0:22:08 lr 0.099977	data 0.0003 (0.0047)	batch 0.9792 (0.9815)	loss 2.6390 (3.3696)	grad_norm 1.6646 (1.8055)	mem 55534MB
Train: [0/180][3300/5004]	eta 0:30:24 lr 0.099997	data 0.0003 (0.0052)	batch 1.0580 (1.0709)	loss 5.3778 (31.9080)	grad_norm 3.4518 (22.4264)	mem 55575MB
Train: [1/180][3700/5004]	eta 0:21:19 lr 0.099977	data 0.0003 (0.0047)	batch 0.9621 (0.9815)	loss 2.6946 (3.3614)	grad_norm 1.5928 (1.8034)	mem 55534MB
Train: [0/180][3350/5004]	eta 0:29:31 lr 0.099997	data 0.0003 (0.0051)	batch 1.0782 (1.0709)	loss 5.4864 (31.5160)	grad_norm 3.7819 (22.1486)	mem 55575MB
Train: [1/180][3750/5004]	eta 0:20:30 lr 0.099977	data 0.0003 (0.0046)	batch 0.9829 (0.9814)	loss 2.6914 (3.3534)	grad_norm 1.6524 (1.8013)	mem 55534MB
Train: [0/180][3400/5004]	eta 0:28:37 lr 0.099996	data 0.0003 (0.0050)	batch 1.0501 (1.0708)	loss 5.4085 (31.1323)	grad_norm 3.3798 (21.8756)	mem 55575MB
Train: [1/180][3800/5004]	eta 0:19:41 lr 0.099976	data 0.0003 (0.0045)	batch 0.9774 (0.9814)	loss 2.7966 (3.3451)	grad_norm 1.6712 (1.7994)	mem 55534MB
Train: [0/180][3450/5004]	eta 0:27:43 lr 0.099996	data 0.0004 (0.0050)	batch 1.0474 (1.0707)	loss 5.2307 (30.7570)	grad_norm 3.5232 (21.6089)	mem 55575MB
Train: [1/180][3850/5004]	eta 0:18:52 lr 0.099976	data 0.0003 (0.0045)	batch 0.9588 (0.9813)	loss 2.7104 (3.3371)	grad_norm 1.5951 (1.7974)	mem 55534MB
Train: [0/180][3500/5004]	eta 0:26:50 lr 0.099996	data 0.0004 (0.0049)	batch 1.0552 (1.0706)	loss 5.1736 (30.3896)	grad_norm 3.3047 (21.3488)	mem 55575MB
Train: [1/180][3900/5004]	eta 0:18:03 lr 0.099976	data 0.0003 (0.0044)	batch 0.9671 (0.9813)	loss 2.8144 (3.3297)	grad_norm 1.7525 (1.7957)	mem 55534MB
Train: [0/180][3550/5004]	eta 0:25:56 lr 0.099996	data 0.0003 (0.0048)	batch 1.0567 (1.0705)	loss 4.9039 (30.0319)	grad_norm 2.9614 (21.0950)	mem 55575MB
Train: [1/180][3950/5004]	eta 0:17:14 lr 0.099976	data 0.0003 (0.0044)	batch 0.9610 (0.9812)	loss 2.5257 (3.3217)	grad_norm 1.5973 (1.7939)	mem 55534MB
Train: [0/180][3600/5004]	eta 0:25:02 lr 0.099996	data 0.0003 (0.0048)	batch 1.0572 (1.0704)	loss 4.8827 (29.6821)	grad_norm 3.0650 (20.8474)	mem 55575MB
Train: [1/180][4000/5004]	eta 0:16:25 lr 0.099975	data 0.0003 (0.0043)	batch 0.9753 (0.9811)	loss 2.9360 (3.3142)	grad_norm 1.7671 (1.7922)	mem 55534MB
Train: [1/180][4050/5004]	eta 0:15:35 lr 0.099975	data 0.0004 (0.0043)	batch 0.9655 (0.9811)	loss 2.6568 (3.3067)	grad_norm 1.6235 (1.7905)	mem 55534MB
Train: [0/180][3650/5004]	eta 0:24:09 lr 0.099996	data 0.0003 (0.0047)	batch 1.0568 (1.0704)	loss 4.4764 (29.3409)	grad_norm 3.1001 (20.6067)	mem 55575MB
Train: [1/180][4100/5004]	eta 0:14:46 lr 0.099975	data 0.0003 (0.0042)	batch 0.9787 (0.9810)	loss 2.5511 (3.2990)	grad_norm 1.6214 (1.7886)	mem 55534MB
Train: [0/180][3700/5004]	eta 0:23:15 lr 0.099996	data 0.0004 (0.0047)	batch 1.0590 (1.0703)	loss 4.4232 (29.0068)	grad_norm 3.1443 (20.3705)	mem 55575MB
Train: [1/180][4150/5004]	eta 0:13:57 lr 0.099975	data 0.0004 (0.0042)	batch 0.9712 (0.9810)	loss 2.6669 (3.2913)	grad_norm 1.6269 (1.7870)	mem 55534MB
Train: [0/180][3750/5004]	eta 0:22:22 lr 0.099996	data 0.0003 (0.0046)	batch 1.0609 (1.0703)	loss 4.4508 (28.6795)	grad_norm 2.9969 (20.1395)	mem 55575MB
Train: [1/180][4200/5004]	eta 0:13:08 lr 0.099974	data 0.0004 (0.0041)	batch 0.9509 (0.9809)	loss 2.5974 (3.2837)	grad_norm 1.6777 (1.7853)	mem 55534MB
Train: [0/180][3800/5004]	eta 0:21:28 lr 0.099996	data 0.0003 (0.0045)	batch 1.0639 (1.0702)	loss 4.2524 (28.3616)	grad_norm 2.9326 (19.9146)	mem 55575MB
Train: [1/180][4250/5004]	eta 0:12:19 lr 0.099974	data 0.0003 (0.0041)	batch 0.9604 (0.9809)	loss 2.6155 (3.2762)	grad_norm 1.6375 (1.7836)	mem 55534MB
Train: [0/180][3850/5004]	eta 0:20:34 lr 0.099995	data 0.0002 (0.0045)	batch 1.0754 (1.0702)	loss 4.0100 (28.0504)	grad_norm 2.9091 (19.6955)	mem 55575MB
Train: [1/180][4300/5004]	eta 0:11:30 lr 0.099974	data 0.0003 (0.0041)	batch 0.9950 (0.9809)	loss 2.7462 (3.2684)	grad_norm 1.7244 (1.7817)	mem 55534MB
Train: [0/180][3900/5004]	eta 0:19:41 lr 0.099995	data 0.0004 (0.0044)	batch 1.0571 (1.0701)	loss 4.4203 (27.7466)	grad_norm 2.8233 (19.4804)	mem 55575MB
Train: [1/180][4350/5004]	eta 0:10:41 lr 0.099973	data 0.0003 (0.0040)	batch 0.9664 (0.9808)	loss 2.5142 (3.2608)	grad_norm 1.6691 (1.7802)	mem 55534MB
Train: [0/180][3950/5004]	eta 0:18:47 lr 0.099995	data 0.0004 (0.0044)	batch 1.0637 (1.0700)	loss 4.1490 (27.4491)	grad_norm 2.8116 (19.2699)	mem 55575MB
Train: [1/180][4400/5004]	eta 0:09:52 lr 0.099973	data 0.0003 (0.0040)	batch 0.9553 (0.9808)	loss 2.5153 (3.2531)	grad_norm 1.6248 (1.7785)	mem 55534MB
Train: [0/180][4000/5004]	eta 0:17:54 lr 0.099995	data 0.0003 (0.0043)	batch 1.0707 (1.0700)	loss 4.4405 (27.1592)	grad_norm 2.9992 (19.0650)	mem 55575MB
Train: [1/180][4450/5004]	eta 0:09:03 lr 0.099973	data 0.0004 (0.0039)	batch 0.9637 (0.9807)	loss 2.5721 (3.2456)	grad_norm 1.6225 (1.7769)	mem 55534MB
Train: [0/180][4050/5004]	eta 0:17:00 lr 0.099995	data 0.0003 (0.0043)	batch 1.0587 (1.0700)	loss 3.8733 (26.8752)	grad_norm 2.6398 (18.8644)	mem 55575MB
Train: [1/180][4500/5004]	eta 0:08:14 lr 0.099973	data 0.0003 (0.0039)	batch 0.9684 (0.9807)	loss 2.5900 (3.2382)	grad_norm 1.6554 (1.7754)	mem 55534MB
Train: [0/180][4100/5004]	eta 0:16:07 lr 0.099995	data 0.0003 (0.0042)	batch 1.0517 (1.0699)	loss 3.7824 (26.5972)	grad_norm 2.6292 (18.6683)	mem 55575MB
Train: [1/180][4550/5004]	eta 0:07:25 lr 0.099972	data 0.0004 (0.0039)	batch 0.9606 (0.9807)	loss 2.5253 (3.2308)	grad_norm 1.6308 (1.7740)	mem 55534MB
Train: [0/180][4150/5004]	eta 0:15:13 lr 0.099995	data 0.0004 (0.0042)	batch 1.0604 (1.0698)	loss 3.9030 (26.3251)	grad_norm 2.6530 (18.4766)	mem 55575MB
Train: [1/180][4600/5004]	eta 0:06:36 lr 0.099972	data 0.0004 (0.0038)	batch 0.9640 (0.9807)	loss 2.5479 (3.2237)	grad_norm 1.6496 (1.7725)	mem 55534MB
Train: [1/180][4650/5004]	eta 0:05:47 lr 0.099972	data 0.0004 (0.0038)	batch 0.9683 (0.9806)	loss 2.5910 (3.2161)	grad_norm 1.6760 (1.7712)	mem 55534MB
Train: [0/180][4200/5004]	eta 0:14:20 lr 0.099995	data 0.0005 (0.0041)	batch 1.0729 (1.0698)	loss 4.2363 (26.0594)	grad_norm 2.7690 (18.2883)	mem 55575MB
Train: [1/180][4700/5004]	eta 0:04:58 lr 0.099971	data 0.0004 (0.0037)	batch 0.9753 (0.9806)	loss 2.5230 (3.2092)	grad_norm 1.6779 (1.7696)	mem 55534MB
Train: [0/180][4250/5004]	eta 0:13:26 lr 0.099995	data 0.0003 (0.0041)	batch 1.0571 (1.0698)	loss 3.9230 (25.7989)	grad_norm 2.6874 (18.1040)	mem 55575MB
Train: [1/180][4750/5004]	eta 0:04:09 lr 0.099971	data 0.0004 (0.0037)	batch 0.9772 (0.9805)	loss 2.4742 (3.2019)	grad_norm 1.7086 (1.7681)	mem 55534MB
Train: [0/180][4300/5004]	eta 0:12:33 lr 0.099994	data 0.0003 (0.0041)	batch 1.0538 (1.0697)	loss 3.8015 (25.5441)	grad_norm 2.6676 (17.9233)	mem 55575MB
Train: [1/180][4800/5004]	eta 0:03:20 lr 0.099971	data 0.0004 (0.0037)	batch 0.9667 (0.9805)	loss 2.7563 (3.1945)	grad_norm 1.7055 (1.7665)	mem 55534MB
Train: [0/180][4350/5004]	eta 0:11:39 lr 0.099994	data 0.0003 (0.0040)	batch 1.0613 (1.0697)	loss 3.8506 (25.2941)	grad_norm 2.6177 (17.7465)	mem 55575MB
Train: [1/180][4850/5004]	eta 0:02:30 lr 0.099970	data 0.0005 (0.0036)	batch 0.9782 (0.9805)	loss 2.2627 (3.1874)	grad_norm 1.5450 (1.7652)	mem 55534MB
Train: [0/180][4400/5004]	eta 0:10:46 lr 0.099994	data 0.0003 (0.0040)	batch 1.0726 (1.0697)	loss 3.6597 (25.0498)	grad_norm 2.2953 (17.5736)	mem 55575MB
Train: [1/180][4900/5004]	eta 0:01:41 lr 0.099970	data 0.0003 (0.0036)	batch 0.9698 (0.9804)	loss 2.4582 (3.1801)	grad_norm 1.5891 (1.7640)	mem 55534MB
Train: [0/180][4450/5004]	eta 0:09:52 lr 0.099994	data 0.0004 (0.0039)	batch 1.0782 (1.0696)	loss 3.8131 (24.8101)	grad_norm 2.5031 (17.4035)	mem 55575MB
Train: [1/180][4950/5004]	eta 0:00:52 lr 0.099970	data 0.0003 (0.0036)	batch 1.0061 (0.9804)	loss 2.3845 (3.1729)	grad_norm 1.6482 (1.7628)	mem 55534MB
Train: [0/180][4500/5004]	eta 0:08:59 lr 0.099994	data 0.0004 (0.0039)	batch 1.0668 (1.0696)	loss 3.7890 (24.5752)	grad_norm 2.4301 (17.2377)	mem 55575MB
Train: [1/180][5000/5004]	eta 0:00:03 lr 0.099970	data 0.0002 (0.0035)	batch 0.9561 (0.9803)	loss 2.4212 (3.1657)	grad_norm 1.5717 (1.7614)	mem 55534MB
Current slope: None 	
EPOCH 1 training takes 1:21:45
Test: [0/196]	Time 10.728 (10.728)	Loss 1.4008 (1.4008)	Acc@1 67.578 (67.578)	Acc@5 91.797 (91.797)	Mem 55534MB
Test: [50/196]	Time 0.219 (0.538)	Loss 1.9306 (2.3687)	Acc@1 52.344 (46.507)	Acc@5 80.859 (74.793)	Mem 55534MB
Train: [0/180][4550/5004]	eta 0:08:05 lr 0.099994	data 0.0004 (0.0039)	batch 1.0783 (1.0696)	loss 3.5015 (24.3453)	grad_norm 2.3902 (17.0748)	mem 55575MB
Test: [100/196]	Time 0.485 (0.451)	Loss 3.6769 (2.5439)	Acc@1 22.656 (44.191)	Acc@5 51.562 (71.964)	Mem 55534MB
Test: [150/196]	Time 0.980 (0.417)	Loss 3.4100 (2.6723)	Acc@1 34.766 (42.974)	Acc@5 59.375 (69.940)	Mem 55534MB
 * Acc@1 42.626 Acc@5 69.172
Accuracy of the network on the 50000 test images: 42.63%
Max accuracy (after decay): 42.63%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [0/180][4600/5004]	eta 0:07:12 lr 0.099994	data 0.0003 (0.0038)	batch 1.0558 (1.0696)	loss 3.5799 (24.1201)	grad_norm 2.4774 (16.9153)	mem 55575MB
Train: [2/180][0/5004]	eta 23:38:44 lr 0.099970	data 16.0007 (16.0007)	batch 17.0113 (17.0113)	loss 2.4682 (2.4682)	grad_norm 1.6083 (1.6083)	mem 55534MB
Train: [0/180][4650/5004]	eta 0:06:18 lr 0.099993	data 0.0004 (0.0038)	batch 1.0555 (1.0695)	loss 3.8226 (23.8992)	grad_norm 2.4732 (16.7585)	mem 55575MB
Train: [2/180][50/5004]	eta 1:46:59 lr 0.099969	data 0.0004 (0.3141)	batch 0.9583 (1.2958)	loss 2.4556 (2.4015)	grad_norm 1.6141 (1.6270)	mem 55534MB
Train: [0/180][4700/5004]	eta 0:05:25 lr 0.099993	data 0.0003 (0.0037)	batch 1.0719 (1.0695)	loss 3.6709 (23.6829)	grad_norm 2.1975 (16.6056)	mem 55575MB
Train: [2/180][100/5004]	eta 1:33:03 lr 0.099969	data 0.0006 (0.1588)	batch 0.9635 (1.1386)	loss 2.2722 (2.3978)	grad_norm 1.5890 (1.6331)	mem 55534MB
Train: [0/180][4750/5004]	eta 0:04:31 lr 0.099993	data 0.0003 (0.0037)	batch 1.0727 (1.0694)	loss 3.3923 (23.4703)	grad_norm 2.3277 (16.4547)	mem 55575MB
Train: [2/180][150/5004]	eta 1:27:44 lr 0.099969	data 0.0004 (0.1063)	batch 0.9547 (1.0845)	loss 2.2974 (2.3862)	grad_norm 1.5859 (1.6192)	mem 55534MB
Train: [2/180][200/5004]	eta 1:24:41 lr 0.099968	data 0.0004 (0.0800)	batch 0.9746 (1.0579)	loss 2.1275 (2.3686)	grad_norm 1.4962 (1.6020)	mem 55534MB
Train: [0/180][4800/5004]	eta 0:03:38 lr 0.099993	data 0.0003 (0.0037)	batch 1.0581 (1.0694)	loss 3.6103 (23.2624)	grad_norm 2.3454 (16.3069)	mem 55575MB
Train: [2/180][250/5004]	eta 1:22:33 lr 0.099968	data 0.0003 (0.0641)	batch 0.9942 (1.0420)	loss 2.5109 (2.3539)	grad_norm 1.5538 (1.8424)	mem 55534MB
Train: [0/180][4850/5004]	eta 0:02:44 lr 0.099993	data 0.0002 (0.0036)	batch 1.0534 (1.0694)	loss 3.5578 (23.0583)	grad_norm 2.2800 (16.1620)	mem 55575MB
Train: [2/180][300/5004]	eta 1:20:49 lr 0.099968	data 0.0004 (0.0535)	batch 0.9674 (1.0309)	loss 2.3201 (2.3455)	grad_norm 1.5228 (1.7906)	mem 55534MB
Train: [0/180][4900/5004]	eta 0:01:51 lr 0.099993	data 0.0004 (0.0036)	batch 1.0550 (1.0693)	loss 3.4443 (22.8578)	grad_norm 2.2063 (16.0198)	mem 55575MB
Train: [2/180][350/5004]	eta 1:19:23 lr 0.099967	data 0.0003 (0.0459)	batch 0.9846 (1.0236)	loss 2.4376 (2.3389)	grad_norm 1.6195 (1.7638)	mem 55534MB
Train: [0/180][4950/5004]	eta 0:00:57 lr 0.099993	data 0.0003 (0.0036)	batch 1.0582 (1.0693)	loss 3.2984 (22.6612)	grad_norm 2.2058 (15.8804)	mem 55575MB
Train: [2/180][400/5004]	eta 1:18:05 lr 0.099967	data 0.0005 (0.0403)	batch 0.9676 (1.0178)	loss 2.1860 (2.3305)	grad_norm 1.5608 (1.7389)	mem 55534MB
Train: [0/180][5000/5004]	eta 0:00:04 lr 0.099992	data 0.0001 (0.0035)	batch 1.0504 (1.0692)	loss 3.2424 (22.4677)	grad_norm 2.2116 (15.7434)	mem 55575MB
Current slope: None 	
EPOCH 0 training takes 1:29:10
Test: [0/196]	Time 10.853 (10.853)	Loss 2.2695 (2.2695)	Acc@1 48.828 (48.828)	Acc@5 77.344 (77.344)	Mem 55575MB
Train: [2/180][450/5004]	eta 1:16:56 lr 0.099967	data 0.0005 (0.0358)	batch 0.9748 (1.0138)	loss 2.3889 (2.3228)	grad_norm 1.5785 (1.7138)	mem 55534MB
Test: [50/196]	Time 0.220 (0.539)	Loss 2.5065 (2.7833)	Acc@1 41.406 (39.017)	Acc@5 72.656 (66.406)	Mem 55575MB
Test: [100/196]	Time 0.711 (0.454)	Loss 4.6513 (3.0029)	Acc@1 8.984 (36.216)	Acc@5 28.516 (63.285)	Mem 55575MB
Test: [150/196]	Time 0.928 (0.419)	Loss 5.7700 (3.2020)	Acc@1 17.188 (34.248)	Acc@5 25.391 (59.882)	Mem 55575MB
Train: [2/180][500/5004]	eta 1:15:51 lr 0.099966	data 0.0004 (0.0323)	batch 0.9735 (1.0106)	loss 1.9813 (2.3123)	grad_norm 1.4744 (1.6943)	mem 55534MB
 * Acc@1 32.852 Acc@5 58.066
Accuracy of the network on the 50000 test images: 32.85%
Max accuracy (after decay): 32.85%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/5004]	eta 23:27:59 lr 0.099992	data 15.7229 (15.7229)	batch 16.8823 (16.8823)	loss 2.9660 (2.9660)	grad_norm 1.9933 (1.9933)	mem 55575MB
Train: [2/180][550/5004]	eta 1:14:47 lr 0.099966	data 0.0003 (0.0294)	batch 0.9770 (1.0075)	loss 2.0060 (2.3040)	grad_norm 1.3999 (1.6786)	mem 55534MB
Train: [1/180][50/5004]	eta 1:53:51 lr 0.099992	data 0.0004 (0.3086)	batch 1.0586 (1.3790)	loss 3.2589 (3.2662)	grad_norm 2.1872 (2.1601)	mem 55575MB
Train: [2/180][600/5004]	eta 1:13:45 lr 0.099966	data 0.0004 (0.0270)	batch 0.9734 (1.0050)	loss 2.2778 (2.3010)	grad_norm 1.5416 (2.1445)	mem 55534MB
Train: [1/180][100/5004]	eta 1:40:04 lr 0.099992	data 0.0003 (0.1560)	batch 1.0680 (1.2245)	loss 3.2396 (3.2939)	grad_norm 2.2954 (2.1798)	mem 55575MB
Train: [2/180][650/5004]	eta 1:12:47 lr 0.099965	data 0.0004 (0.0249)	batch 0.9726 (1.0031)	loss 2.3137 (2.2975)	grad_norm 1.5335 (2.0976)	mem 55534MB
Train: [1/180][150/5004]	eta 1:34:44 lr 0.099992	data 0.0003 (0.1045)	batch 1.0597 (1.1710)	loss 3.2389 (3.2715)	grad_norm 2.1087 (2.1624)	mem 55575MB
Train: [2/180][700/5004]	eta 1:11:48 lr 0.099965	data 0.0004 (0.0232)	batch 0.9997 (1.0011)	loss 2.2416 (2.2895)	grad_norm 1.5302 (2.0567)	mem 55534MB
Train: [1/180][200/5004]	eta 1:31:41 lr 0.099992	data 0.0004 (0.0786)	batch 1.0698 (1.1452)	loss 2.9944 (3.2439)	grad_norm 2.0521 (2.1353)	mem 55575MB
Train: [2/180][750/5004]	eta 1:10:52 lr 0.099965	data 0.0004 (0.0217)	batch 0.9662 (0.9996)	loss 2.3091 (2.2838)	grad_norm 1.5634 (2.0218)	mem 55534MB
Train: [1/180][250/5004]	eta 1:29:29 lr 0.099992	data 0.0003 (0.0630)	batch 1.0704 (1.1295)	loss 3.3797 (3.2408)	grad_norm 2.1449 (2.1291)	mem 55575MB
Train: [2/180][800/5004]	eta 1:09:57 lr 0.099964	data 0.0004 (0.0203)	batch 0.9775 (0.9985)	loss 2.1548 (2.2762)	grad_norm 1.5087 (1.9893)	mem 55534MB
Train: [2/180][850/5004]	eta 1:09:02 lr 0.099964	data 0.0003 (0.0192)	batch 0.9660 (0.9973)	loss 2.2537 (2.2747)	grad_norm 1.5876 (2.3252)	mem 55534MB
Train: [1/180][300/5004]	eta 1:27:44 lr 0.099991	data 0.0003 (0.0526)	batch 1.0552 (1.1191)	loss 3.0777 (3.2222)	grad_norm 2.0299 (2.1166)	mem 55575MB
Train: [2/180][900/5004]	eta 1:08:08 lr 0.099964	data 0.0003 (0.0181)	batch 0.9579 (0.9963)	loss 2.2384 (2.2686)	grad_norm 1.5240 (2.2808)	mem 55534MB
Train: [1/180][350/5004]	eta 1:26:11 lr 0.099991	data 0.0003 (0.0451)	batch 1.0560 (1.1111)	loss 3.0468 (3.2123)	grad_norm 2.0493 (2.1087)	mem 55575MB
Train: [2/180][950/5004]	eta 1:07:15 lr 0.099963	data 0.0003 (0.0172)	batch 0.9798 (0.9954)	loss 2.3763 (2.2616)	grad_norm 1.7095 (2.2399)	mem 55534MB
Train: [1/180][400/5004]	eta 1:24:49 lr 0.099991	data 0.0003 (0.0396)	batch 1.0618 (1.1053)	loss 2.9769 (3.1996)	grad_norm 2.0083 (2.1000)	mem 55575MB
Train: [2/180][1000/5004]	eta 1:06:21 lr 0.099963	data 0.0004 (0.0163)	batch 0.9610 (0.9944)	loss 2.0264 (2.2555)	grad_norm 1.5079 (2.2035)	mem 55534MB
Train: [1/180][450/5004]	eta 1:23:34 lr 0.099991	data 0.0003 (0.0353)	batch 1.0608 (1.1011)	loss 3.1097 (3.1879)	grad_norm 1.9745 (2.0887)	mem 55575MB
Train: [2/180][1050/5004]	eta 1:05:28 lr 0.099963	data 0.0003 (0.0156)	batch 0.9727 (0.9936)	loss 2.1518 (2.2479)	grad_norm 1.5258 (2.1707)	mem 55534MB
Train: [1/180][500/5004]	eta 1:22:23 lr 0.099991	data 0.0004 (0.0318)	batch 1.0610 (1.0976)	loss 3.0232 (3.1773)	grad_norm 1.8703 (2.0760)	mem 55575MB
Train: [2/180][1100/5004]	eta 1:04:35 lr 0.099962	data 0.0003 (0.0149)	batch 0.9915 (0.9928)	loss 2.1747 (2.2403)	grad_norm 1.5487 (2.1401)	mem 55534MB
Train: [1/180][550/5004]	eta 1:21:15 lr 0.099991	data 0.0003 (0.0289)	batch 1.0751 (1.0947)	loss 2.9575 (3.1606)	grad_norm 1.9662 (2.0636)	mem 55575MB
Train: [2/180][1150/5004]	eta 1:03:43 lr 0.099962	data 0.0004 (0.0143)	batch 0.9863 (0.9921)	loss 2.0293 (2.2340)	grad_norm 1.4524 (2.1114)	mem 55534MB
Train: [1/180][600/5004]	eta 1:20:10 lr 0.099990	data 0.0004 (0.0265)	batch 1.0716 (1.0924)	loss 3.0348 (3.1492)	grad_norm 2.0970 (2.0579)	mem 55575MB
Train: [2/180][1200/5004]	eta 1:02:51 lr 0.099962	data 0.0004 (0.0137)	batch 0.9608 (0.9914)	loss 2.2127 (2.2274)	grad_norm 1.5309 (2.0852)	mem 55534MB
Train: [1/180][650/5004]	eta 1:19:07 lr 0.099990	data 0.0003 (0.0245)	batch 1.0639 (1.0905)	loss 2.9144 (3.1394)	grad_norm 1.9080 (2.0500)	mem 55575MB
Train: [2/180][1250/5004]	eta 1:01:59 lr 0.099961	data 0.0003 (0.0131)	batch 1.0136 (0.9908)	loss 1.9008 (2.2209)	grad_norm 1.4963 (2.0611)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.1
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 1 day, 7:59:27 lr 0.100000	data 15.9024 (15.9024)	batch 23.0151 (23.0151)	loss 27.0408 (27.0408)	grad_norm 71.8521 (71.8521)	mem 55534MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/mengli/Data_raid/datasets/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_CKPT: /home/mengli/projects/wenxuanzeng/RePriv/mobilenetv2_140_ra-21a4e913.pth
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: ''
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.1
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.130632
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/5004]	eta 1 day, 7:08:23 lr 0.100000	data 16.0448 (16.0448)	batch 22.4028 (22.4028)	loss 6.9619 (6.9619)	grad_norm 1.3009 (1.3009)	mem 55534MB
Train: [0/180][50/5004]	eta 4:40:54 lr 0.100000	data 0.0003 (0.3150)	batch 3.0554 (3.4023)	loss 6.9512 (6.9774)	grad_norm 1.2404 (1.2868)	mem 55534MB
Train: [0/180][100/5004]	eta 4:22:44 lr 0.100000	data 0.0005 (0.1593)	batch 3.0931 (3.2146)	loss 6.8047 (6.9386)	grad_norm 1.3151 (1.2870)	mem 55534MB
Train: [0/180][150/5004]	eta 4:14:39 lr 0.100000	data 0.0004 (0.1067)	batch 3.0721 (3.1477)	loss 6.6667 (6.8832)	grad_norm 1.3269 (1.3050)	mem 55534MB
Train: [0/180][200/5004]	eta 4:09:15 lr 0.100000	data 0.0003 (0.0802)	batch 3.0022 (3.1131)	loss 6.6823 (6.8297)	grad_norm 1.2913 (1.3105)	mem 55534MB
Train: [0/180][250/5004]	eta 4:04:37 lr 0.100000	data 0.0003 (0.0643)	batch 3.0382 (3.0875)	loss 6.5369 (6.7756)	grad_norm 1.2826 (1.3121)	mem 55534MB
Train: [0/180][300/5004]	eta 4:01:04 lr 0.100000	data 0.0003 (0.0537)	batch 2.9908 (3.0750)	loss 6.5056 (6.7247)	grad_norm 1.3481 (1.3154)	mem 55534MB
Train: [0/180][350/5004]	eta 3:57:40 lr 0.100000	data 0.0003 (0.0461)	batch 3.0380 (3.0642)	loss 6.3786 (6.6764)	grad_norm 1.2941 (1.3129)	mem 55534MB
Train: [0/180][400/5004]	eta 3:54:28 lr 0.100000	data 0.0003 (0.0404)	batch 2.9901 (3.0557)	loss 6.2384 (6.6309)	grad_norm 1.2783 (1.3106)	mem 55534MB
Train: [0/180][450/5004]	eta 3:51:15 lr 0.100000	data 0.0003 (0.0360)	batch 2.8998 (3.0469)	loss 6.2211 (6.5868)	grad_norm 1.2389 (1.3077)	mem 55534MB
Train: [0/180][500/5004]	eta 3:48:12 lr 0.100000	data 0.0004 (0.0324)	batch 2.8761 (3.0400)	loss 6.1287 (6.5455)	grad_norm 1.2674 (1.3034)	mem 55534MB
Train: [0/180][550/5004]	eta 3:45:23 lr 0.100000	data 0.0003 (0.0295)	batch 2.9210 (3.0362)	loss 6.0577 (6.5076)	grad_norm 1.2625 (1.3000)	mem 55534MB
Train: [0/180][600/5004]	eta 3:42:25 lr 0.100000	data 0.0003 (0.0271)	batch 2.9886 (3.0302)	loss 6.2025 (6.4711)	grad_norm 1.2690 (1.2997)	mem 55534MB
Train: [0/180][650/5004]	eta 3:39:35 lr 0.100000	data 0.0003 (0.0250)	batch 2.9916 (3.0262)	loss 6.0952 (6.4354)	grad_norm 1.2928 (1.2974)	mem 55534MB
Train: [0/180][700/5004]	eta 3:36:53 lr 0.100000	data 0.0003 (0.0233)	batch 3.0733 (3.0237)	loss 5.9659 (6.4005)	grad_norm 1.2624 (1.2969)	mem 55534MB
Train: [0/180][750/5004]	eta 3:34:15 lr 0.100000	data 0.0003 (0.0217)	batch 2.9597 (3.0221)	loss 6.0432 (6.3698)	grad_norm 1.2555 (1.2954)	mem 55534MB
Train: [0/180][800/5004]	eta 3:31:30 lr 0.100000	data 0.0003 (0.0204)	batch 2.9682 (3.0187)	loss 5.7443 (6.3382)	grad_norm 1.2378 (1.2936)	mem 55534MB
Train: [0/180][850/5004]	eta 3:26:39 lr 0.100000	data 0.0003 (0.0192)	batch 3.0226 (2.9851)	loss 5.7811 (6.3093)	grad_norm 1.2459 (1.2925)	mem 55534MB
Train: [0/180][900/5004]	eta 3:24:13 lr 0.100000	data 0.0006 (0.0182)	batch 2.9358 (2.9857)	loss 5.7238 (6.2801)	grad_norm 1.2401 (1.2916)	mem 55534MB
Train: [0/180][950/5004]	eta 3:21:43 lr 0.100000	data 0.0004 (0.0173)	batch 3.0305 (2.9855)	loss 5.8518 (6.2520)	grad_norm 1.2597 (1.2907)	mem 55534MB
Train: [0/180][1000/5004]	eta 3:19:10 lr 0.100000	data 0.0003 (0.0164)	batch 2.9186 (2.9846)	loss 5.8074 (6.2237)	grad_norm 1.3038 (1.2907)	mem 55534MB
Train: [0/180][1050/5004]	eta 3:16:38 lr 0.100000	data 0.0004 (0.0157)	batch 2.9450 (2.9839)	loss 5.7034 (6.1968)	grad_norm 1.3208 (1.2908)	mem 55534MB
Train: [0/180][1100/5004]	eta 3:14:09 lr 0.100000	data 0.0004 (0.0150)	batch 3.0453 (2.9840)	loss 5.5105 (6.1711)	grad_norm 1.2815 (1.2901)	mem 55534MB
Train: [0/180][1150/5004]	eta 3:11:38 lr 0.100000	data 0.0003 (0.0143)	batch 3.0766 (2.9834)	loss 5.4386 (6.1463)	grad_norm 1.2698 (1.2899)	mem 55534MB
Train: [0/180][1200/5004]	eta 3:09:09 lr 0.100000	data 0.0003 (0.0137)	batch 3.1276 (2.9835)	loss 5.5071 (6.1216)	grad_norm 1.2683 (1.2903)	mem 55534MB
Train: [0/180][1250/5004]	eta 3:06:38 lr 0.100000	data 0.0003 (0.0132)	batch 2.9403 (2.9832)	loss 5.4256 (6.0975)	grad_norm 1.2845 (1.2899)	mem 55534MB
Train: [0/180][1300/5004]	eta 3:04:09 lr 0.099999	data 0.0003 (0.0127)	batch 2.9765 (2.9832)	loss 5.5911 (6.0737)	grad_norm 1.3127 (1.2905)	mem 55534MB
Train: [0/180][1350/5004]	eta 3:01:39 lr 0.099999	data 0.0004 (0.0123)	batch 3.0380 (2.9829)	loss 5.5309 (6.0502)	grad_norm 1.2826 (1.2907)	mem 55534MB
Train: [0/180][1400/5004]	eta 2:59:07 lr 0.099999	data 0.0004 (0.0118)	batch 2.9795 (2.9822)	loss 5.4960 (6.0270)	grad_norm 1.2928 (1.2917)	mem 55534MB
Train: [0/180][1450/5004]	eta 2:56:34 lr 0.099999	data 0.0004 (0.0114)	batch 3.0310 (2.9810)	loss 5.3006 (6.0046)	grad_norm 1.2929 (1.2916)	mem 55534MB
Train: [0/180][1500/5004]	eta 2:53:59 lr 0.099999	data 0.0003 (0.0111)	batch 2.8361 (2.9793)	loss 5.2862 (5.9826)	grad_norm 1.3202 (1.2921)	mem 55534MB
Train: [0/180][1550/5004]	eta 2:51:27 lr 0.099999	data 0.0004 (0.0107)	batch 2.8415 (2.9785)	loss 5.3399 (5.9618)	grad_norm 1.2873 (1.2922)	mem 55534MB
Train: [0/180][1600/5004]	eta 2:48:51 lr 0.099999	data 0.0004 (0.0104)	batch 2.9218 (2.9764)	loss 5.3148 (5.9407)	grad_norm 1.2701 (1.2927)	mem 55534MB
Train: [0/180][1650/5004]	eta 2:46:19 lr 0.099999	data 0.0003 (0.0101)	batch 2.9884 (2.9755)	loss 5.2121 (5.9196)	grad_norm 1.3073 (1.2929)	mem 55534MB
Train: [0/180][1700/5004]	eta 2:43:46 lr 0.099999	data 0.0003 (0.0098)	batch 2.8915 (2.9740)	loss 5.2167 (5.8992)	grad_norm 1.2529 (1.2933)	mem 55534MB
Train: [0/180][1750/5004]	eta 2:41:14 lr 0.099999	data 0.0003 (0.0096)	batch 2.9492 (2.9731)	loss 5.0661 (5.8797)	grad_norm 1.2788 (1.2936)	mem 55534MB
Train: [0/180][1800/5004]	eta 2:38:41 lr 0.099999	data 0.0003 (0.0093)	batch 2.7767 (2.9719)	loss 5.0909 (5.8602)	grad_norm 1.3380 (1.2938)	mem 55534MB
Train: [0/180][1850/5004]	eta 2:36:10 lr 0.099999	data 0.0003 (0.0091)	batch 3.0187 (2.9709)	loss 5.1301 (5.8408)	grad_norm 1.3225 (1.2942)	mem 55534MB
Train: [0/180][1900/5004]	eta 2:33:12 lr 0.099999	data 0.0004 (0.0088)	batch 2.3963 (2.9615)	loss 5.0130 (5.8215)	grad_norm 1.3270 (1.2947)	mem 55534MB
Train: [0/180][1950/5004]	eta 2:30:26 lr 0.099999	data 0.0003 (0.0086)	batch 3.0148 (2.9557)	loss 5.0180 (5.8025)	grad_norm 1.3207 (1.2949)	mem 55534MB
