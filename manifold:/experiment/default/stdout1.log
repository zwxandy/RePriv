Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: ''
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 13:55:45 lr 0.050000	data 19.0650 (19.0650)	batch 22.2745 (22.2745)	loss 26.1722 (26.1722)	grad_norm 78.1417 (78.1417)	mem 28676MB
Train: [0/180][50/10009]	eta 4:03:47 lr 0.050000	data 0.0005 (0.3745)	batch 1.0338 (1.4687)	loss 22.6965 (17.5224)	grad_norm 26.2484 (37.3264)	mem 28676MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: ''
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 17:22:25 lr 0.050000	data 20.3197 (20.3197)	batch 23.5134 (23.5134)	loss 26.1720 (26.1720)	grad_norm 78.1497 (78.1497)	mem 28677MB
Train: [0/180][50/10009]	eta 4:07:13 lr 0.050000	data 0.0005 (0.3991)	batch 1.0294 (1.4894)	loss 23.9042 (17.4893)	grad_norm 54.3308 (37.7026)	mem 28677MB
Train: [0/180][100/10009]	eta 3:29:02 lr 0.050000	data 0.0005 (0.2018)	batch 1.0176 (1.2657)	loss 32.0689 (21.5857)	grad_norm 36.6223 (40.6974)	mem 28677MB
Train: [0/180][150/10009]	eta 3:15:36 lr 0.050000	data 0.0004 (0.1351)	batch 1.0208 (1.1904)	loss 33.8439 (25.2545)	grad_norm 65.6065 (42.6321)	mem 28677MB
Train: [0/180][200/10009]	eta 3:08:26 lr 0.050000	data 0.0004 (0.1017)	batch 1.0483 (1.1527)	loss 35.9788 (27.6283)	grad_norm 49.3241 (43.7951)	mem 28677MB
Train: [0/180][250/10009]	eta 3:03:35 lr 0.050000	data 0.0005 (0.0815)	batch 1.0408 (1.1288)	loss 40.5244 (29.9999)	grad_norm 74.6959 (45.2115)	mem 28677MB
Train: [0/180][300/10009]	eta 3:00:23 lr 0.050000	data 0.0006 (0.0681)	batch 1.0353 (1.1148)	loss 31.9914 (31.6943)	grad_norm 38.4567 (45.5874)	mem 28677MB
Train: [0/180][350/10009]	eta 2:59:05 lr 0.050000	data 0.0058 (0.0585)	batch 1.2894 (1.1124)	loss 45.7168 (32.9790)	grad_norm 26.5322 (46.7129)	mem 28677MB
Train: [0/180][400/10009]	eta 3:01:44 lr 0.050000	data 0.0016 (0.0513)	batch 1.2909 (1.1349)	loss 45.4703 (34.2171)	grad_norm 26.6656 (47.6775)	mem 28677MB
Train: [0/180][450/10009]	eta 3:03:31 lr 0.050000	data 0.0004 (0.0458)	batch 1.2847 (1.1520)	loss 31.1468 (35.0163)	grad_norm 48.9501 (48.5405)	mem 28677MB
Train: [0/180][500/10009]	eta 3:04:45 lr 0.050000	data 0.0005 (0.0414)	batch 1.2690 (1.1658)	loss 39.6054 (35.6517)	grad_norm 31.4895 (49.2337)	mem 28677MB
Train: [0/180][550/10009]	eta 3:05:33 lr 0.050000	data 0.0005 (0.0378)	batch 1.2567 (1.1770)	loss 47.9599 (36.0357)	grad_norm 54.2383 (49.2323)	mem 28677MB
Train: [0/180][600/10009]	eta 3:06:04 lr 0.050000	data 0.0003 (0.0348)	batch 1.2921 (1.1866)	loss 29.5946 (36.3325)	grad_norm 30.5565 (49.5357)	mem 28677MB
Train: [0/180][650/10009]	eta 3:06:21 lr 0.050000	data 0.0004 (0.0322)	batch 1.3286 (1.1948)	loss 43.4999 (36.7158)	grad_norm 38.9715 (49.2719)	mem 28677MB
Train: [0/180][700/10009]	eta 3:06:27 lr 0.050000	data 0.0012 (0.0300)	batch 1.2724 (1.2018)	loss 31.2479 (36.9284)	grad_norm 69.9298 (49.0068)	mem 28677MB
Train: [0/180][750/10009]	eta 3:06:25 lr 0.050000	data 0.0014 (0.0281)	batch 1.2922 (1.2080)	loss 37.1795 (37.0176)	grad_norm 32.2837 (48.7604)	mem 28677MB
Train: [0/180][800/10009]	eta 3:06:13 lr 0.050000	data 0.0004 (0.0265)	batch 1.2643 (1.2134)	loss 36.4087 (37.0829)	grad_norm 44.5989 (48.3748)	mem 28677MB
Train: [0/180][850/10009]	eta 3:05:55 lr 0.050000	data 0.0051 (0.0250)	batch 1.3093 (1.2180)	loss 40.2347 (36.9490)	grad_norm 24.8080 (47.9724)	mem 28677MB
Train: [0/180][900/10009]	eta 3:05:35 lr 0.050000	data 0.0005 (0.0237)	batch 1.3009 (1.2224)	loss 29.8079 (36.8836)	grad_norm 33.5783 (47.4510)	mem 28677MB
Train: [0/180][950/10009]	eta 3:05:10 lr 0.050000	data 0.0006 (0.0226)	batch 1.2815 (1.2265)	loss 30.2129 (36.7359)	grad_norm 47.1180 (47.1328)	mem 28677MB
Train: [0/180][1000/10009]	eta 3:04:38 lr 0.050000	data 0.0004 (0.0215)	batch 1.3043 (1.2297)	loss 26.5026 (36.5959)	grad_norm 22.8952 (46.9796)	mem 28677MB
Train: [0/180][1050/10009]	eta 3:04:05 lr 0.050000	data 0.0012 (0.0206)	batch 1.2858 (1.2329)	loss 28.2497 (36.3468)	grad_norm 22.9636 (46.3979)	mem 28677MB
Train: [0/180][1100/10009]	eta 3:03:28 lr 0.050000	data 0.0005 (0.0197)	batch 1.2993 (1.2356)	loss 27.1655 (36.0150)	grad_norm 40.1273 (45.7431)	mem 28677MB
Train: [0/180][1150/10009]	eta 3:02:50 lr 0.050000	data 0.0004 (0.0189)	batch 1.3145 (1.2383)	loss 32.6995 (35.6665)	grad_norm 37.2550 (45.0910)	mem 28677MB
Train: [0/180][1200/10009]	eta 3:02:09 lr 0.050000	data 0.0005 (0.0182)	batch 1.3048 (1.2407)	loss 25.9321 (35.3227)	grad_norm 26.0116 (44.6838)	mem 28677MB
Train: [0/180][1250/10009]	eta 3:01:30 lr 0.050000	data 0.0004 (0.0176)	batch 1.3731 (1.2433)	loss 24.7411 (34.9740)	grad_norm 31.4001 (44.0175)	mem 28677MB
Train: [0/180][1300/10009]	eta 3:00:45 lr 0.050000	data 0.0052 (0.0170)	batch 1.3342 (1.2453)	loss 21.6029 (34.5268)	grad_norm 16.8509 (43.4465)	mem 28677MB
Train: [0/180][1350/10009]	eta 2:59:59 lr 0.050000	data 0.0027 (0.0164)	batch 1.3479 (1.2472)	loss 22.5051 (34.1406)	grad_norm 17.8034 (42.9394)	mem 28677MB
Train: [0/180][1400/10009]	eta 2:59:13 lr 0.050000	data 0.0006 (0.0159)	batch 1.2746 (1.2491)	loss 20.4150 (33.7481)	grad_norm 41.4397 (42.4591)	mem 28677MB
Train: [0/180][1450/10009]	eta 2:58:22 lr 0.050000	data 0.0005 (0.0154)	batch 1.2868 (1.2504)	loss 26.6200 (33.4094)	grad_norm 33.8859 (41.9816)	mem 28677MB
Train: [0/180][1500/10009]	eta 2:57:26 lr 0.050000	data 0.0004 (0.0149)	batch 1.2518 (1.2512)	loss 19.7975 (33.0462)	grad_norm 20.8121 (41.4876)	mem 28677MB
Train: [0/180][1550/10009]	eta 2:56:08 lr 0.050000	data 0.0054 (0.0145)	batch 1.1855 (1.2494)	loss 22.2768 (32.7413)	grad_norm 20.6534 (41.0755)	mem 28677MB
Train: [0/180][1600/10009]	eta 2:54:46 lr 0.050000	data 0.0006 (0.0141)	batch 1.0376 (1.2470)	loss 24.1764 (32.4204)	grad_norm 17.7026 (40.6721)	mem 28677MB
Train: [0/180][1650/10009]	eta 2:52:51 lr 0.050000	data 0.0004 (0.0137)	batch 1.0436 (1.2407)	loss 23.6794 (32.0986)	grad_norm 21.5382 (40.1914)	mem 28677MB
Train: [0/180][1700/10009]	eta 2:51:00 lr 0.050000	data 0.0004 (0.0133)	batch 1.0416 (1.2348)	loss 21.1030 (31.8096)	grad_norm 26.8903 (39.8893)	mem 28677MB
Train: [0/180][1750/10009]	eta 2:49:13 lr 0.050000	data 0.0005 (0.0130)	batch 1.0334 (1.2294)	loss 24.5922 (31.5391)	grad_norm 88.8516 (39.5075)	mem 28677MB
Train: [0/180][1800/10009]	eta 2:47:28 lr 0.050000	data 0.0005 (0.0126)	batch 1.0387 (1.2241)	loss 21.0945 (31.3003)	grad_norm 15.9459 (39.1702)	mem 28677MB
Train: [0/180][1850/10009]	eta 2:45:46 lr 0.050000	data 0.0004 (0.0123)	batch 1.0389 (1.2191)	loss 20.8079 (31.0245)	grad_norm 26.1726 (38.7757)	mem 28677MB
Train: [0/180][1900/10009]	eta 2:44:06 lr 0.050000	data 0.0004 (0.0120)	batch 1.0407 (1.2143)	loss 23.1898 (30.7481)	grad_norm 36.3080 (38.4363)	mem 28677MB
Train: [0/180][1950/10009]	eta 2:42:28 lr 0.050000	data 0.0005 (0.0117)	batch 1.0355 (1.2097)	loss 22.2302 (30.5005)	grad_norm 14.6812 (38.0586)	mem 28677MB
Train: [0/180][2000/10009]	eta 2:40:53 lr 0.050000	data 0.0005 (0.0114)	batch 1.0392 (1.2053)	loss 16.5343 (30.2219)	grad_norm 18.8989 (37.6745)	mem 28677MB
Train: [0/180][2050/10009]	eta 2:39:20 lr 0.050000	data 0.0005 (0.0112)	batch 1.0142 (1.2012)	loss 15.3817 (29.9344)	grad_norm 18.5148 (37.3947)	mem 28677MB
Train: [0/180][2100/10009]	eta 2:37:49 lr 0.050000	data 0.0006 (0.0109)	batch 1.0439 (1.1974)	loss 17.9598 (29.6726)	grad_norm 18.3932 (37.0510)	mem 28677MB
Train: [0/180][2150/10009]	eta 2:36:20 lr 0.050000	data 0.0005 (0.0107)	batch 1.0363 (1.1936)	loss 22.4334 (29.4287)	grad_norm 41.4803 (36.7461)	mem 28677MB
Train: [0/180][2200/10009]	eta 2:34:52 lr 0.050000	data 0.0004 (0.0104)	batch 1.0421 (1.1900)	loss 16.2568 (29.1806)	grad_norm 19.9805 (36.4083)	mem 28677MB
Train: [0/180][2250/10009]	eta 2:33:27 lr 0.050000	data 0.0004 (0.0102)	batch 1.0388 (1.1866)	loss 20.4940 (28.9294)	grad_norm 44.3071 (36.0835)	mem 28677MB
Train: [0/180][2300/10009]	eta 2:32:01 lr 0.050000	data 0.0004 (0.0100)	batch 1.0169 (1.1833)	loss 16.2100 (28.6906)	grad_norm 25.7675 (35.7843)	mem 28677MB
Train: [0/180][2350/10009]	eta 2:30:38 lr 0.050000	data 0.0005 (0.0098)	batch 1.0157 (1.1802)	loss 21.3760 (28.4612)	grad_norm 40.9848 (35.4696)	mem 28677MB
Train: [0/180][2400/10009]	eta 2:29:17 lr 0.050000	data 0.0004 (0.0096)	batch 1.0405 (1.1772)	loss 15.1745 (28.2364)	grad_norm 14.3174 (35.1835)	mem 28677MB
Train: [0/180][2450/10009]	eta 2:27:57 lr 0.050000	data 0.0005 (0.0094)	batch 1.0386 (1.1745)	loss 18.5904 (27.9916)	grad_norm 16.2827 (34.8475)	mem 28677MB
Train: [0/180][2500/10009]	eta 2:26:37 lr 0.050000	data 0.0004 (0.0092)	batch 1.0299 (1.1717)	loss 14.0887 (27.7464)	grad_norm 23.0063 (34.5554)	mem 28677MB
Train: [0/180][2550/10009]	eta 2:25:19 lr 0.050000	data 0.0005 (0.0091)	batch 1.0290 (1.1690)	loss 15.7568 (27.5063)	grad_norm 18.7405 (34.2637)	mem 28677MB
Train: [0/180][2600/10009]	eta 2:24:01 lr 0.050000	data 0.0004 (0.0089)	batch 1.0422 (1.1664)	loss 16.8186 (27.2792)	grad_norm 16.6622 (33.9862)	mem 28677MB
Train: [0/180][2650/10009]	eta 2:22:44 lr 0.050000	data 0.0005 (0.0087)	batch 1.0314 (1.1639)	loss 13.0744 (27.0574)	grad_norm 21.7009 (33.6748)	mem 28677MB
Train: [0/180][2700/10009]	eta 2:21:40 lr 0.050000	data 0.0055 (0.0086)	batch 1.1813 (1.1630)	loss 14.9468 (26.8327)	grad_norm 14.0585 (33.3750)	mem 28677MB
Train: [0/180][2750/10009]	eta 2:20:43 lr 0.050000	data 0.0009 (0.0085)	batch 1.0215 (1.1632)	loss 16.6389 (26.6248)	grad_norm 42.5881 (33.1574)	mem 28677MB
Train: [0/180][2800/10009]	eta 2:19:29 lr 0.050000	data 0.0006 (0.0083)	batch 1.0472 (1.1610)	loss 16.0162 (26.4206)	grad_norm 12.7362 (32.8820)	mem 28677MB
Train: [0/180][2850/10009]	eta 2:18:15 lr 0.050000	data 0.0004 (0.0082)	batch 1.0465 (1.1588)	loss 13.4939 (26.2108)	grad_norm 21.9609 (32.6208)	mem 28677MB
Train: [0/180][2900/10009]	eta 2:17:02 lr 0.050000	data 0.0004 (0.0081)	batch 1.0343 (1.1567)	loss 15.5014 (25.9978)	grad_norm 22.9508 (32.3765)	mem 28677MB
Train: [0/180][2950/10009]	eta 2:15:50 lr 0.050000	data 0.0005 (0.0079)	batch 1.0625 (1.1547)	loss 11.9602 (25.7987)	grad_norm 15.0098 (32.1203)	mem 28677MB
Train: [0/180][3000/10009]	eta 2:14:39 lr 0.050000	data 0.0004 (0.0078)	batch 1.0438 (1.1527)	loss 11.8010 (25.5960)	grad_norm 34.5644 (31.8604)	mem 28677MB
Train: [0/180][3050/10009]	eta 2:13:28 lr 0.050000	data 0.0005 (0.0077)	batch 1.0151 (1.1508)	loss 11.5268 (25.3884)	grad_norm 13.1229 (31.6036)	mem 28677MB
Train: [0/180][3100/10009]	eta 2:12:17 lr 0.050000	data 0.0004 (0.0076)	batch 1.0369 (1.1489)	loss 10.2654 (25.1825)	grad_norm 15.7089 (31.3425)	mem 28677MB
Train: [0/180][3150/10009]	eta 2:11:07 lr 0.050000	data 0.0005 (0.0075)	batch 1.0359 (1.1471)	loss 12.8388 (24.9822)	grad_norm 13.2563 (31.1040)	mem 28677MB
Train: [0/180][3200/10009]	eta 2:09:58 lr 0.050000	data 0.0004 (0.0074)	batch 1.0292 (1.1453)	loss 11.6420 (24.7852)	grad_norm 13.0619 (30.8527)	mem 28677MB
Train: [0/180][3250/10009]	eta 2:08:49 lr 0.050000	data 0.0005 (0.0073)	batch 1.0442 (1.1436)	loss 10.9436 (24.5869)	grad_norm 14.1952 (30.5991)	mem 28677MB
Train: [0/180][3300/10009]	eta 2:07:41 lr 0.050000	data 0.0005 (0.0072)	batch 1.0291 (1.1419)	loss 12.4921 (24.3905)	grad_norm 12.4182 (30.3650)	mem 28677MB
Train: [0/180][3350/10009]	eta 2:06:33 lr 0.050000	data 0.0004 (0.0071)	batch 1.0329 (1.1403)	loss 12.0937 (24.1905)	grad_norm 10.9062 (30.1293)	mem 28677MB
Train: [0/180][3400/10009]	eta 2:05:26 lr 0.050000	data 0.0004 (0.0070)	batch 1.0394 (1.1388)	loss 9.7002 (23.9881)	grad_norm 14.5174 (29.8859)	mem 28677MB
Train: [0/180][3450/10009]	eta 2:04:19 lr 0.050000	data 0.0004 (0.0069)	batch 1.0321 (1.1373)	loss 10.1205 (23.7977)	grad_norm 10.9847 (29.6659)	mem 28677MB
Train: [0/180][3500/10009]	eta 2:03:13 lr 0.050000	data 0.0006 (0.0068)	batch 1.0361 (1.1359)	loss 8.5429 (23.6085)	grad_norm 15.2432 (29.4289)	mem 28677MB
Train: [0/180][3550/10009]	eta 2:02:08 lr 0.050000	data 0.0005 (0.0067)	batch 1.0429 (1.1346)	loss 9.0949 (23.4128)	grad_norm 10.1579 (29.1935)	mem 28677MB
Train: [0/180][3600/10009]	eta 2:01:02 lr 0.050000	data 0.0004 (0.0066)	batch 1.0388 (1.1332)	loss 7.6703 (23.2166)	grad_norm 11.8564 (28.9603)	mem 28677MB
Train: [0/180][3650/10009]	eta 1:59:57 lr 0.049999	data 0.0004 (0.0065)	batch 1.0225 (1.1319)	loss 8.4773 (23.0193)	grad_norm 11.1737 (28.7339)	mem 28677MB
Train: [0/180][3700/10009]	eta 1:58:52 lr 0.049999	data 0.0004 (0.0064)	batch 1.0415 (1.1306)	loss 8.8496 (22.8254)	grad_norm 18.0068 (28.5061)	mem 28677MB
Train: [0/180][3750/10009]	eta 1:57:48 lr 0.049999	data 0.0005 (0.0064)	batch 1.0116 (1.1293)	loss 8.1304 (22.6353)	grad_norm 10.5890 (28.2828)	mem 28677MB
Train: [0/180][3800/10009]	eta 1:56:44 lr 0.049999	data 0.0005 (0.0063)	batch 1.0451 (1.1281)	loss 9.0103 (22.4468)	grad_norm 11.1469 (28.0623)	mem 28677MB
Train: [0/180][3850/10009]	eta 1:55:40 lr 0.049999	data 0.0005 (0.0062)	batch 1.0320 (1.1269)	loss 7.0636 (22.2571)	grad_norm 10.1090 (27.8469)	mem 28677MB
Train: [0/180][3900/10009]	eta 1:54:36 lr 0.049999	data 0.0005 (0.0061)	batch 1.0408 (1.1257)	loss 7.5452 (22.0719)	grad_norm 10.4509 (27.6343)	mem 28677MB
Train: [0/180][3950/10009]	eta 1:53:33 lr 0.049999	data 0.0004 (0.0061)	batch 1.0173 (1.1246)	loss 8.6493 (21.8931)	grad_norm 10.6735 (27.4250)	mem 28677MB
Train: [0/180][4000/10009]	eta 1:52:31 lr 0.049999	data 0.0005 (0.0060)	batch 1.0386 (1.1236)	loss 7.3086 (21.7143)	grad_norm 13.4759 (27.2164)	mem 28677MB
Train: [0/180][4050/10009]	eta 1:51:28 lr 0.049999	data 0.0007 (0.0059)	batch 1.0418 (1.1225)	loss 7.1078 (21.5359)	grad_norm 10.3078 (27.0100)	mem 28677MB
Train: [0/180][4100/10009]	eta 1:50:26 lr 0.049999	data 0.0004 (0.0059)	batch 1.0409 (1.1215)	loss 7.0322 (21.3612)	grad_norm 9.2433 (26.8066)	mem 28677MB
Train: [0/180][4150/10009]	eta 1:49:25 lr 0.049999	data 0.0006 (0.0058)	batch 1.0444 (1.1206)	loss 6.7543 (21.1865)	grad_norm 9.6214 (26.6060)	mem 28677MB
Train: [0/180][4200/10009]	eta 1:48:23 lr 0.049999	data 0.0004 (0.0057)	batch 1.0333 (1.1196)	loss 6.5029 (21.0160)	grad_norm 8.9380 (26.4069)	mem 28677MB
Train: [0/180][4250/10009]	eta 1:47:21 lr 0.049999	data 0.0005 (0.0057)	batch 1.0317 (1.1186)	loss 5.9934 (20.8469)	grad_norm 8.5313 (26.2113)	mem 28677MB
Train: [0/180][4300/10009]	eta 1:46:20 lr 0.049999	data 0.0004 (0.0056)	batch 1.0381 (1.1176)	loss 6.4441 (20.6823)	grad_norm 9.7145 (26.0185)	mem 28677MB
Train: [0/180][4350/10009]	eta 1:45:18 lr 0.049999	data 0.0005 (0.0056)	batch 1.0248 (1.1166)	loss 5.9266 (20.5204)	grad_norm 8.4864 (25.8319)	mem 28677MB
Train: [0/180][4400/10009]	eta 1:44:17 lr 0.049999	data 0.0004 (0.0055)	batch 1.0467 (1.1157)	loss 6.4227 (20.3605)	grad_norm 9.9220 (25.6449)	mem 28677MB
Train: [0/180][4450/10009]	eta 1:43:17 lr 0.049999	data 0.0004 (0.0054)	batch 1.0419 (1.1149)	loss 5.7219 (20.2033)	grad_norm 8.3020 (25.4611)	mem 28677MB
Train: [0/180][4500/10009]	eta 1:42:18 lr 0.049999	data 0.0005 (0.0054)	batch 1.0659 (1.1142)	loss 5.9625 (20.0490)	grad_norm 8.7237 (25.2798)	mem 28677MB
Train: [0/180][4550/10009]	eta 1:41:18 lr 0.049999	data 0.0005 (0.0053)	batch 1.0360 (1.1135)	loss 6.2640 (19.8969)	grad_norm 9.4776 (25.1019)	mem 28677MB
Train: [0/180][4600/10009]	eta 1:40:18 lr 0.049999	data 0.0005 (0.0053)	batch 1.1232 (1.1126)	loss 6.4229 (19.7476)	grad_norm 8.3971 (24.9260)	mem 28677MB
Train: [0/180][4650/10009]	eta 1:39:27 lr 0.049999	data 0.0003 (0.0053)	batch 1.2663 (1.1135)	loss 5.9663 (19.6031)	grad_norm 8.4283 (24.7581)	mem 28677MB
Train: [0/180][4700/10009]	eta 1:38:30 lr 0.049999	data 0.0008 (0.0052)	batch 1.0357 (1.1133)	loss 6.0173 (19.4605)	grad_norm 7.9572 (24.5918)	mem 28677MB
Train: [0/180][4750/10009]	eta 1:37:31 lr 0.049999	data 0.0004 (0.0052)	batch 1.0692 (1.1126)	loss 5.6025 (19.3197)	grad_norm 8.2403 (24.4248)	mem 28677MB
Train: [0/180][4800/10009]	eta 1:36:33 lr 0.049999	data 0.0004 (0.0051)	batch 1.0684 (1.1121)	loss 5.8418 (19.1808)	grad_norm 8.1555 (24.2593)	mem 28677MB
Train: [0/180][4850/10009]	eta 1:35:35 lr 0.049999	data 0.0005 (0.0051)	batch 1.0655 (1.1117)	loss 5.9239 (19.0442)	grad_norm 8.3971 (24.0971)	mem 28677MB
Train: [0/180][4900/10009]	eta 1:34:35 lr 0.049999	data 0.0004 (0.0050)	batch 1.0339 (1.1109)	loss 5.9782 (18.9089)	grad_norm 7.8947 (23.9364)	mem 28677MB
Train: [0/180][4950/10009]	eta 1:33:36 lr 0.049999	data 0.0005 (0.0050)	batch 1.0312 (1.1102)	loss 5.3480 (18.7771)	grad_norm 7.6386 (23.7785)	mem 28677MB
Train: [0/180][5000/10009]	eta 1:32:43 lr 0.049999	data 0.0053 (0.0049)	batch 1.3654 (1.1108)	loss 6.1407 (18.6477)	grad_norm 7.7629 (23.6227)	mem 28677MB
Train: [0/180][5050/10009]	eta 1:31:49 lr 0.049999	data 0.0004 (0.0049)	batch 1.0341 (1.1110)	loss 6.0531 (18.5202)	grad_norm 8.5966 (23.4676)	mem 28677MB
Train: [0/180][5100/10009]	eta 1:30:50 lr 0.049999	data 0.0005 (0.0049)	batch 1.0400 (1.1103)	loss 5.7060 (18.3940)	grad_norm 7.7798 (23.3154)	mem 28677MB
Train: [0/180][5150/10009]	eta 1:29:51 lr 0.049999	data 0.0006 (0.0048)	batch 1.0401 (1.1096)	loss 5.8939 (18.2693)	grad_norm 7.6640 (23.1641)	mem 28677MB
Train: [0/180][5200/10009]	eta 1:28:52 lr 0.049999	data 0.0004 (0.0048)	batch 1.0244 (1.1089)	loss 5.8694 (18.1474)	grad_norm 8.1689 (23.0149)	mem 28677MB
Train: [0/180][5250/10009]	eta 1:27:53 lr 0.049999	data 0.0005 (0.0047)	batch 1.0255 (1.1082)	loss 5.4548 (18.0271)	grad_norm 7.2993 (22.8683)	mem 28677MB
Train: [0/180][5300/10009]	eta 1:26:55 lr 0.049999	data 0.0005 (0.0047)	batch 1.0359 (1.1075)	loss 5.2795 (17.9093)	grad_norm 7.5546 (22.7243)	mem 28677MB
Train: [0/180][5350/10009]	eta 1:25:56 lr 0.049999	data 0.0005 (0.0047)	batch 1.0400 (1.1068)	loss 5.5322 (17.7930)	grad_norm 7.0362 (22.5811)	mem 28677MB
Train: [0/180][5400/10009]	eta 1:24:58 lr 0.049999	data 0.0005 (0.0046)	batch 1.0384 (1.1062)	loss 5.6961 (17.6776)	grad_norm 7.4639 (22.4393)	mem 28677MB
Train: [0/180][5450/10009]	eta 1:24:00 lr 0.049999	data 0.0005 (0.0046)	batch 1.0290 (1.1055)	loss 5.0766 (17.5649)	grad_norm 6.9553 (22.3001)	mem 28677MB
Train: [0/180][5500/10009]	eta 1:23:01 lr 0.049999	data 0.0005 (0.0045)	batch 1.0425 (1.1049)	loss 5.1190 (17.4539)	grad_norm 7.1933 (22.1634)	mem 28677MB
Train: [0/180][5550/10009]	eta 1:22:03 lr 0.049999	data 0.0005 (0.0045)	batch 1.0370 (1.1042)	loss 5.3234 (17.3445)	grad_norm 6.8289 (22.0273)	mem 28677MB
Train: [0/180][5600/10009]	eta 1:21:06 lr 0.049999	data 0.0005 (0.0045)	batch 1.0362 (1.1037)	loss 5.3843 (17.2372)	grad_norm 6.7473 (21.8945)	mem 28677MB
Train: [0/180][5650/10009]	eta 1:20:08 lr 0.049999	data 0.0005 (0.0044)	batch 1.0359 (1.1031)	loss 5.1445 (17.1319)	grad_norm 6.5336 (21.7620)	mem 28677MB
Train: [0/180][5700/10009]	eta 1:19:10 lr 0.049999	data 0.0005 (0.0044)	batch 1.0297 (1.1025)	loss 4.7930 (17.0270)	grad_norm 6.2673 (21.6299)	mem 28677MB
Train: [0/180][5750/10009]	eta 1:18:12 lr 0.049999	data 0.0004 (0.0044)	batch 1.0382 (1.1019)	loss 5.1624 (16.9236)	grad_norm 6.3837 (21.4992)	mem 28677MB
Train: [0/180][5800/10009]	eta 1:17:15 lr 0.049999	data 0.0005 (0.0043)	batch 1.0603 (1.1014)	loss 5.0352 (16.8217)	grad_norm 6.7784 (21.3710)	mem 28677MB
Train: [0/180][5850/10009]	eta 1:16:18 lr 0.049999	data 0.0005 (0.0043)	batch 1.0284 (1.1010)	loss 5.0420 (16.7217)	grad_norm 6.3031 (21.2440)	mem 28677MB
Train: [0/180][5900/10009]	eta 1:15:21 lr 0.049999	data 0.0005 (0.0043)	batch 1.0372 (1.1004)	loss 5.2939 (16.6225)	grad_norm 6.9294 (21.1186)	mem 28677MB
Train: [0/180][5950/10009]	eta 1:14:24 lr 0.049999	data 0.0005 (0.0042)	batch 1.0412 (1.0999)	loss 4.9263 (16.5250)	grad_norm 6.1857 (20.9946)	mem 28677MB
Train: [0/180][6000/10009]	eta 1:13:27 lr 0.049999	data 0.0005 (0.0042)	batch 1.0347 (1.0993)	loss 5.4968 (16.4292)	grad_norm 6.5328 (20.8722)	mem 28677MB
Train: [0/180][6050/10009]	eta 1:12:30 lr 0.049999	data 0.0004 (0.0042)	batch 1.0456 (1.0988)	loss 4.9247 (16.3339)	grad_norm 6.4344 (20.7511)	mem 28677MB
Train: [0/180][6100/10009]	eta 1:11:33 lr 0.049999	data 0.0005 (0.0041)	batch 1.0462 (1.0983)	loss 5.0066 (16.2402)	grad_norm 6.1791 (20.6305)	mem 28677MB
Train: [0/180][6150/10009]	eta 1:10:36 lr 0.049999	data 0.0004 (0.0041)	batch 1.0412 (1.0978)	loss 5.0987 (16.1477)	grad_norm 6.0178 (20.5129)	mem 28677MB
Train: [0/180][6200/10009]	eta 1:09:39 lr 0.049999	data 0.0005 (0.0041)	batch 1.0300 (1.0973)	loss 4.9350 (16.0567)	grad_norm 5.6722 (20.3959)	mem 28677MB
Train: [0/180][6250/10009]	eta 1:08:42 lr 0.049999	data 0.0004 (0.0041)	batch 1.0142 (1.0968)	loss 5.2175 (15.9666)	grad_norm 6.0786 (20.2802)	mem 28677MB
Train: [0/180][6300/10009]	eta 1:07:46 lr 0.049998	data 0.0005 (0.0040)	batch 1.0333 (1.0963)	loss 4.6955 (15.8779)	grad_norm 6.0116 (20.1656)	mem 28677MB
Train: [0/180][6350/10009]	eta 1:06:49 lr 0.049998	data 0.0005 (0.0040)	batch 1.0285 (1.0958)	loss 4.8535 (15.7901)	grad_norm 5.7383 (20.0522)	mem 28677MB
Train: [0/180][6400/10009]	eta 1:05:53 lr 0.049998	data 0.0005 (0.0040)	batch 1.0509 (1.0953)	loss 4.7642 (15.7032)	grad_norm 5.5056 (19.9397)	mem 28677MB
Train: [0/180][6450/10009]	eta 1:04:56 lr 0.049998	data 0.0005 (0.0039)	batch 1.0332 (1.0949)	loss 5.1862 (15.6179)	grad_norm 5.8818 (19.8288)	mem 28677MB
Train: [0/180][6500/10009]	eta 1:04:00 lr 0.049998	data 0.0005 (0.0039)	batch 1.0385 (1.0944)	loss 4.4674 (15.5339)	grad_norm 5.4612 (19.7191)	mem 28677MB
Train: [0/180][6550/10009]	eta 1:03:04 lr 0.049998	data 0.0004 (0.0039)	batch 1.0345 (1.0940)	loss 4.5711 (15.4502)	grad_norm 5.5479 (19.6109)	mem 28677MB
Train: [0/180][6600/10009]	eta 1:02:08 lr 0.049998	data 0.0005 (0.0039)	batch 1.0411 (1.0936)	loss 4.3750 (15.3678)	grad_norm 5.2798 (19.5045)	mem 28677MB
Train: [0/180][6650/10009]	eta 1:01:12 lr 0.049998	data 0.0004 (0.0038)	batch 1.0337 (1.0933)	loss 4.6633 (15.2864)	grad_norm 5.3975 (19.3989)	mem 28677MB
Train: [0/180][6700/10009]	eta 1:00:16 lr 0.049998	data 0.0005 (0.0038)	batch 1.0358 (1.0929)	loss 4.1598 (15.2058)	grad_norm 5.4677 (19.2944)	mem 28677MB
Train: [0/180][6750/10009]	eta 0:59:20 lr 0.049998	data 0.0004 (0.0038)	batch 1.0423 (1.0925)	loss 4.4225 (15.1266)	grad_norm 5.2470 (19.1914)	mem 28677MB
Train: [0/180][6800/10009]	eta 0:58:24 lr 0.049998	data 0.0005 (0.0038)	batch 1.0378 (1.0921)	loss 4.4015 (15.0481)	grad_norm 5.3481 (19.0894)	mem 28677MB
Train: [0/180][6850/10009]	eta 0:57:28 lr 0.049998	data 0.0004 (0.0037)	batch 1.0844 (1.0917)	loss 4.4671 (14.9708)	grad_norm 5.4940 (18.9890)	mem 28677MB
Train: [0/180][6900/10009]	eta 0:56:32 lr 0.049998	data 0.0005 (0.0037)	batch 1.0392 (1.0913)	loss 4.3593 (14.8943)	grad_norm 5.4934 (18.8899)	mem 28677MB
Train: [0/180][6950/10009]	eta 0:55:37 lr 0.049998	data 0.0005 (0.0037)	batch 1.0371 (1.0910)	loss 4.3588 (14.8184)	grad_norm 5.2380 (18.7919)	mem 28677MB
Train: [0/180][7000/10009]	eta 0:54:41 lr 0.049998	data 0.0004 (0.0037)	batch 1.0150 (1.0906)	loss 4.6666 (14.7433)	grad_norm 5.3696 (18.6951)	mem 28677MB
Train: [0/180][7050/10009]	eta 0:53:46 lr 0.049998	data 0.0004 (0.0037)	batch 1.2047 (1.0903)	loss 3.9237 (14.6690)	grad_norm 4.9907 (18.5996)	mem 28677MB
Train: [0/180][7100/10009]	eta 0:52:50 lr 0.049998	data 0.0005 (0.0036)	batch 1.0371 (1.0899)	loss 4.2409 (14.5957)	grad_norm 5.1923 (18.5055)	mem 28677MB
Train: [0/180][7150/10009]	eta 0:51:55 lr 0.049998	data 0.0005 (0.0036)	batch 1.0408 (1.0896)	loss 4.5600 (14.5232)	grad_norm 5.3002 (18.4125)	mem 28677MB
Train: [0/180][7200/10009]	eta 0:50:59 lr 0.049998	data 0.0005 (0.0036)	batch 1.0294 (1.0893)	loss 4.0594 (14.4515)	grad_norm 5.1096 (18.3206)	mem 28677MB
Train: [0/180][7250/10009]	eta 0:50:04 lr 0.049998	data 0.0005 (0.0036)	batch 1.0363 (1.0889)	loss 3.9351 (14.3804)	grad_norm 4.9866 (18.2300)	mem 28677MB
Train: [0/180][7300/10009]	eta 0:49:09 lr 0.049998	data 0.0005 (0.0035)	batch 1.0551 (1.0886)	loss 4.0615 (14.3102)	grad_norm 5.1924 (18.1406)	mem 28677MB
Train: [0/180][7350/10009]	eta 0:48:13 lr 0.049998	data 0.0004 (0.0035)	batch 1.0240 (1.0883)	loss 3.7715 (14.2404)	grad_norm 5.1258 (18.0523)	mem 28677MB
Train: [0/180][7400/10009]	eta 0:47:18 lr 0.049998	data 0.0005 (0.0035)	batch 1.0388 (1.0879)	loss 3.7684 (14.1715)	grad_norm 5.1021 (17.9651)	mem 28677MB
Train: [0/180][7450/10009]	eta 0:46:23 lr 0.049998	data 0.0004 (0.0035)	batch 1.0476 (1.0877)	loss 4.2080 (14.1032)	grad_norm 5.2709 (17.8791)	mem 28677MB
Train: [0/180][7500/10009]	eta 0:45:28 lr 0.049998	data 0.0004 (0.0035)	batch 1.0402 (1.0874)	loss 4.1055 (14.0357)	grad_norm 5.1506 (17.7941)	mem 28677MB
Train: [0/180][7550/10009]	eta 0:44:34 lr 0.049998	data 0.0002 (0.0034)	batch 1.5251 (1.0877)	loss 3.9153 (13.9691)	grad_norm 4.9935 (17.7103)	mem 28677MB
Train: [0/180][7600/10009]	eta 0:43:46 lr 0.049998	data 0.0005 (0.0034)	batch 1.5012 (1.0905)	loss 3.8859 (13.9031)	grad_norm 5.0397 (17.6276)	mem 28677MB
Train: [0/180][7650/10009]	eta 0:42:51 lr 0.049998	data 0.0005 (0.0034)	batch 1.0397 (1.0902)	loss 4.0249 (13.8381)	grad_norm 5.1039 (17.5459)	mem 28677MB
Train: [0/180][7700/10009]	eta 0:41:58 lr 0.049998	data 0.0005 (0.0034)	batch 1.0211 (1.0909)	loss 3.5824 (13.7737)	grad_norm 5.0000 (17.4653)	mem 28677MB
Train: [0/180][7750/10009]	eta 0:41:03 lr 0.049998	data 0.0004 (0.0034)	batch 1.0400 (1.0906)	loss 3.6871 (13.7101)	grad_norm 5.0565 (17.3857)	mem 28677MB
Train: [0/180][7800/10009]	eta 0:40:08 lr 0.049998	data 0.0004 (0.0033)	batch 1.0410 (1.0902)	loss 3.6400 (13.6470)	grad_norm 4.9783 (17.3071)	mem 28677MB
Train: [0/180][7850/10009]	eta 0:39:16 lr 0.049998	data 0.0005 (0.0033)	batch 1.6784 (1.0915)	loss 3.8017 (13.5844)	grad_norm 5.1357 (17.2293)	mem 28677MB
Train: [0/180][7900/10009]	eta 0:38:23 lr 0.049998	data 0.0005 (0.0033)	batch 1.0409 (1.0921)	loss 3.7689 (13.5226)	grad_norm 4.8955 (17.1526)	mem 28677MB
Train: [0/180][7950/10009]	eta 0:37:27 lr 0.049998	data 0.0004 (0.0033)	batch 1.0874 (1.0918)	loss 4.0050 (13.4614)	grad_norm 5.1649 (17.0769)	mem 28677MB
Train: [0/180][8000/10009]	eta 0:36:35 lr 0.049998	data 0.0005 (0.0033)	batch 1.0341 (1.0930)	loss 3.7040 (13.4010)	grad_norm 5.1897 (17.0020)	mem 28677MB
Train: [0/180][8050/10009]	eta 0:35:40 lr 0.049998	data 0.0004 (0.0033)	batch 1.0412 (1.0926)	loss 3.5041 (13.3409)	grad_norm 5.0544 (16.9281)	mem 28677MB
Train: [0/180][8100/10009]	eta 0:34:45 lr 0.049998	data 0.0006 (0.0032)	batch 1.0215 (1.0923)	loss 3.4140 (13.2817)	grad_norm 5.0271 (16.8550)	mem 28677MB
Train: [0/180][8150/10009]	eta 0:33:49 lr 0.049997	data 0.0005 (0.0032)	batch 1.0407 (1.0919)	loss 3.6268 (13.2229)	grad_norm 5.0827 (16.7829)	mem 28677MB
Train: [0/180][8200/10009]	eta 0:32:54 lr 0.049997	data 0.0005 (0.0032)	batch 1.0263 (1.0916)	loss 3.6224 (13.1649)	grad_norm 5.0565 (16.7116)	mem 28677MB
Train: [0/180][8250/10009]	eta 0:31:59 lr 0.049997	data 0.0005 (0.0032)	batch 1.0308 (1.0912)	loss 3.7174 (13.1074)	grad_norm 5.0982 (16.6411)	mem 28677MB
Train: [0/180][8300/10009]	eta 0:31:04 lr 0.049997	data 0.0005 (0.0032)	batch 1.0316 (1.0909)	loss 3.5811 (13.0504)	grad_norm 5.1840 (16.5715)	mem 28677MB
Train: [0/180][8350/10009]	eta 0:30:09 lr 0.049997	data 0.0005 (0.0032)	batch 1.0344 (1.0906)	loss 3.4179 (12.9941)	grad_norm 4.9596 (16.5027)	mem 28677MB
Train: [0/180][8400/10009]	eta 0:29:14 lr 0.049997	data 0.0005 (0.0031)	batch 1.0313 (1.0903)	loss 3.5330 (12.9384)	grad_norm 5.0699 (16.4347)	mem 28677MB
Train: [0/180][8450/10009]	eta 0:28:19 lr 0.049997	data 0.0005 (0.0031)	batch 1.0341 (1.0899)	loss 3.4150 (12.8833)	grad_norm 4.8741 (16.3676)	mem 28677MB
Train: [0/180][8500/10009]	eta 0:27:24 lr 0.049997	data 0.0005 (0.0031)	batch 1.0110 (1.0896)	loss 3.3553 (12.8288)	grad_norm 4.9831 (16.3012)	mem 28677MB
Train: [0/180][8550/10009]	eta 0:26:29 lr 0.049997	data 0.0006 (0.0031)	batch 1.0399 (1.0893)	loss 3.4785 (12.7748)	grad_norm 5.0964 (16.2356)	mem 28677MB
Train: [0/180][8600/10009]	eta 0:25:34 lr 0.049997	data 0.0005 (0.0031)	batch 1.0217 (1.0890)	loss 3.4138 (12.7216)	grad_norm 5.1060 (16.1707)	mem 28677MB
Train: [0/180][8650/10009]	eta 0:24:39 lr 0.049997	data 0.0005 (0.0031)	batch 1.0333 (1.0887)	loss 3.4880 (12.6688)	grad_norm 5.0882 (16.1066)	mem 28677MB
Train: [0/180][8700/10009]	eta 0:23:44 lr 0.049997	data 0.0003 (0.0031)	batch 1.0205 (1.0884)	loss 3.6968 (12.6165)	grad_norm 5.1073 (16.0433)	mem 28677MB
Train: [0/180][8750/10009]	eta 0:22:50 lr 0.049997	data 0.0005 (0.0030)	batch 1.0426 (1.0882)	loss 3.9444 (12.5650)	grad_norm 5.1389 (15.9807)	mem 28677MB
Train: [0/180][8800/10009]	eta 0:21:55 lr 0.049997	data 0.0005 (0.0030)	batch 1.0418 (1.0880)	loss 3.7844 (12.5139)	grad_norm 5.0893 (15.9187)	mem 28677MB
Train: [0/180][8850/10009]	eta 0:21:00 lr 0.049997	data 0.0005 (0.0030)	batch 1.0329 (1.0877)	loss 3.3559 (12.4630)	grad_norm 5.0561 (15.8574)	mem 28677MB
Train: [0/180][8900/10009]	eta 0:20:05 lr 0.049997	data 0.0004 (0.0030)	batch 1.0289 (1.0874)	loss 3.6090 (12.4127)	grad_norm 5.2119 (15.7968)	mem 28677MB
Train: [0/180][8950/10009]	eta 0:19:11 lr 0.049997	data 0.0005 (0.0030)	batch 1.0223 (1.0871)	loss 3.2929 (12.3630)	grad_norm 5.1523 (15.7368)	mem 28677MB
Train: [0/180][9000/10009]	eta 0:18:16 lr 0.049997	data 0.0005 (0.0030)	batch 1.0384 (1.0868)	loss 3.5998 (12.3137)	grad_norm 5.1566 (15.6776)	mem 28677MB
Train: [0/180][9050/10009]	eta 0:17:22 lr 0.049997	data 0.0005 (0.0030)	batch 1.0203 (1.0866)	loss 3.5309 (12.2649)	grad_norm 5.0212 (15.6190)	mem 28677MB
Train: [0/180][9100/10009]	eta 0:16:27 lr 0.049997	data 0.0005 (0.0029)	batch 1.0369 (1.0863)	loss 3.6112 (12.2167)	grad_norm 5.1360 (15.5610)	mem 28677MB
Train: [0/180][9150/10009]	eta 0:15:32 lr 0.049997	data 0.0005 (0.0029)	batch 1.0324 (1.0860)	loss 3.9304 (12.1690)	grad_norm 5.2152 (15.5038)	mem 28677MB
Train: [0/180][9200/10009]	eta 0:14:38 lr 0.049997	data 0.0005 (0.0029)	batch 1.0433 (1.0857)	loss 3.5643 (12.1216)	grad_norm 5.0351 (15.4471)	mem 28677MB
Train: [0/180][9250/10009]	eta 0:13:43 lr 0.049997	data 0.0004 (0.0029)	batch 1.0455 (1.0855)	loss 3.4764 (12.0748)	grad_norm 5.0152 (15.3910)	mem 28677MB
Train: [0/180][9300/10009]	eta 0:12:49 lr 0.049997	data 0.0005 (0.0029)	batch 1.0139 (1.0852)	loss 3.6096 (12.0284)	grad_norm 5.1366 (15.3355)	mem 28677MB
Train: [0/180][9350/10009]	eta 0:11:55 lr 0.049997	data 0.0005 (0.0029)	batch 1.0353 (1.0850)	loss 3.6647 (11.9825)	grad_norm 5.1797 (15.2806)	mem 28677MB
Train: [0/180][9400/10009]	eta 0:11:00 lr 0.049997	data 0.0004 (0.0029)	batch 1.0376 (1.0847)	loss 3.5959 (11.9370)	grad_norm 5.4106 (15.2263)	mem 28677MB
Train: [0/180][9450/10009]	eta 0:10:06 lr 0.049997	data 0.0004 (0.0029)	batch 1.0406 (1.0845)	loss 3.8764 (11.8921)	grad_norm 5.2223 (15.1724)	mem 28677MB
Train: [0/180][9500/10009]	eta 0:09:11 lr 0.049997	data 0.0005 (0.0028)	batch 1.0332 (1.0842)	loss 3.1631 (11.8475)	grad_norm 4.9801 (15.1192)	mem 28677MB
Train: [0/180][9550/10009]	eta 0:08:17 lr 0.049997	data 0.0005 (0.0028)	batch 1.0373 (1.0840)	loss 3.4306 (11.8035)	grad_norm 5.0564 (15.0665)	mem 28677MB
Train: [0/180][9600/10009]	eta 0:07:23 lr 0.049996	data 0.0005 (0.0028)	batch 1.0374 (1.0837)	loss 3.3297 (11.7600)	grad_norm 5.1397 (15.0139)	mem 28677MB
Train: [0/180][9650/10009]	eta 0:06:28 lr 0.049996	data 0.0004 (0.0028)	batch 1.0450 (1.0835)	loss 3.4772 (11.7166)	grad_norm 6.8460 (14.9622)	mem 28677MB
Train: [0/180][9700/10009]	eta 0:05:34 lr 0.049996	data 0.0004 (0.0028)	batch 1.0485 (1.0833)	loss 3.1630 (11.6738)	grad_norm 5.1216 (14.9107)	mem 28677MB
Train: [0/180][9750/10009]	eta 0:04:40 lr 0.049996	data 0.0004 (0.0028)	batch 1.0439 (1.0831)	loss 3.5180 (11.6312)	grad_norm 5.2025 (14.8596)	mem 28677MB
Train: [0/180][9800/10009]	eta 0:03:46 lr 0.049996	data 0.0005 (0.0028)	batch 1.0370 (1.0829)	loss 3.4740 (11.5892)	grad_norm 4.7878 (14.8095)	mem 28677MB
Train: [0/180][9850/10009]	eta 0:02:52 lr 0.049996	data 0.0005 (0.0028)	batch 1.0232 (1.0826)	loss 3.6581 (11.5477)	grad_norm 5.2092 (14.7595)	mem 28677MB
Train: [0/180][9900/10009]	eta 0:01:57 lr 0.049996	data 0.0005 (0.0027)	batch 1.0377 (1.0824)	loss 3.2915 (11.5064)	grad_norm 5.0197 (14.7104)	mem 28677MB
Train: [0/180][9950/10009]	eta 0:01:03 lr 0.049996	data 0.0005 (0.0027)	batch 1.0345 (1.0822)	loss 3.1814 (11.4655)	grad_norm 5.0748 (14.6619)	mem 28677MB
Train: [0/180][10000/10009]	eta 0:00:09 lr 0.049996	data 0.0002 (0.0027)	batch 1.0185 (1.0820)	loss 3.2361 (11.4250)	grad_norm 4.4963 (14.6137)	mem 28677MB
Current slope: None 	
EPOCH 0 training takes 3:00:29
Test: [0/391]	Time 12.871 (12.871)	Loss 1.3860 (1.3860)	Acc@1 70.312 (70.312)	Acc@5 87.500 (87.500)	Mem 28677MB
Test: [50/391]	Time 0.255 (0.504)	Loss 1.2184 (1.7048)	Acc@1 71.875 (60.509)	Acc@5 88.281 (82.874)	Mem 28677MB
Test: [100/391]	Time 0.256 (0.381)	Loss 1.9861 (1.7631)	Acc@1 46.094 (57.488)	Acc@5 84.375 (83.338)	Mem 28677MB
Test: [150/391]	Time 0.255 (0.340)	Loss 1.6834 (1.7291)	Acc@1 49.219 (58.356)	Acc@5 88.281 (83.878)	Mem 28677MB
Test: [200/391]	Time 0.255 (0.319)	Loss 2.6963 (1.8863)	Acc@1 35.156 (55.772)	Acc@5 69.531 (81.363)	Mem 28677MB
Test: [250/391]	Time 0.275 (0.307)	Loss 2.0148 (1.9780)	Acc@1 54.688 (54.336)	Acc@5 72.656 (79.684)	Mem 28677MB
Test: [300/391]	Time 0.256 (0.298)	Loss 2.3729 (2.0503)	Acc@1 53.125 (53.187)	Acc@5 71.875 (78.460)	Mem 28677MB
Test: [350/391]	Time 0.256 (0.292)	Loss 2.3035 (2.1059)	Acc@1 50.781 (52.295)	Acc@5 71.094 (77.600)	Mem 28677MB
 * Acc@1 52.686 Acc@5 77.962
Accuracy of the network on the 50000 test images: 52.69%
Max accuracy (after decay): 52.69%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/10009]	eta 2 days, 11:31:44 lr 0.049996	data 20.0914 (20.0914)	batch 21.4111 (21.4111)	loss 2.9758 (2.9758)	grad_norm 4.9285 (4.9285)	mem 28677MB
Train: [1/180][50/10009]	eta 3:58:10 lr 0.049996	data 0.0005 (0.3945)	batch 1.0383 (1.4350)	loss 3.3330 (3.3348)	grad_norm 5.0522 (5.0437)	mem 28677MB
Train: [1/180][100/10009]	eta 3:24:31 lr 0.049996	data 0.0004 (0.1994)	batch 1.0348 (1.2384)	loss 3.2989 (3.3334)	grad_norm 4.9229 (5.0459)	mem 28677MB
Train: [1/180][150/10009]	eta 3:12:55 lr 0.049996	data 0.0005 (0.1336)	batch 1.0314 (1.1741)	loss 3.0590 (3.3594)	grad_norm 5.0594 (5.0468)	mem 28677MB
Train: [1/180][200/10009]	eta 3:06:16 lr 0.049996	data 0.0005 (0.1005)	batch 1.0435 (1.1394)	loss 3.1645 (3.3365)	grad_norm 4.9853 (5.0578)	mem 28677MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 18:36:50 lr 0.050000	data 20.4909 (20.4909)	batch 23.9594 (23.9594)	loss 26.1720 (26.1720)	grad_norm 77.7584 (77.7584)	mem 28688MB
Train: [0/180][50/10009]	eta 4:03:21 lr 0.050000	data 0.0007 (0.4025)	batch 1.0151 (1.4661)	loss 21.3813 (18.0123)	grad_norm 44.5680 (39.2141)	mem 28688MB
Train: [0/180][100/10009]	eta 3:24:57 lr 0.050000	data 0.0006 (0.2036)	batch 1.0091 (1.2410)	loss 31.3111 (22.2733)	grad_norm 38.6416 (44.2126)	mem 28688MB
Train: [0/180][150/10009]	eta 3:11:24 lr 0.050000	data 0.0006 (0.1364)	batch 1.0084 (1.1648)	loss 36.8463 (26.4985)	grad_norm 67.9949 (46.8915)	mem 28688MB
Train: [0/180][200/10009]	eta 3:04:04 lr 0.050000	data 0.0006 (0.1026)	batch 1.0102 (1.1259)	loss 34.2665 (29.4380)	grad_norm 37.1258 (47.1660)	mem 28688MB
Train: [0/180][250/10009]	eta 2:59:27 lr 0.050000	data 0.0006 (0.0823)	batch 1.0112 (1.1034)	loss 36.0970 (31.5677)	grad_norm 76.9009 (47.5399)	mem 28688MB
Train: [0/180][300/10009]	eta 2:55:57 lr 0.050000	data 0.0005 (0.0687)	batch 1.0075 (1.0874)	loss 42.1702 (32.7377)	grad_norm 78.6082 (47.6377)	mem 28688MB
Train: [0/180][350/10009]	eta 2:53:17 lr 0.050000	data 0.0006 (0.0590)	batch 1.0108 (1.0764)	loss 50.9760 (33.9115)	grad_norm 32.2644 (48.4668)	mem 28688MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 19:08:47 lr 0.050000	data 20.4998 (20.4998)	batch 24.1510 (24.1510)	loss 26.3928 (26.3928)	grad_norm 78.5018 (78.5018)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 20:37:05 lr 0.050000	data 21.1060 (21.1060)	batch 24.6803 (24.6803)	loss 26.3927 (26.3927)	grad_norm 78.5066 (78.5066)	mem 28740MB
Train: [0/180][50/10009]	eta 4:22:21 lr 0.050000	data 0.0005 (0.4145)	batch 1.1215 (1.5806)	loss 17.4167 (16.7958)	grad_norm 31.8235 (39.1475)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.8
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108795
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.8
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 16:26:09 lr 0.050000	data 20.3606 (20.3606)	batch 23.1761 (23.1761)	loss 29.3612 (29.3612)	grad_norm 109.7287 (109.7287)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.4
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act(
          (act_fun): Learnable_Relu6_Hard()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108795
Unsupported operator aten::sub encountered 140 time(s)
Unsupported operator aten::mul encountered 105 time(s)
Unsupported operator aten::add encountered 105 time(s)
Unsupported operator aten::rsub encountered 35 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 9:19:00 lr 0.050000	data 17.8729 (17.8729)	batch 20.6155 (20.6155)	loss 38.4489 (38.4489)	grad_norm 136.9953 (136.9953)	mem 24443MB
Train: [0/180][50/10009]	eta 2:54:16 lr 0.050000	data 0.0045 (0.3529)	batch 0.6450 (1.0499)	loss 6.0013 (7.2478)	grad_norm 2.8370 (6.7022)	mem 24443MB
Train: [0/180][100/10009]	eta 2:20:57 lr 0.050000	data 0.0006 (0.1792)	batch 0.6937 (0.8535)	loss 5.6775 (6.5839)	grad_norm 2.5795 (4.7527)	mem 24443MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard
  ACT_LIST: []
  ADD_FINAL_ACT: relu6
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: false
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.4
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: true
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/ondevice_ai_tools/tree/users/yongganfu/experiments/depth_shrink/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Keep 40.0%  activation funtions: [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.]
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (2): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (3): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (3): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
    )
    (4): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (5): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
      (1): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Identity()
      )
      (2): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
    (6): Sequential(
      (0): InvertedResidual_Share_Act(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (final_act): Final_Act()
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
number of params (M): 6.108776
Unsupported operator aten::leaky_relu encountered 46 time(s)
Unsupported operator aten::sub encountered 46 time(s)
Unsupported operator aten::mul encountered 46 time(s)
Unsupported operator aten::add encountered 46 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 10:32:25 lr 0.050000	data 18.3746 (18.3746)	batch 21.0556 (21.0556)	loss 36.8887 (36.8887)	grad_norm 134.1788 (134.1788)	mem 20527MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 15:36:25 lr 0.050000	data 19.5568 (19.5568)	batch 22.8780 (22.8780)	loss 26.3925 (26.3925)	grad_norm 78.5071 (78.5071)	mem 28740MB
Train: [0/180][50/10009]	eta 4:17:47 lr 0.050000	data 0.0006 (0.3842)	batch 1.0967 (1.5531)	loss 17.3878 (17.2248)	grad_norm 19.2616 (39.7637)	mem 28740MB
Train: [0/180][100/10009]	eta 3:41:18 lr 0.050000	data 0.0007 (0.1943)	batch 1.1243 (1.3401)	loss 28.0392 (21.1658)	grad_norm 52.9788 (43.5419)	mem 28740MB
Train: [0/180][150/10009]	eta 3:28:27 lr 0.050000	data 0.0005 (0.1302)	batch 1.1091 (1.2686)	loss 36.5794 (25.0274)	grad_norm 37.7002 (44.6134)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 16:14:53 lr 0.050000	data 19.4338 (19.4338)	batch 23.1085 (23.1085)	loss 26.3926 (26.3926)	grad_norm 78.5022 (78.5022)	mem 28740MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 12:28:29 lr 0.050000	data 18.3738 (18.3738)	batch 21.7514 (21.7514)	loss 26.3928 (26.3928)	grad_norm 78.9190 (78.9190)	mem 28723MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: false
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: true
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: true
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act(
          (act_fun): Learnable_Relu6_Hard_SNL()
        )
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act(
    (act_fun): Learnable_Relu6_Hard_SNL()
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::sub encountered 175 time(s)
Unsupported operator aten::mul encountered 210 time(s)
Unsupported operator aten::add encountered 210 time(s)
Unsupported operator aten::rsub encountered 105 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 15:11:17 lr 0.050000	data 19.3871 (19.3871)	batch 22.7273 (22.7273)	loss 15.8777 (15.8777)	grad_norm 30.2834 (30.2834)	mem 28724MB
Full config saved to manifold://experiment/default/config.json
AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /opt/dataset/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 6
  PIN_MEMORY: true
  PREFETCH_FACTOR: 6
  ZIP_MODE: false
DISTILL: false
DS:
  ACT_DISTRIB: first
  ACT_FROM_LIST: false
  ACT_FROM_SEARCH: true
  ACT_FUN: learnable_relu6_hard_snl
  ACT_LIST: []
  ADD_FINAL_ACT: ''
  CHL_WISE: true
  DECAY_MODE: iter
  DECAY_SLOPE: false
  DISTILL: true
  DISTILL_FEATURE: false
  DISTILL_FEATURE_WEIGHT: 0.001
  DISTILL_WEIGHT: 0.7
  EA:
    CYCLES: 5000
    POP_SIZE: 64
    SAMPLE_SIZE: 16
    SEARCH: false
    SPARSE_RATIO: 0.5
  END_EPOCH: 0
  END_SLOPE: 1
  EXPAND_RATIO: 6
  FINAL_ACT_LR_SCALE: 1.0
  GS_SAMPLE:
    DECAY_RATE: 0.95
    ENABLE: false
    EPOCH: 60
    INIT_TEMP: 3
  KEEP_ALL_ACT: false
  L0_SPARSITY: 0.6
  L1_WEIGHT: 0.0
  LAT_AFTER:
  - 11.38781
  - 17.8929
  - 8.85087
  - 4.89339
  - 4.36431
  - 2.21807
  - 2.21807
  - 2.95533
  - 3.1424
  - 3.1424
  - 3.1424
  - 4.53318
  - 6.70935
  - 6.70935
  - 7.68199
  - 7.07067
  - 7.07067
  - 13.7805
  - 23.0013
  LAT_BEFORE:
  - 18.5039
  - 59.0279
  - 67.7049
  - 61.0657
  - 30.3438
  - 23.3904
  - 23.3904
  - 13.6589
  - 18.5797
  - 18.5797
  - 18.5797
  - 21.1564
  - 35.212
  - 35.212
  - 23.3629
  - 25.978
  - 25.978
  - 34.261
  - 23.0013
  LAT_COST_WEIGHT: 0.0001
  MERGE: false
  NO_BN_STATS: false
  PIXEL_WISE: false
  PRETRAINED: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
  PROG_REMOVE: false
  PROG_REMOVE_EPOCH: 120
  PROG_REMOVE_MODE: forward
  RANDOM_DROP: false
  REMOVE_BLOCK: false
  SEARCH: false
  SEARCH_CKPT: /home/zwx/projects/DepthShrinker/manifold:/experiment/default/ckpt.pth
  START_EPOCH: 0
  START_SLOPE: 0
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_BLOCK_RATE: 0.0
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: mobilenetv2_140_block_ds
  NUM_CLASSES: 1000
  RESUME: ''
  TYPE: mobilenetv2_140_block_ds
OUTPUT: manifold://experiment/default
PRINT_FREQ: 50
RANK: 0
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_BATCH_SIZE: 256
  BASE_LR: 0.05
  CLIP_GRAD: 5.0
  EPOCHS: 180
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MAX: 192
  MIN_LR: 2.5e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  USE_CONV_PROJ: true
  WARMUP_EPOCHS: 0
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 1.0e-05
WORLD_SIZE: 1
dist_url: tcp://127.0.0.1:10000
gpu: 0
machine_rank: 0
num_nodes: 1

Creating model:mobilenetv2_140_block_ds/mobilenetv2_140_block_ds
Successfully load pretrained model: /home/zwx/projects/DepthShrinker/mobilenetv2_140_ra-21a4e913.pth
EfficientNet(
  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): Changable_Act()
  (blocks): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (se): Identity()
        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)
        (bn2): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(528, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(816, 816, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(816, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): Changable_Act()
        (conv_dw): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
        (bn2): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Changable_Act()
        (se): Identity()
        (conv_pwl): Conv2d(1344, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv_head): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): Changable_Act()
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (classifier): Linear(in_features=1792, out_features=1000, bias=True)
)
Successfully build teacher model
number of params (M): 6.108776
Unsupported operator aten::maximum encountered 35 time(s)
Unsupported operator aten::minimum encountered 35 time(s)
Unsupported operator aten::mul encountered 70 time(s)
Unsupported operator aten::add encountered 70 time(s)
Unsupported operator aten::sub encountered 70 time(s)
Unsupported operator aten::add_ encountered 10 time(s)
number of GFLOPs: 0.60143048
Start training
Train: [0/180][0/10009]	eta 2 days, 13:53:04 lr 0.050000	data 18.8869 (18.8869)	batch 22.2584 (22.2584)	loss 26.3927 (26.3927)	grad_norm 78.5029 (78.5029)	mem 28740MB
Train: [0/180][50/10009]	eta 4:15:46 lr 0.050000	data 0.0006 (0.3710)	batch 1.1149 (1.5410)	loss 19.7971 (17.1180)	grad_norm 37.9164 (38.6045)	mem 28740MB
Train: [0/180][100/10009]	eta 3:40:49 lr 0.050000	data 0.0020 (0.1877)	batch 1.1438 (1.3371)	loss 34.6794 (21.7959)	grad_norm 39.8440 (43.4914)	mem 28740MB
Train: [0/180][150/10009]	eta 3:28:13 lr 0.050000	data 0.0008 (0.1258)	batch 1.1151 (1.2672)	loss 42.1576 (25.6200)	grad_norm 26.8793 (44.8707)	mem 28740MB
Train: [0/180][200/10009]	eta 3:21:11 lr 0.050000	data 0.0007 (0.0946)	batch 1.1095 (1.2306)	loss 43.8776 (28.2958)	grad_norm 31.7363 (44.7153)	mem 28740MB
Train: [0/180][250/10009]	eta 3:16:45 lr 0.050000	data 0.0005 (0.0759)	batch 1.1240 (1.2097)	loss 36.0181 (29.9919)	grad_norm 44.1459 (44.7584)	mem 28740MB
Train: [0/180][300/10009]	eta 3:13:22 lr 0.050000	data 0.0005 (0.0634)	batch 1.1189 (1.1950)	loss 35.3503 (31.1017)	grad_norm 38.3818 (45.1930)	mem 28740MB
Train: [0/180][350/10009]	eta 3:10:48 lr 0.050000	data 0.0006 (0.0544)	batch 1.1232 (1.1852)	loss 37.6685 (31.9795)	grad_norm 31.4093 (45.1518)	mem 28740MB
Train: [0/180][400/10009]	eta 3:08:38 lr 0.050000	data 0.0005 (0.0477)	batch 1.1105 (1.1779)	loss 26.9126 (32.7043)	grad_norm 32.2979 (44.9036)	mem 28740MB
Train: [0/180][450/10009]	eta 3:06:40 lr 0.050000	data 0.0006 (0.0425)	batch 1.1311 (1.1718)	loss 37.7066 (33.2279)	grad_norm 32.2750 (45.0364)	mem 28740MB
Train: [0/180][500/10009]	eta 3:04:58 lr 0.050000	data 0.0006 (0.0383)	batch 1.1230 (1.1672)	loss 37.2251 (33.7205)	grad_norm 31.5737 (45.2901)	mem 28740MB
Train: [0/180][550/10009]	eta 3:03:24 lr 0.050000	data 0.0005 (0.0349)	batch 1.1314 (1.1633)	loss 42.4326 (34.2908)	grad_norm 49.7087 (45.4069)	mem 28740MB
Train: [0/180][600/10009]	eta 3:01:55 lr 0.050000	data 0.0006 (0.0320)	batch 1.1370 (1.1601)	loss 34.8362 (34.6531)	grad_norm 67.3586 (45.5515)	mem 28740MB
Train: [0/180][650/10009]	eta 3:00:29 lr 0.050000	data 0.0006 (0.0296)	batch 1.1302 (1.1571)	loss 33.8553 (34.9306)	grad_norm 30.7120 (45.2951)	mem 28740MB
Train: [0/180][700/10009]	eta 2:59:05 lr 0.050000	data 0.0005 (0.0276)	batch 1.1148 (1.1544)	loss 40.5728 (35.1293)	grad_norm 40.0386 (45.1458)	mem 28740MB
Train: [0/180][750/10009]	eta 2:57:46 lr 0.050000	data 0.0005 (0.0258)	batch 1.1226 (1.1520)	loss 38.6506 (35.2600)	grad_norm 32.3301 (45.0586)	mem 28740MB
Train: [0/180][800/10009]	eta 2:56:25 lr 0.050000	data 0.0005 (0.0242)	batch 1.1156 (1.1495)	loss 32.0605 (35.3770)	grad_norm 30.0165 (45.0552)	mem 28740MB
Train: [0/180][850/10009]	eta 2:55:09 lr 0.050000	data 0.0007 (0.0228)	batch 1.1177 (1.1475)	loss 44.0726 (35.5426)	grad_norm 39.3739 (45.0366)	mem 28740MB
Train: [0/180][900/10009]	eta 2:53:56 lr 0.050000	data 0.0005 (0.0216)	batch 1.1181 (1.1458)	loss 39.5124 (35.6214)	grad_norm 25.6164 (44.7896)	mem 28740MB
Train: [0/180][950/10009]	eta 2:52:50 lr 0.050000	data 0.0006 (0.0205)	batch 1.1190 (1.1448)	loss 38.5569 (35.6153)	grad_norm 37.5066 (44.6632)	mem 28740MB
Train: [0/180][1000/10009]	eta 2:51:39 lr 0.050000	data 0.0005 (0.0195)	batch 1.0996 (1.1432)	loss 36.2517 (35.6726)	grad_norm 46.1211 (44.9423)	mem 28740MB
Train: [0/180][1050/10009]	eta 2:50:31 lr 0.050000	data 0.0005 (0.0186)	batch 1.1037 (1.1421)	loss 35.8613 (35.6458)	grad_norm 41.2295 (44.7748)	mem 28740MB
Train: [0/180][1100/10009]	eta 2:49:25 lr 0.050000	data 0.0006 (0.0178)	batch 1.1163 (1.1410)	loss 37.9932 (35.6081)	grad_norm 46.9647 (44.6843)	mem 28740MB
Train: [0/180][1150/10009]	eta 2:48:19 lr 0.050000	data 0.0008 (0.0170)	batch 1.1143 (1.1400)	loss 39.1624 (35.6400)	grad_norm 45.7768 (44.6441)	mem 28740MB
Train: [0/180][1200/10009]	eta 2:47:14 lr 0.050000	data 0.0005 (0.0163)	batch 1.1191 (1.1391)	loss 33.6524 (35.6446)	grad_norm 29.7090 (44.5931)	mem 28740MB
Train: [0/180][1250/10009]	eta 2:46:09 lr 0.050000	data 0.0005 (0.0157)	batch 1.1003 (1.1382)	loss 42.1040 (35.6877)	grad_norm 46.4890 (44.4950)	mem 28740MB
Train: [0/180][1300/10009]	eta 2:45:07 lr 0.050000	data 0.0005 (0.0151)	batch 1.1171 (1.1376)	loss 35.2900 (35.7107)	grad_norm 28.0676 (44.3523)	mem 28740MB
Train: [0/180][1350/10009]	eta 2:44:04 lr 0.050000	data 0.0006 (0.0146)	batch 1.1291 (1.1369)	loss 33.0685 (35.6863)	grad_norm 51.2031 (44.1474)	mem 28740MB
Train: [0/180][1400/10009]	eta 2:43:02 lr 0.050000	data 0.0006 (0.0141)	batch 1.1234 (1.1363)	loss 34.7462 (35.6376)	grad_norm 32.5611 (44.0751)	mem 28740MB
Train: [0/180][1450/10009]	eta 2:42:00 lr 0.050000	data 0.0006 (0.0136)	batch 1.1219 (1.1357)	loss 40.2228 (35.6165)	grad_norm 55.6789 (43.8358)	mem 28740MB
Train: [0/180][1500/10009]	eta 2:40:58 lr 0.050000	data 0.0006 (0.0132)	batch 1.1292 (1.1351)	loss 39.4655 (35.5761)	grad_norm 44.5788 (43.7029)	mem 28740MB
Train: [0/180][1550/10009]	eta 2:39:57 lr 0.050000	data 0.0005 (0.0128)	batch 1.1108 (1.1346)	loss 43.2307 (35.5441)	grad_norm 28.7810 (43.4548)	mem 28740MB
Train: [0/180][1600/10009]	eta 2:38:58 lr 0.050000	data 0.0005 (0.0124)	batch 1.1224 (1.1343)	loss 29.5026 (35.4359)	grad_norm 24.0636 (43.2070)	mem 28740MB
Train: [0/180][1650/10009]	eta 2:37:57 lr 0.050000	data 0.0007 (0.0120)	batch 1.1398 (1.1338)	loss 32.4848 (35.3162)	grad_norm 44.0116 (43.0132)	mem 28740MB
Train: [0/180][1700/10009]	eta 2:36:56 lr 0.050000	data 0.0005 (0.0117)	batch 1.0973 (1.1333)	loss 24.9588 (35.1491)	grad_norm 27.0713 (42.8389)	mem 28740MB
Train: [0/180][1750/10009]	eta 2:35:58 lr 0.050000	data 0.0006 (0.0114)	batch 1.1245 (1.1331)	loss 35.1977 (34.9922)	grad_norm 66.2657 (42.5581)	mem 28740MB
Train: [0/180][1800/10009]	eta 2:34:59 lr 0.050000	data 0.0006 (0.0111)	batch 1.1136 (1.1329)	loss 25.6406 (34.8051)	grad_norm 35.0751 (42.2081)	mem 28740MB
Train: [0/180][1850/10009]	eta 2:34:00 lr 0.050000	data 0.0004 (0.0108)	batch 1.1067 (1.1326)	loss 25.7065 (34.5766)	grad_norm 17.7849 (41.8659)	mem 28740MB
Train: [0/180][1900/10009]	eta 2:33:02 lr 0.050000	data 0.0005 (0.0105)	batch 1.1463 (1.1324)	loss 16.9247 (34.2966)	grad_norm 39.9721 (41.5558)	mem 28740MB
Train: [0/180][1950/10009]	eta 2:32:03 lr 0.050000	data 0.0005 (0.0103)	batch 1.1287 (1.1321)	loss 24.4472 (34.0327)	grad_norm 26.2368 (41.2051)	mem 28740MB
Train: [0/180][2000/10009]	eta 2:31:06 lr 0.050000	data 0.0005 (0.0100)	batch 1.1154 (1.1320)	loss 23.2505 (33.7698)	grad_norm 16.9634 (40.7887)	mem 28740MB
Train: [0/180][2050/10009]	eta 2:30:07 lr 0.050000	data 0.0006 (0.0098)	batch 1.1241 (1.1318)	loss 21.8409 (33.4484)	grad_norm 20.8955 (40.3564)	mem 28740MB
Train: [0/180][2100/10009]	eta 2:29:09 lr 0.050000	data 0.0006 (0.0096)	batch 1.1099 (1.1315)	loss 19.0771 (33.1227)	grad_norm 32.8935 (40.0374)	mem 28740MB
Train: [0/180][2150/10009]	eta 2:28:11 lr 0.050000	data 0.0006 (0.0094)	batch 1.1059 (1.1314)	loss 19.6735 (32.7984)	grad_norm 30.6139 (39.5723)	mem 28740MB
Train: [0/180][2200/10009]	eta 2:27:13 lr 0.050000	data 0.0005 (0.0092)	batch 1.1424 (1.1312)	loss 18.8202 (32.4818)	grad_norm 34.2764 (39.1427)	mem 28740MB
Train: [0/180][2250/10009]	eta 2:26:16 lr 0.050000	data 0.0005 (0.0090)	batch 1.1407 (1.1312)	loss 20.3098 (32.1632)	grad_norm 12.9057 (38.7544)	mem 28740MB
Train: [0/180][2300/10009]	eta 2:25:18 lr 0.050000	data 0.0006 (0.0088)	batch 1.1159 (1.1310)	loss 14.5332 (31.8181)	grad_norm 11.9029 (38.2898)	mem 28740MB
Train: [0/180][2350/10009]	eta 2:24:21 lr 0.050000	data 0.0008 (0.0086)	batch 1.2511 (1.1309)	loss 18.1069 (31.4761)	grad_norm 10.8539 (37.8756)	mem 28740MB
Train: [0/180][2400/10009]	eta 2:23:23 lr 0.050000	data 0.0005 (0.0085)	batch 1.1192 (1.1307)	loss 14.3448 (31.1404)	grad_norm 13.0095 (37.4454)	mem 28740MB
Train: [0/180][2450/10009]	eta 2:22:25 lr 0.050000	data 0.0005 (0.0083)	batch 1.1256 (1.1305)	loss 20.0949 (30.8112)	grad_norm 10.8130 (37.0314)	mem 28740MB
Train: [0/180][2500/10009]	eta 2:21:28 lr 0.050000	data 0.0006 (0.0081)	batch 1.1374 (1.1304)	loss 13.6399 (30.4923)	grad_norm 13.8783 (36.6156)	mem 28740MB
Train: [0/180][2550/10009]	eta 2:20:30 lr 0.050000	data 0.0005 (0.0080)	batch 1.1161 (1.1303)	loss 13.5135 (30.1776)	grad_norm 11.2232 (36.2197)	mem 28740MB
Train: [0/180][2600/10009]	eta 2:19:33 lr 0.050000	data 0.0005 (0.0079)	batch 1.1409 (1.1302)	loss 15.2236 (29.8710)	grad_norm 38.8679 (35.8300)	mem 28740MB
Train: [0/180][2650/10009]	eta 2:18:36 lr 0.050000	data 0.0006 (0.0077)	batch 1.1377 (1.1301)	loss 13.4567 (29.5690)	grad_norm 14.5867 (35.4848)	mem 28740MB
Train: [0/180][2700/10009]	eta 2:17:38 lr 0.050000	data 0.0004 (0.0076)	batch 1.1050 (1.1299)	loss 17.8113 (29.2813)	grad_norm 17.7664 (35.1141)	mem 28740MB
Train: [0/180][2750/10009]	eta 2:16:40 lr 0.050000	data 0.0006 (0.0075)	batch 1.1178 (1.1298)	loss 13.5309 (28.9952)	grad_norm 11.8006 (34.7738)	mem 28740MB
Train: [0/180][2800/10009]	eta 2:15:43 lr 0.050000	data 0.0005 (0.0073)	batch 1.1159 (1.1296)	loss 15.9811 (28.7182)	grad_norm 19.2678 (34.4192)	mem 28740MB
Train: [0/180][2850/10009]	eta 2:14:45 lr 0.050000	data 0.0006 (0.0072)	batch 1.1233 (1.1295)	loss 13.5856 (28.4453)	grad_norm 12.4080 (34.0822)	mem 28740MB
Train: [0/180][2900/10009]	eta 2:13:47 lr 0.050000	data 0.0005 (0.0071)	batch 1.1201 (1.1292)	loss 11.3928 (28.1769)	grad_norm 10.8110 (33.7588)	mem 28740MB
Train: [0/180][2950/10009]	eta 2:12:50 lr 0.050000	data 0.0005 (0.0070)	batch 1.1225 (1.1291)	loss 11.7436 (27.9162)	grad_norm 17.7363 (33.4542)	mem 28740MB
Train: [0/180][3000/10009]	eta 2:11:52 lr 0.050000	data 0.0005 (0.0069)	batch 1.1153 (1.1289)	loss 13.4915 (27.6461)	grad_norm 12.7197 (33.1240)	mem 28740MB
Train: [0/180][3050/10009]	eta 2:10:54 lr 0.050000	data 0.0005 (0.0068)	batch 1.1024 (1.1286)	loss 11.3124 (27.3820)	grad_norm 12.1626 (32.8017)	mem 28740MB
Train: [0/180][3100/10009]	eta 2:09:57 lr 0.050000	data 0.0006 (0.0067)	batch 1.1032 (1.1286)	loss 12.3478 (27.1226)	grad_norm 25.0034 (32.4873)	mem 28740MB
Train: [0/180][3150/10009]	eta 2:09:00 lr 0.050000	data 0.0006 (0.0066)	batch 1.1049 (1.1285)	loss 12.2758 (26.8705)	grad_norm 11.2357 (32.1942)	mem 28740MB
Train: [0/180][3200/10009]	eta 2:08:03 lr 0.050000	data 0.0005 (0.0065)	batch 1.1324 (1.1284)	loss 9.9186 (26.6175)	grad_norm 11.1624 (31.8826)	mem 28740MB
Train: [0/180][3250/10009]	eta 2:07:05 lr 0.050000	data 0.0005 (0.0064)	batch 1.1167 (1.1282)	loss 11.2872 (26.3740)	grad_norm 10.3875 (31.5869)	mem 28740MB
Train: [0/180][3300/10009]	eta 2:06:08 lr 0.050000	data 0.0006 (0.0063)	batch 1.1196 (1.1282)	loss 11.6028 (26.1340)	grad_norm 11.2445 (31.2954)	mem 28740MB
Train: [0/180][3350/10009]	eta 2:05:11 lr 0.050000	data 0.0005 (0.0062)	batch 1.1253 (1.1280)	loss 10.1979 (25.9022)	grad_norm 9.1513 (31.0176)	mem 28740MB
Train: [0/180][3400/10009]	eta 2:04:14 lr 0.050000	data 0.0005 (0.0061)	batch 1.1065 (1.1279)	loss 9.9199 (25.6705)	grad_norm 8.7186 (30.7337)	mem 28740MB
Train: [0/180][3450/10009]	eta 2:03:17 lr 0.050000	data 0.0005 (0.0061)	batch 1.1183 (1.1278)	loss 11.1454 (25.4390)	grad_norm 9.3329 (30.4612)	mem 28740MB
Train: [0/180][3500/10009]	eta 2:02:20 lr 0.050000	data 0.0005 (0.0060)	batch 1.1407 (1.1277)	loss 8.8509 (25.2145)	grad_norm 8.2429 (30.1918)	mem 28740MB
Train: [0/180][3550/10009]	eta 2:01:23 lr 0.050000	data 0.0006 (0.0059)	batch 1.1172 (1.1276)	loss 8.6365 (24.9933)	grad_norm 10.7485 (29.9235)	mem 28740MB
Train: [0/180][3600/10009]	eta 2:00:26 lr 0.050000	data 0.0005 (0.0058)	batch 1.1483 (1.1275)	loss 8.4738 (24.7760)	grad_norm 8.3498 (29.6735)	mem 28740MB
Train: [0/180][3650/10009]	eta 1:59:29 lr 0.049999	data 0.0005 (0.0058)	batch 1.1174 (1.1275)	loss 7.7335 (24.5617)	grad_norm 10.6947 (29.4259)	mem 28740MB
Train: [0/180][3700/10009]	eta 1:58:32 lr 0.049999	data 0.0005 (0.0057)	batch 1.1073 (1.1273)	loss 9.3518 (24.3446)	grad_norm 10.4417 (29.1655)	mem 28740MB
Train: [0/180][3750/10009]	eta 1:57:35 lr 0.049999	data 0.0005 (0.0056)	batch 1.1236 (1.1272)	loss 7.9919 (24.1265)	grad_norm 8.4813 (28.9091)	mem 28740MB
Train: [0/180][3800/10009]	eta 1:56:38 lr 0.049999	data 0.0006 (0.0056)	batch 1.1130 (1.1272)	loss 8.0064 (23.9159)	grad_norm 11.5490 (28.6582)	mem 28740MB
Train: [0/180][3850/10009]	eta 1:55:41 lr 0.049999	data 0.0006 (0.0055)	batch 1.1378 (1.1271)	loss 6.8547 (23.7032)	grad_norm 8.6049 (28.4083)	mem 28740MB
Train: [0/180][3900/10009]	eta 1:54:45 lr 0.049999	data 0.0005 (0.0054)	batch 1.1181 (1.1271)	loss 7.7956 (23.4919)	grad_norm 9.0113 (28.1643)	mem 28740MB
Train: [0/180][3950/10009]	eta 1:53:48 lr 0.049999	data 0.0005 (0.0054)	batch 1.1199 (1.1270)	loss 6.5333 (23.2838)	grad_norm 8.5811 (27.9243)	mem 28740MB
Train: [0/180][4000/10009]	eta 1:52:51 lr 0.049999	data 0.0005 (0.0053)	batch 1.1315 (1.1269)	loss 6.0023 (23.0783)	grad_norm 7.5451 (27.6860)	mem 28740MB
Train: [0/180][4050/10009]	eta 1:51:54 lr 0.049999	data 0.0005 (0.0052)	batch 1.1397 (1.1268)	loss 6.7279 (22.8735)	grad_norm 8.8896 (27.4488)	mem 28740MB
Train: [0/180][4100/10009]	eta 1:50:57 lr 0.049999	data 0.0005 (0.0052)	batch 1.1136 (1.1266)	loss 6.6155 (22.6729)	grad_norm 8.0001 (27.2166)	mem 28740MB
Train: [0/180][4150/10009]	eta 1:50:00 lr 0.049999	data 0.0005 (0.0051)	batch 1.1051 (1.1265)	loss 6.2124 (22.4745)	grad_norm 7.9127 (26.9898)	mem 28740MB
Train: [0/180][4200/10009]	eta 1:49:03 lr 0.049999	data 0.0005 (0.0051)	batch 1.1019 (1.1264)	loss 5.6607 (22.2789)	grad_norm 7.1692 (26.7647)	mem 28740MB
Train: [0/180][4250/10009]	eta 1:48:06 lr 0.049999	data 0.0005 (0.0050)	batch 1.1180 (1.1264)	loss 5.2078 (22.0849)	grad_norm 6.7084 (26.5422)	mem 28740MB
Train: [0/180][4300/10009]	eta 1:47:10 lr 0.049999	data 0.0005 (0.0050)	batch 1.1128 (1.1263)	loss 5.7095 (21.8941)	grad_norm 7.7719 (26.3243)	mem 28740MB
Train: [0/180][4350/10009]	eta 1:46:13 lr 0.049999	data 0.0006 (0.0049)	batch 1.1073 (1.1263)	loss 5.6113 (21.7077)	grad_norm 7.6514 (26.1101)	mem 28740MB
Train: [0/180][4400/10009]	eta 1:45:16 lr 0.049999	data 0.0006 (0.0049)	batch 1.1149 (1.1262)	loss 5.7709 (21.5222)	grad_norm 10.2229 (25.8971)	mem 28740MB
Train: [0/180][4450/10009]	eta 1:44:20 lr 0.049999	data 0.0006 (0.0048)	batch 1.1212 (1.1261)	loss 5.5522 (21.3419)	grad_norm 7.1192 (25.6901)	mem 28740MB
Train: [0/180][4500/10009]	eta 1:43:23 lr 0.049999	data 0.0005 (0.0048)	batch 1.1175 (1.1260)	loss 5.2670 (21.1642)	grad_norm 7.3054 (25.4856)	mem 28740MB
Train: [0/180][4550/10009]	eta 1:42:26 lr 0.049999	data 0.0005 (0.0047)	batch 1.1102 (1.1259)	loss 5.0260 (20.9883)	grad_norm 6.8389 (25.2833)	mem 28740MB
Train: [0/180][4600/10009]	eta 1:41:29 lr 0.049999	data 0.0005 (0.0047)	batch 1.1382 (1.1259)	loss 4.9967 (20.8166)	grad_norm 6.6727 (25.0855)	mem 28740MB
Train: [0/180][4650/10009]	eta 1:40:33 lr 0.049999	data 0.0005 (0.0046)	batch 1.1469 (1.1258)	loss 5.2373 (20.6497)	grad_norm 6.1985 (24.8933)	mem 28740MB
Train: [0/180][4700/10009]	eta 1:39:36 lr 0.049999	data 0.0005 (0.0046)	batch 1.2616 (1.1258)	loss 4.4832 (20.4839)	grad_norm 5.8047 (24.7008)	mem 28740MB
Train: [0/180][4750/10009]	eta 1:38:39 lr 0.049999	data 0.0009 (0.0046)	batch 1.1430 (1.1257)	loss 4.9482 (20.3208)	grad_norm 6.5269 (24.5130)	mem 28740MB
Train: [0/180][4800/10009]	eta 1:37:43 lr 0.049999	data 0.0002 (0.0045)	batch 1.1046 (1.1256)	loss 4.8631 (20.1615)	grad_norm 6.1983 (24.3275)	mem 28740MB
Train: [0/180][4850/10009]	eta 1:36:46 lr 0.049999	data 0.0005 (0.0045)	batch 1.1107 (1.1255)	loss 4.9140 (20.0041)	grad_norm 7.1949 (24.1439)	mem 28740MB
Train: [0/180][4900/10009]	eta 1:35:49 lr 0.049999	data 0.0005 (0.0044)	batch 1.1264 (1.1254)	loss 4.8630 (19.8489)	grad_norm 7.3519 (23.9633)	mem 28740MB
Train: [0/180][4950/10009]	eta 1:34:53 lr 0.049999	data 0.0005 (0.0044)	batch 1.1097 (1.1254)	loss 4.1733 (19.6974)	grad_norm 5.8375 (23.7856)	mem 28740MB
Train: [0/180][5000/10009]	eta 1:33:56 lr 0.049999	data 0.0005 (0.0044)	batch 1.1188 (1.1253)	loss 4.7643 (19.5486)	grad_norm 6.1539 (23.6107)	mem 28740MB
Train: [0/180][5050/10009]	eta 1:32:59 lr 0.049999	data 0.0005 (0.0043)	batch 1.1091 (1.1252)	loss 5.0862 (19.4024)	grad_norm 6.3645 (23.4388)	mem 28740MB
Train: [0/180][5100/10009]	eta 1:32:03 lr 0.049999	data 0.0007 (0.0043)	batch 1.1105 (1.1251)	loss 4.5658 (19.2589)	grad_norm 5.7374 (23.2703)	mem 28740MB
Train: [0/180][5150/10009]	eta 1:31:06 lr 0.049999	data 0.0005 (0.0042)	batch 1.1173 (1.1251)	loss 5.0125 (19.1171)	grad_norm 6.1914 (23.1030)	mem 28740MB
Train: [0/180][5200/10009]	eta 1:30:10 lr 0.049999	data 0.0018 (0.0042)	batch 1.1018 (1.1250)	loss 4.6421 (18.9784)	grad_norm 5.9226 (22.9386)	mem 28740MB
Train: [0/180][5250/10009]	eta 1:29:13 lr 0.049999	data 0.0005 (0.0042)	batch 1.1088 (1.1249)	loss 4.7720 (18.8418)	grad_norm 6.2699 (22.7764)	mem 28740MB
Train: [0/180][5300/10009]	eta 1:28:17 lr 0.049999	data 0.0005 (0.0041)	batch 1.1098 (1.1249)	loss 4.3340 (18.7073)	grad_norm 5.5002 (22.6163)	mem 28740MB
Train: [0/180][5350/10009]	eta 1:27:20 lr 0.049999	data 0.0004 (0.0041)	batch 1.1135 (1.1248)	loss 4.6351 (18.5751)	grad_norm 6.1468 (22.4587)	mem 28740MB
Train: [0/180][5400/10009]	eta 1:26:23 lr 0.049999	data 0.0005 (0.0041)	batch 1.1179 (1.1247)	loss 4.5356 (18.4445)	grad_norm 5.3267 (22.3026)	mem 28740MB
Train: [0/180][5450/10009]	eta 1:25:27 lr 0.049999	data 0.0006 (0.0040)	batch 1.1224 (1.1247)	loss 3.9553 (18.3162)	grad_norm 5.4287 (22.1488)	mem 28740MB
Train: [0/180][5500/10009]	eta 1:24:31 lr 0.049999	data 0.0006 (0.0040)	batch 1.1275 (1.1247)	loss 4.3974 (18.1895)	grad_norm 4.9404 (21.9971)	mem 28740MB
Train: [0/180][5550/10009]	eta 1:23:35 lr 0.049999	data 0.0005 (0.0040)	batch 1.1426 (1.1247)	loss 4.5895 (18.0653)	grad_norm 5.5476 (21.8479)	mem 28740MB
Train: [0/180][5600/10009]	eta 1:22:38 lr 0.049999	data 0.0004 (0.0039)	batch 1.1157 (1.1247)	loss 4.2198 (17.9435)	grad_norm 5.3116 (21.7006)	mem 28740MB
Train: [0/180][5650/10009]	eta 1:21:42 lr 0.049999	data 0.0005 (0.0039)	batch 1.1461 (1.1248)	loss 3.9525 (17.8234)	grad_norm 4.9341 (21.5549)	mem 28740MB
Train: [0/180][5700/10009]	eta 1:20:46 lr 0.049999	data 0.0005 (0.0039)	batch 1.1413 (1.1248)	loss 4.3817 (17.7051)	grad_norm 4.7801 (21.4113)	mem 28740MB
Train: [0/180][5750/10009]	eta 1:19:50 lr 0.049999	data 0.0005 (0.0039)	batch 1.1353 (1.1248)	loss 4.1919 (17.5888)	grad_norm 4.7980 (21.2690)	mem 28740MB
Train: [0/180][5800/10009]	eta 1:18:54 lr 0.049999	data 0.0005 (0.0038)	batch 1.1229 (1.1248)	loss 4.2962 (17.4746)	grad_norm 5.0405 (21.1294)	mem 28740MB
Train: [0/180][5850/10009]	eta 1:17:57 lr 0.049999	data 0.0004 (0.0038)	batch 1.1214 (1.1247)	loss 4.6276 (17.3618)	grad_norm 5.2522 (20.9916)	mem 28740MB
Train: [0/180][5900/10009]	eta 1:17:01 lr 0.049999	data 0.0009 (0.0038)	batch 1.1182 (1.1248)	loss 4.3917 (17.2504)	grad_norm 4.9185 (20.8547)	mem 28740MB
Train: [0/180][5950/10009]	eta 1:16:05 lr 0.049999	data 0.0006 (0.0037)	batch 1.0990 (1.1247)	loss 4.2629 (17.1402)	grad_norm 4.7170 (20.7196)	mem 28740MB
Train: [0/180][6000/10009]	eta 1:15:09 lr 0.049999	data 0.0005 (0.0037)	batch 1.1176 (1.1247)	loss 4.1494 (17.0317)	grad_norm 5.1076 (20.5860)	mem 28740MB
Train: [0/180][6050/10009]	eta 1:14:12 lr 0.049999	data 0.0005 (0.0037)	batch 1.1082 (1.1246)	loss 3.9751 (16.9247)	grad_norm 4.2930 (20.4539)	mem 28740MB
Train: [0/180][6100/10009]	eta 1:13:16 lr 0.049999	data 0.0005 (0.0037)	batch 1.1373 (1.1246)	loss 4.0726 (16.8189)	grad_norm 4.6179 (20.3236)	mem 28740MB
Train: [0/180][6150/10009]	eta 1:12:19 lr 0.049999	data 0.0005 (0.0036)	batch 1.1159 (1.1246)	loss 4.0440 (16.7148)	grad_norm 4.4829 (20.1955)	mem 28740MB
Train: [0/180][6200/10009]	eta 1:11:23 lr 0.049999	data 0.0005 (0.0036)	batch 1.1120 (1.1245)	loss 3.8807 (16.6121)	grad_norm 4.1511 (20.0680)	mem 28740MB
Train: [0/180][6250/10009]	eta 1:10:26 lr 0.049999	data 0.0005 (0.0036)	batch 1.1165 (1.1245)	loss 4.4899 (16.5107)	grad_norm 4.6332 (19.9420)	mem 28740MB
Train: [0/180][6300/10009]	eta 1:09:30 lr 0.049998	data 0.0004 (0.0036)	batch 1.1044 (1.1244)	loss 3.6976 (16.4107)	grad_norm 4.2109 (19.8180)	mem 28740MB
Train: [0/180][6350/10009]	eta 1:08:34 lr 0.049998	data 0.0005 (0.0035)	batch 1.1298 (1.1244)	loss 4.1565 (16.3121)	grad_norm 4.3179 (19.6949)	mem 28740MB
Train: [0/180][6400/10009]	eta 1:07:37 lr 0.049998	data 0.0005 (0.0035)	batch 1.1186 (1.1244)	loss 3.9623 (16.2143)	grad_norm 4.1717 (19.5727)	mem 28740MB
Train: [0/180][6450/10009]	eta 1:06:41 lr 0.049998	data 0.0005 (0.0035)	batch 1.1339 (1.1244)	loss 3.7380 (16.1181)	grad_norm 4.0317 (19.4526)	mem 28740MB
Train: [0/180][6500/10009]	eta 1:05:45 lr 0.049998	data 0.0005 (0.0035)	batch 1.1122 (1.1244)	loss 3.5938 (16.0234)	grad_norm 3.7454 (19.3339)	mem 28740MB
Train: [0/180][6550/10009]	eta 1:04:49 lr 0.049998	data 0.0005 (0.0035)	batch 1.1134 (1.1244)	loss 3.9085 (15.9293)	grad_norm 4.3334 (19.2160)	mem 28740MB
Train: [0/180][6600/10009]	eta 1:03:52 lr 0.049998	data 0.0005 (0.0034)	batch 1.1160 (1.1244)	loss 3.4609 (15.8367)	grad_norm 3.7001 (19.1001)	mem 28740MB
Train: [0/180][6650/10009]	eta 1:02:56 lr 0.049998	data 0.0006 (0.0034)	batch 1.1057 (1.1243)	loss 3.7269 (15.7453)	grad_norm 3.6437 (18.9851)	mem 28740MB
Train: [0/180][6700/10009]	eta 1:02:00 lr 0.049998	data 0.0005 (0.0034)	batch 1.1055 (1.1243)	loss 3.3448 (15.6551)	grad_norm 3.5972 (18.8711)	mem 28740MB
Train: [0/180][6750/10009]	eta 1:01:04 lr 0.049998	data 0.0005 (0.0034)	batch 1.1199 (1.1243)	loss 3.8695 (15.5660)	grad_norm 3.6970 (18.7590)	mem 28740MB
Train: [0/180][6800/10009]	eta 1:00:07 lr 0.049998	data 0.0005 (0.0033)	batch 1.1216 (1.1243)	loss 3.7732 (15.4776)	grad_norm 3.9070 (18.6480)	mem 28740MB
Train: [0/180][6850/10009]	eta 0:59:11 lr 0.049998	data 0.0005 (0.0033)	batch 1.1369 (1.1243)	loss 3.6752 (15.3911)	grad_norm 3.4742 (18.5387)	mem 28740MB
Train: [0/180][6900/10009]	eta 0:58:15 lr 0.049998	data 0.0005 (0.0033)	batch 1.1195 (1.1243)	loss 3.5893 (15.3052)	grad_norm 3.6997 (18.4304)	mem 28740MB
Train: [0/180][6950/10009]	eta 0:57:19 lr 0.049998	data 0.0005 (0.0033)	batch 1.1110 (1.1242)	loss 3.3537 (15.2204)	grad_norm 3.5913 (18.3232)	mem 28740MB
Train: [0/180][7000/10009]	eta 0:56:22 lr 0.049998	data 0.0005 (0.0033)	batch 1.1145 (1.1242)	loss 3.6359 (15.1371)	grad_norm 3.4837 (18.2176)	mem 28740MB
Train: [0/180][7050/10009]	eta 0:55:26 lr 0.049998	data 0.0006 (0.0032)	batch 1.2999 (1.1243)	loss 3.0635 (15.0543)	grad_norm 3.1545 (18.1127)	mem 28740MB
Train: [0/180][7100/10009]	eta 0:54:30 lr 0.049998	data 0.0010 (0.0032)	batch 1.1101 (1.1243)	loss 3.6550 (14.9727)	grad_norm 3.3582 (18.0091)	mem 28740MB
Train: [0/180][7150/10009]	eta 0:53:34 lr 0.049998	data 0.0005 (0.0032)	batch 1.1207 (1.1242)	loss 3.8608 (14.8919)	grad_norm 3.6871 (17.9063)	mem 28740MB
Train: [0/180][7200/10009]	eta 0:52:38 lr 0.049998	data 0.0005 (0.0032)	batch 1.1202 (1.1243)	loss 3.5167 (14.8121)	grad_norm 3.2635 (17.8053)	mem 28740MB
Train: [0/180][7250/10009]	eta 0:51:41 lr 0.049998	data 0.0005 (0.0032)	batch 1.1230 (1.1242)	loss 3.5961 (14.7333)	grad_norm 3.2681 (17.7049)	mem 28740MB
Train: [0/180][7300/10009]	eta 0:50:45 lr 0.049998	data 0.0005 (0.0032)	batch 1.1082 (1.1242)	loss 3.4744 (14.6556)	grad_norm 3.2595 (17.6061)	mem 28740MB
Train: [0/180][7350/10009]	eta 0:49:49 lr 0.049998	data 0.0005 (0.0031)	batch 1.1194 (1.1242)	loss 3.1938 (14.5783)	grad_norm 3.1595 (17.5077)	mem 28740MB
Train: [0/180][7400/10009]	eta 0:48:53 lr 0.049998	data 0.0006 (0.0031)	batch 1.1286 (1.1242)	loss 3.2885 (14.5021)	grad_norm 3.1333 (17.4109)	mem 28740MB
Train: [0/180][7450/10009]	eta 0:47:56 lr 0.049998	data 0.0005 (0.0031)	batch 1.1172 (1.1242)	loss 3.2686 (14.4266)	grad_norm 3.0009 (17.3147)	mem 28740MB
Train: [0/180][7500/10009]	eta 0:47:00 lr 0.049998	data 0.0005 (0.0031)	batch 1.1413 (1.1242)	loss 3.1074 (14.3522)	grad_norm 2.9109 (17.2201)	mem 28740MB
Train: [0/180][7550/10009]	eta 0:46:04 lr 0.049998	data 0.0005 (0.0031)	batch 1.1334 (1.1242)	loss 3.3269 (14.2789)	grad_norm 3.0549 (17.1262)	mem 28740MB
Train: [0/180][7600/10009]	eta 0:45:08 lr 0.049998	data 0.0005 (0.0031)	batch 1.1186 (1.1242)	loss 3.3606 (14.2065)	grad_norm 3.0825 (17.0338)	mem 28740MB
Train: [0/180][7650/10009]	eta 0:44:12 lr 0.049998	data 0.0005 (0.0030)	batch 1.1195 (1.1242)	loss 3.2309 (14.1350)	grad_norm 2.9226 (16.9425)	mem 28740MB
Train: [0/180][7700/10009]	eta 0:43:15 lr 0.049998	data 0.0005 (0.0030)	batch 1.1180 (1.1242)	loss 3.0084 (14.0644)	grad_norm 2.9605 (16.8521)	mem 28740MB
Train: [0/180][7750/10009]	eta 0:42:19 lr 0.049998	data 0.0005 (0.0030)	batch 1.1243 (1.1242)	loss 3.1022 (13.9945)	grad_norm 2.8181 (16.7626)	mem 28740MB
Train: [0/180][7800/10009]	eta 0:41:23 lr 0.049998	data 0.0006 (0.0030)	batch 1.0971 (1.1242)	loss 2.9428 (13.9255)	grad_norm 2.6548 (16.6740)	mem 28740MB
Train: [0/180][7850/10009]	eta 0:40:27 lr 0.049998	data 0.0005 (0.0030)	batch 1.1188 (1.1242)	loss 3.0671 (13.8570)	grad_norm 2.6585 (16.5862)	mem 28740MB
Train: [0/180][7900/10009]	eta 0:39:31 lr 0.049998	data 0.0006 (0.0030)	batch 1.1116 (1.1242)	loss 3.2179 (13.7894)	grad_norm 2.9195 (16.4995)	mem 28740MB
Train: [0/180][7950/10009]	eta 0:38:34 lr 0.049998	data 0.0005 (0.0029)	batch 1.1034 (1.1242)	loss 3.3105 (13.7227)	grad_norm 2.9899 (16.4138)	mem 28740MB
Train: [0/180][8000/10009]	eta 0:37:38 lr 0.049998	data 0.0005 (0.0029)	batch 1.1196 (1.1242)	loss 3.2119 (13.6569)	grad_norm 2.7862 (16.3288)	mem 28740MB
Train: [0/180][8050/10009]	eta 0:36:42 lr 0.049998	data 0.0005 (0.0029)	batch 1.1184 (1.1242)	loss 2.9876 (13.5913)	grad_norm 2.5515 (16.2447)	mem 28740MB
Train: [0/180][8100/10009]	eta 0:35:46 lr 0.049998	data 0.0005 (0.0029)	batch 1.1141 (1.1242)	loss 3.0530 (13.5269)	grad_norm 2.8304 (16.1618)	mem 28740MB
Train: [0/180][8150/10009]	eta 0:34:49 lr 0.049997	data 0.0005 (0.0029)	batch 1.1226 (1.1242)	loss 3.3490 (13.4631)	grad_norm 2.7947 (16.0796)	mem 28740MB
Train: [0/180][8200/10009]	eta 0:33:53 lr 0.049997	data 0.0007 (0.0029)	batch 1.1203 (1.1242)	loss 3.0309 (13.3997)	grad_norm 2.7267 (15.9985)	mem 28740MB
Train: [0/180][8250/10009]	eta 0:32:57 lr 0.049997	data 0.0006 (0.0029)	batch 1.1059 (1.1242)	loss 3.3887 (13.3373)	grad_norm 3.1083 (15.9182)	mem 28740MB
Train: [0/180][8300/10009]	eta 0:32:01 lr 0.049997	data 0.0005 (0.0028)	batch 1.1187 (1.1242)	loss 3.1821 (13.2752)	grad_norm 2.8339 (15.8384)	mem 28740MB
Train: [0/180][8350/10009]	eta 0:31:05 lr 0.049997	data 0.0005 (0.0028)	batch 1.1195 (1.1242)	loss 3.2689 (13.2142)	grad_norm 2.6485 (15.7598)	mem 28740MB
Train: [0/180][8400/10009]	eta 0:30:08 lr 0.049997	data 0.0005 (0.0028)	batch 1.1150 (1.1242)	loss 2.8607 (13.1538)	grad_norm 2.5587 (15.6817)	mem 28740MB
Train: [0/180][8450/10009]	eta 0:29:12 lr 0.049997	data 0.0006 (0.0028)	batch 1.1236 (1.1242)	loss 2.7943 (13.0937)	grad_norm 2.5206 (15.6047)	mem 28740MB
Train: [0/180][8500/10009]	eta 0:28:16 lr 0.049997	data 0.0005 (0.0028)	batch 1.1186 (1.1243)	loss 2.8746 (13.0349)	grad_norm 2.8216 (15.5286)	mem 28740MB
Train: [0/180][8550/10009]	eta 0:27:20 lr 0.049997	data 0.0006 (0.0028)	batch 1.1449 (1.1243)	loss 3.0683 (12.9765)	grad_norm 2.5656 (15.4530)	mem 28740MB
Train: [0/180][8600/10009]	eta 0:26:24 lr 0.049997	data 0.0005 (0.0028)	batch 1.1189 (1.1242)	loss 2.7640 (12.9185)	grad_norm 2.6895 (15.3781)	mem 28740MB
Train: [0/180][8650/10009]	eta 0:25:27 lr 0.049997	data 0.0005 (0.0028)	batch 1.1207 (1.1242)	loss 2.7629 (12.8614)	grad_norm 2.5852 (15.3041)	mem 28740MB
Train: [0/180][8700/10009]	eta 0:24:31 lr 0.049997	data 0.0006 (0.0027)	batch 1.1258 (1.1242)	loss 3.1617 (12.8046)	grad_norm 2.8192 (15.2311)	mem 28740MB
Train: [0/180][8750/10009]	eta 0:23:35 lr 0.049997	data 0.0005 (0.0027)	batch 1.1373 (1.1242)	loss 3.2112 (12.7486)	grad_norm 2.4982 (15.1585)	mem 28740MB
Train: [0/180][8800/10009]	eta 0:22:39 lr 0.049997	data 0.0005 (0.0027)	batch 1.1161 (1.1242)	loss 3.1113 (12.6932)	grad_norm 2.5945 (15.0868)	mem 28740MB
Train: [0/180][8850/10009]	eta 0:21:42 lr 0.049997	data 0.0007 (0.0027)	batch 1.1352 (1.1242)	loss 2.7589 (12.6382)	grad_norm 2.3236 (15.0158)	mem 28740MB
Train: [0/180][8900/10009]	eta 0:20:46 lr 0.049997	data 0.0005 (0.0027)	batch 1.1061 (1.1242)	loss 2.8125 (12.5836)	grad_norm 2.3616 (14.9454)	mem 28740MB
Train: [0/180][8950/10009]	eta 0:19:50 lr 0.049997	data 0.0005 (0.0027)	batch 1.1251 (1.1242)	loss 2.8535 (12.5298)	grad_norm 2.5876 (14.8761)	mem 28740MB
Train: [0/180][9000/10009]	eta 0:18:54 lr 0.049997	data 0.0005 (0.0027)	batch 1.1109 (1.1242)	loss 2.9319 (12.4765)	grad_norm 2.5771 (14.8072)	mem 28740MB
Train: [0/180][9050/10009]	eta 0:17:58 lr 0.049997	data 0.0006 (0.0027)	batch 1.1239 (1.1242)	loss 3.0401 (12.4237)	grad_norm 2.4172 (14.7389)	mem 28740MB
Train: [0/180][9100/10009]	eta 0:17:01 lr 0.049997	data 0.0005 (0.0026)	batch 1.1303 (1.1242)	loss 2.9265 (12.3715)	grad_norm 2.3172 (14.6714)	mem 28740MB
Train: [0/180][9150/10009]	eta 0:16:05 lr 0.049997	data 0.0005 (0.0026)	batch 1.1220 (1.1242)	loss 2.7853 (12.3199)	grad_norm 2.4774 (14.6047)	mem 28740MB
Train: [0/180][9200/10009]	eta 0:15:09 lr 0.049997	data 0.0005 (0.0026)	batch 1.1239 (1.1242)	loss 3.0253 (12.2685)	grad_norm 2.6614 (14.5384)	mem 28740MB
Train: [0/180][9250/10009]	eta 0:14:13 lr 0.049997	data 0.0005 (0.0026)	batch 1.1270 (1.1242)	loss 2.8807 (12.2179)	grad_norm 2.2132 (14.4728)	mem 28740MB
Train: [0/180][9300/10009]	eta 0:13:17 lr 0.049997	data 0.0005 (0.0026)	batch 1.1117 (1.1242)	loss 3.1229 (12.1678)	grad_norm 2.5891 (14.4079)	mem 28740MB
Train: [0/180][9350/10009]	eta 0:12:20 lr 0.049997	data 0.0005 (0.0026)	batch 1.1278 (1.1242)	loss 2.8542 (12.1181)	grad_norm 2.2631 (14.3435)	mem 28740MB
Train: [0/180][9400/10009]	eta 0:11:24 lr 0.049997	data 0.0006 (0.0026)	batch 1.2990 (1.1242)	loss 3.1183 (12.0692)	grad_norm 2.5718 (14.2802)	mem 28740MB
Train: [0/180][9450/10009]	eta 0:10:28 lr 0.049997	data 0.0005 (0.0026)	batch 1.1159 (1.1242)	loss 2.9771 (12.0207)	grad_norm 2.1955 (14.2173)	mem 28740MB
Train: [0/180][9500/10009]	eta 0:09:32 lr 0.049997	data 0.0005 (0.0026)	batch 1.1117 (1.1242)	loss 2.7411 (11.9726)	grad_norm 2.2692 (14.1549)	mem 28740MB
Train: [0/180][9550/10009]	eta 0:08:36 lr 0.049997	data 0.0005 (0.0025)	batch 1.1044 (1.1242)	loss 2.9027 (11.9249)	grad_norm 2.3205 (14.0932)	mem 28740MB
Train: [0/180][9600/10009]	eta 0:07:39 lr 0.049996	data 0.0005 (0.0025)	batch 1.1299 (1.1242)	loss 2.8645 (11.8778)	grad_norm 2.3758 (14.0319)	mem 28740MB
Train: [0/180][9650/10009]	eta 0:06:43 lr 0.049996	data 0.0005 (0.0025)	batch 1.1170 (1.1242)	loss 2.9082 (11.8311)	grad_norm 2.2398 (13.9712)	mem 28740MB
Train: [0/180][9700/10009]	eta 0:05:47 lr 0.049996	data 0.0005 (0.0025)	batch 1.1170 (1.1242)	loss 2.7358 (11.7849)	grad_norm 2.2773 (13.9112)	mem 28740MB
Train: [0/180][9750/10009]	eta 0:04:51 lr 0.049996	data 0.0005 (0.0025)	batch 1.1535 (1.1242)	loss 2.9676 (11.7390)	grad_norm 2.5067 (13.8515)	mem 28740MB
Train: [0/180][9800/10009]	eta 0:03:54 lr 0.049996	data 0.0004 (0.0025)	batch 1.1089 (1.1242)	loss 2.9390 (11.6936)	grad_norm 2.3021 (13.7926)	mem 28740MB
Train: [0/180][9850/10009]	eta 0:02:58 lr 0.049996	data 0.0006 (0.0025)	batch 1.1298 (1.1242)	loss 3.1944 (11.6487)	grad_norm 2.3901 (13.7342)	mem 28740MB
Train: [0/180][9900/10009]	eta 0:02:02 lr 0.049996	data 0.0005 (0.0025)	batch 1.1059 (1.1242)	loss 2.8730 (11.6041)	grad_norm 2.3506 (13.6765)	mem 28740MB
Train: [0/180][9950/10009]	eta 0:01:06 lr 0.049996	data 0.0005 (0.0025)	batch 1.1448 (1.1242)	loss 2.3779 (11.5600)	grad_norm 2.0644 (13.6191)	mem 28740MB
Train: [0/180][10000/10009]	eta 0:00:10 lr 0.049996	data 0.0002 (0.0025)	batch 1.0966 (1.1242)	loss 2.4723 (11.5161)	grad_norm 2.2503 (13.5623)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 0 training takes 3:07:32
Test: [0/391]	Time 13.104 (13.104)	Loss 1.0508 (1.0508)	Acc@1 74.219 (74.219)	Acc@5 92.969 (92.969)	Mem 28740MB
Test: [50/391]	Time 0.285 (0.521)	Loss 0.9194 (1.4256)	Acc@1 75.781 (65.717)	Acc@5 91.406 (86.336)	Mem 28740MB
Test: [100/391]	Time 0.300 (0.396)	Loss 1.4835 (1.4483)	Acc@1 59.375 (63.204)	Acc@5 85.938 (87.028)	Mem 28740MB
Test: [150/391]	Time 0.267 (0.353)	Loss 1.4570 (1.4169)	Acc@1 54.688 (64.197)	Acc@5 89.844 (87.521)	Mem 28740MB
Test: [200/391]	Time 0.289 (0.332)	Loss 1.6581 (1.5732)	Acc@1 56.250 (61.482)	Acc@5 85.938 (85.199)	Mem 28740MB
Test: [250/391]	Time 0.293 (0.319)	Loss 1.8376 (1.6654)	Acc@1 63.281 (59.998)	Acc@5 78.906 (83.696)	Mem 28740MB
Test: [300/391]	Time 0.271 (0.311)	Loss 1.7244 (1.7457)	Acc@1 68.750 (58.656)	Acc@5 81.250 (82.447)	Mem 28740MB
Test: [350/391]	Time 0.283 (0.305)	Loss 1.7898 (1.8081)	Acc@1 61.719 (57.530)	Acc@5 79.688 (81.490)	Mem 28740MB
 * Acc@1 57.912 Acc@5 81.818
Accuracy of the network on the 50000 test images: 57.91%
Max accuracy (after decay): 57.91%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [1/180][0/10009]	eta 2 days, 12:42:50 lr 0.049996	data 20.6083 (20.6083)	batch 21.8374 (21.8374)	loss 2.4098 (2.4098)	grad_norm 2.3526 (2.3526)	mem 28740MB
Train: [1/180][50/10009]	eta 4:13:52 lr 0.049996	data 0.0005 (0.4046)	batch 1.1127 (1.5295)	loss 2.8129 (2.7875)	grad_norm 2.2132 (2.2615)	mem 28740MB
Train: [1/180][100/10009]	eta 3:39:01 lr 0.049996	data 0.0005 (0.2046)	batch 1.1099 (1.3262)	loss 2.6236 (2.7907)	grad_norm 2.0213 (2.2419)	mem 28740MB
Train: [1/180][150/10009]	eta 3:26:48 lr 0.049996	data 0.0005 (0.1370)	batch 1.1181 (1.2585)	loss 2.6751 (2.8012)	grad_norm 2.2983 (2.2560)	mem 28740MB
Train: [1/180][200/10009]	eta 3:20:24 lr 0.049996	data 0.0006 (0.1031)	batch 1.1238 (1.2259)	loss 2.5152 (2.7782)	grad_norm 2.1290 (2.2478)	mem 28740MB
Train: [1/180][250/10009]	eta 3:16:00 lr 0.049996	data 0.0005 (0.0827)	batch 1.1377 (1.2051)	loss 3.0066 (2.7821)	grad_norm 2.1803 (2.2396)	mem 28740MB
Train: [1/180][300/10009]	eta 3:12:47 lr 0.049996	data 0.0005 (0.0690)	batch 1.1194 (1.1914)	loss 2.6702 (2.7843)	grad_norm 2.2252 (2.2394)	mem 28740MB
Train: [1/180][350/10009]	eta 3:10:10 lr 0.049996	data 0.0010 (0.0593)	batch 1.1431 (1.1813)	loss 2.8822 (2.7822)	grad_norm 2.1653 (2.2344)	mem 28740MB
Train: [1/180][400/10009]	eta 3:08:00 lr 0.049996	data 0.0005 (0.0520)	batch 1.1163 (1.1740)	loss 2.3630 (2.7765)	grad_norm 2.1861 (2.2281)	mem 28740MB
Train: [1/180][450/10009]	eta 3:06:09 lr 0.049996	data 0.0006 (0.0463)	batch 1.1037 (1.1685)	loss 2.6809 (2.7772)	grad_norm 2.1821 (2.2257)	mem 28740MB
Train: [1/180][500/10009]	eta 3:04:26 lr 0.049996	data 0.0006 (0.0417)	batch 1.1204 (1.1638)	loss 2.8979 (2.7779)	grad_norm 2.1893 (2.2237)	mem 28740MB
Train: [1/180][550/10009]	eta 3:02:56 lr 0.049996	data 0.0005 (0.0380)	batch 1.1433 (1.1604)	loss 2.7968 (2.7764)	grad_norm 2.1923 (2.2196)	mem 28740MB
Train: [1/180][600/10009]	eta 3:01:31 lr 0.049996	data 0.0004 (0.0349)	batch 1.1437 (1.1575)	loss 2.8213 (2.7711)	grad_norm 2.0006 (2.2146)	mem 28740MB
Train: [1/180][650/10009]	eta 3:00:11 lr 0.049996	data 0.0005 (0.0323)	batch 1.1150 (1.1552)	loss 2.7165 (2.7698)	grad_norm 1.9886 (2.2108)	mem 28740MB
Train: [1/180][700/10009]	eta 2:58:48 lr 0.049996	data 0.0005 (0.0300)	batch 1.1283 (1.1525)	loss 2.5400 (2.7675)	grad_norm 2.0397 (2.2052)	mem 28740MB
Train: [1/180][750/10009]	eta 2:57:34 lr 0.049996	data 0.0005 (0.0280)	batch 1.2921 (1.1507)	loss 2.6537 (2.7685)	grad_norm 2.2671 (2.2023)	mem 28740MB
Train: [1/180][800/10009]	eta 2:56:19 lr 0.049996	data 0.0005 (0.0263)	batch 1.1397 (1.1488)	loss 2.8019 (2.7667)	grad_norm 2.2062 (2.1994)	mem 28740MB
Train: [1/180][850/10009]	eta 2:55:06 lr 0.049996	data 0.0005 (0.0248)	batch 1.1146 (1.1471)	loss 2.8853 (2.7658)	grad_norm 2.0584 (2.1944)	mem 28740MB
Train: [1/180][900/10009]	eta 2:53:56 lr 0.049995	data 0.0009 (0.0235)	batch 1.1273 (1.1458)	loss 2.9753 (2.7656)	grad_norm 2.1261 (2.1913)	mem 28740MB
Train: [1/180][950/10009]	eta 2:52:47 lr 0.049995	data 0.0005 (0.0223)	batch 1.1136 (1.1444)	loss 3.0053 (2.7640)	grad_norm 2.1992 (2.1859)	mem 28740MB
Train: [1/180][1000/10009]	eta 2:51:40 lr 0.049995	data 0.0005 (0.0212)	batch 1.1070 (1.1434)	loss 2.9265 (2.7625)	grad_norm 2.2093 (2.1834)	mem 28740MB
Train: [1/180][1050/10009]	eta 2:50:33 lr 0.049995	data 0.0005 (0.0202)	batch 1.1167 (1.1423)	loss 2.4259 (2.7608)	grad_norm 2.1414 (2.1815)	mem 28740MB
Train: [1/180][1100/10009]	eta 2:49:28 lr 0.049995	data 0.0005 (0.0193)	batch 1.1426 (1.1414)	loss 2.5736 (2.7587)	grad_norm 2.1582 (2.1785)	mem 28740MB
Train: [1/180][1150/10009]	eta 2:48:25 lr 0.049995	data 0.0005 (0.0185)	batch 1.1169 (1.1407)	loss 2.8618 (2.7574)	grad_norm 2.1432 (2.1758)	mem 28740MB
Train: [1/180][1200/10009]	eta 2:47:22 lr 0.049995	data 0.0004 (0.0177)	batch 1.1189 (1.1400)	loss 2.8821 (2.7546)	grad_norm 2.0887 (2.1729)	mem 28740MB
Train: [1/180][1250/10009]	eta 2:46:19 lr 0.049995	data 0.0005 (0.0170)	batch 1.1354 (1.1393)	loss 2.4826 (2.7541)	grad_norm 2.0140 (2.1719)	mem 28740MB
Train: [1/180][1300/10009]	eta 2:45:16 lr 0.049995	data 0.0004 (0.0164)	batch 1.1132 (1.1386)	loss 2.8176 (2.7533)	grad_norm 2.0918 (2.1704)	mem 28740MB
Train: [1/180][1350/10009]	eta 2:44:13 lr 0.049995	data 0.0005 (0.0158)	batch 1.1198 (1.1379)	loss 3.0255 (2.7535)	grad_norm 2.1763 (2.1681)	mem 28740MB
Train: [1/180][1400/10009]	eta 2:43:13 lr 0.049995	data 0.0005 (0.0153)	batch 1.1110 (1.1375)	loss 2.5548 (2.7529)	grad_norm 2.1348 (2.1650)	mem 28740MB
Train: [1/180][1450/10009]	eta 2:42:12 lr 0.049995	data 0.0005 (0.0148)	batch 1.1195 (1.1371)	loss 2.5918 (2.7504)	grad_norm 1.9992 (2.1614)	mem 28740MB
Train: [1/180][1500/10009]	eta 2:41:12 lr 0.049995	data 0.0005 (0.0143)	batch 1.1456 (1.1367)	loss 2.6764 (2.7501)	grad_norm 2.0471 (2.1591)	mem 28740MB
Train: [1/180][1550/10009]	eta 2:40:10 lr 0.049995	data 0.0005 (0.0139)	batch 1.1027 (1.1362)	loss 2.9484 (2.7491)	grad_norm 2.1728 (2.1556)	mem 28740MB
Train: [1/180][1600/10009]	eta 2:39:10 lr 0.049995	data 0.0005 (0.0134)	batch 1.1140 (1.1357)	loss 2.5014 (2.7471)	grad_norm 1.8396 (2.1526)	mem 28740MB
Train: [1/180][1650/10009]	eta 2:38:09 lr 0.049995	data 0.0005 (0.0130)	batch 1.1374 (1.1352)	loss 2.5836 (2.7461)	grad_norm 2.0367 (2.1507)	mem 28740MB
Train: [1/180][1700/10009]	eta 2:37:10 lr 0.049995	data 0.0005 (0.0127)	batch 1.1203 (1.1349)	loss 2.7540 (2.7441)	grad_norm 2.0275 (2.1479)	mem 28740MB
Train: [1/180][1750/10009]	eta 2:36:09 lr 0.049995	data 0.0004 (0.0123)	batch 1.1396 (1.1344)	loss 2.6735 (2.7428)	grad_norm 2.2189 (2.1456)	mem 28740MB
Train: [1/180][1800/10009]	eta 2:35:08 lr 0.049995	data 0.0005 (0.0120)	batch 1.1236 (1.1340)	loss 2.6671 (2.7419)	grad_norm 2.1527 (2.1423)	mem 28740MB
Train: [1/180][1850/10009]	eta 2:34:09 lr 0.049995	data 0.0005 (0.0117)	batch 1.1422 (1.1336)	loss 2.8054 (2.7408)	grad_norm 2.0368 (2.1397)	mem 28740MB
Train: [1/180][1900/10009]	eta 2:33:09 lr 0.049995	data 0.0004 (0.0114)	batch 1.1386 (1.1333)	loss 2.3803 (2.7389)	grad_norm 1.9374 (2.1366)	mem 28740MB
Train: [1/180][1950/10009]	eta 2:32:10 lr 0.049995	data 0.0005 (0.0111)	batch 1.1188 (1.1330)	loss 2.7043 (2.7391)	grad_norm 2.0903 (2.1342)	mem 28740MB
Train: [1/180][2000/10009]	eta 2:31:12 lr 0.049995	data 0.0005 (0.0109)	batch 1.1120 (1.1327)	loss 2.5480 (2.7372)	grad_norm 2.0254 (2.1324)	mem 28740MB
Train: [1/180][2050/10009]	eta 2:30:12 lr 0.049994	data 0.0006 (0.0106)	batch 1.1385 (1.1324)	loss 2.9157 (2.7355)	grad_norm 2.0486 (2.1297)	mem 28740MB
Train: [1/180][2100/10009]	eta 2:29:14 lr 0.049994	data 0.0004 (0.0104)	batch 1.1219 (1.1322)	loss 2.6719 (2.7339)	grad_norm 1.9597 (2.1269)	mem 28740MB
Train: [1/180][2150/10009]	eta 2:28:15 lr 0.049994	data 0.0004 (0.0101)	batch 1.1241 (1.1319)	loss 2.6159 (2.7325)	grad_norm 1.9193 (2.1250)	mem 28740MB
Train: [1/180][2200/10009]	eta 2:27:17 lr 0.049994	data 0.0005 (0.0099)	batch 1.1119 (1.1317)	loss 2.5327 (2.7311)	grad_norm 2.0858 (2.1228)	mem 28740MB
Train: [1/180][2250/10009]	eta 2:26:18 lr 0.049994	data 0.0063 (0.0097)	batch 1.1131 (1.1314)	loss 2.6037 (2.7298)	grad_norm 2.0373 (2.1206)	mem 28740MB
Train: [1/180][2300/10009]	eta 2:25:20 lr 0.049994	data 0.0005 (0.0095)	batch 1.1265 (1.1313)	loss 2.6502 (2.7277)	grad_norm 2.0061 (2.1187)	mem 28740MB
Train: [1/180][2350/10009]	eta 2:24:22 lr 0.049994	data 0.0005 (0.0093)	batch 1.1320 (1.1310)	loss 2.7523 (2.7274)	grad_norm 2.0842 (2.1172)	mem 28740MB
Train: [1/180][2400/10009]	eta 2:23:24 lr 0.049994	data 0.0004 (0.0091)	batch 1.1290 (1.1309)	loss 2.6680 (2.7261)	grad_norm 2.0400 (2.1153)	mem 28740MB
Train: [1/180][2450/10009]	eta 2:22:26 lr 0.049994	data 0.0005 (0.0090)	batch 1.1151 (1.1307)	loss 2.9610 (2.7251)	grad_norm 2.2057 (2.1136)	mem 28740MB
Train: [1/180][2500/10009]	eta 2:21:28 lr 0.049994	data 0.0005 (0.0088)	batch 1.1186 (1.1305)	loss 2.8396 (2.7241)	grad_norm 1.9174 (2.1113)	mem 28740MB
Train: [1/180][2550/10009]	eta 2:20:31 lr 0.049994	data 0.0004 (0.0086)	batch 1.1340 (1.1304)	loss 2.4835 (2.7224)	grad_norm 1.9322 (2.1092)	mem 28740MB
Train: [1/180][2600/10009]	eta 2:19:34 lr 0.049994	data 0.0005 (0.0085)	batch 1.0877 (1.1303)	loss 2.5511 (2.7208)	grad_norm 1.7837 (2.1076)	mem 28740MB
Train: [1/180][2650/10009]	eta 2:18:36 lr 0.049994	data 0.0005 (0.0083)	batch 1.1378 (1.1301)	loss 2.5098 (2.7195)	grad_norm 1.8581 (2.1054)	mem 28740MB
Train: [1/180][2700/10009]	eta 2:17:38 lr 0.049994	data 0.0005 (0.0082)	batch 1.1205 (1.1300)	loss 2.2832 (2.7185)	grad_norm 1.8266 (2.1038)	mem 28740MB
Train: [1/180][2750/10009]	eta 2:16:41 lr 0.049994	data 0.0005 (0.0080)	batch 1.1291 (1.1298)	loss 2.7397 (2.7169)	grad_norm 1.9866 (2.1013)	mem 28740MB
Train: [1/180][2800/10009]	eta 2:15:44 lr 0.049994	data 0.0005 (0.0079)	batch 1.1113 (1.1297)	loss 2.8633 (2.7156)	grad_norm 1.9052 (2.0991)	mem 28740MB
Train: [1/180][2850/10009]	eta 2:14:47 lr 0.049994	data 0.0005 (0.0078)	batch 1.1409 (1.1296)	loss 2.4313 (2.7138)	grad_norm 1.9852 (2.0972)	mem 28740MB
Train: [1/180][2900/10009]	eta 2:13:50 lr 0.049994	data 0.0004 (0.0077)	batch 1.1293 (1.1296)	loss 2.9011 (2.7128)	grad_norm 1.8935 (2.0951)	mem 28740MB
Train: [1/180][2950/10009]	eta 2:12:52 lr 0.049994	data 0.0004 (0.0075)	batch 1.1169 (1.1294)	loss 2.6944 (2.7123)	grad_norm 1.8967 (2.0931)	mem 28740MB
Train: [1/180][3000/10009]	eta 2:11:54 lr 0.049994	data 0.0009 (0.0074)	batch 1.1076 (1.1293)	loss 2.6633 (2.7105)	grad_norm 2.0042 (2.0908)	mem 28740MB
Train: [1/180][3050/10009]	eta 2:10:57 lr 0.049994	data 0.0004 (0.0073)	batch 1.1129 (1.1291)	loss 2.4782 (2.7095)	grad_norm 1.9929 (2.0888)	mem 28740MB
Train: [1/180][3100/10009]	eta 2:10:00 lr 0.049993	data 0.0005 (0.0072)	batch 1.2880 (1.1291)	loss 2.7382 (2.7093)	grad_norm 1.9971 (2.0867)	mem 28740MB
Train: [1/180][3150/10009]	eta 2:09:03 lr 0.049993	data 0.0005 (0.0071)	batch 1.1140 (1.1289)	loss 2.6790 (2.7081)	grad_norm 2.0529 (2.0845)	mem 28740MB
Train: [1/180][3200/10009]	eta 2:08:06 lr 0.049993	data 0.0004 (0.0070)	batch 1.0997 (1.1288)	loss 2.4339 (2.7065)	grad_norm 2.0189 (2.0825)	mem 28740MB
Train: [1/180][3250/10009]	eta 2:07:09 lr 0.049993	data 0.0005 (0.0069)	batch 1.1285 (1.1287)	loss 2.2819 (2.7057)	grad_norm 1.9257 (2.0811)	mem 28740MB
Train: [1/180][3300/10009]	eta 2:06:11 lr 0.049993	data 0.0004 (0.0068)	batch 1.1195 (1.1286)	loss 2.5797 (2.7055)	grad_norm 1.9194 (2.0792)	mem 28740MB
Train: [1/180][3350/10009]	eta 2:05:14 lr 0.049993	data 0.0004 (0.0067)	batch 1.1184 (1.1285)	loss 2.8655 (2.7039)	grad_norm 2.0472 (2.0769)	mem 28740MB
Train: [1/180][3400/10009]	eta 2:04:17 lr 0.049993	data 0.0006 (0.0066)	batch 1.1242 (1.1284)	loss 2.4234 (2.7025)	grad_norm 2.0593 (2.0751)	mem 28740MB
Train: [1/180][3450/10009]	eta 2:03:20 lr 0.049993	data 0.0005 (0.0065)	batch 1.1028 (1.1283)	loss 2.5741 (2.7014)	grad_norm 1.9224 (2.0728)	mem 28740MB
Train: [1/180][3500/10009]	eta 2:02:23 lr 0.049993	data 0.0006 (0.0064)	batch 1.1122 (1.1282)	loss 2.5013 (2.7008)	grad_norm 1.8314 (2.0713)	mem 28740MB
Train: [1/180][3550/10009]	eta 2:01:26 lr 0.049993	data 0.0005 (0.0063)	batch 1.1391 (1.1281)	loss 2.5348 (2.6997)	grad_norm 1.9434 (2.0696)	mem 28740MB
Train: [1/180][3600/10009]	eta 2:00:29 lr 0.049993	data 0.0005 (0.0063)	batch 1.1224 (1.1281)	loss 2.6669 (2.6985)	grad_norm 1.9070 (2.0676)	mem 28740MB
Train: [1/180][3650/10009]	eta 1:59:32 lr 0.049993	data 0.0005 (0.0062)	batch 1.1073 (1.1279)	loss 2.7114 (2.6975)	grad_norm 1.7818 (2.0658)	mem 28740MB
Train: [1/180][3700/10009]	eta 1:58:35 lr 0.049993	data 0.0005 (0.0061)	batch 1.1147 (1.1278)	loss 2.6616 (2.6965)	grad_norm 1.9876 (2.0645)	mem 28740MB
Train: [1/180][3750/10009]	eta 1:57:38 lr 0.049993	data 0.0005 (0.0060)	batch 1.1185 (1.1277)	loss 2.5594 (2.6957)	grad_norm 2.0888 (2.0628)	mem 28740MB
Train: [1/180][3800/10009]	eta 1:56:41 lr 0.049993	data 0.0010 (0.0060)	batch 1.1418 (1.1277)	loss 2.3815 (2.6948)	grad_norm 1.9809 (2.0612)	mem 28740MB
Train: [1/180][3850/10009]	eta 1:55:44 lr 0.049993	data 0.0004 (0.0059)	batch 1.1390 (1.1276)	loss 2.6187 (2.6936)	grad_norm 1.8661 (2.0598)	mem 28740MB
Train: [1/180][3900/10009]	eta 1:54:47 lr 0.049993	data 0.0005 (0.0058)	batch 1.1047 (1.1275)	loss 2.3226 (2.6924)	grad_norm 1.8422 (2.0582)	mem 28740MB
Train: [1/180][3950/10009]	eta 1:53:51 lr 0.049993	data 0.0004 (0.0058)	batch 1.1058 (1.1274)	loss 2.6741 (2.6916)	grad_norm 1.8303 (2.0564)	mem 28740MB
Train: [1/180][4000/10009]	eta 1:52:53 lr 0.049993	data 0.0005 (0.0057)	batch 1.1236 (1.1273)	loss 2.6311 (2.6908)	grad_norm 1.8942 (2.0548)	mem 28740MB
Train: [1/180][4050/10009]	eta 1:51:57 lr 0.049992	data 0.0008 (0.0056)	batch 1.1077 (1.1273)	loss 2.3843 (2.6900)	grad_norm 1.9485 (2.0535)	mem 28740MB
Train: [1/180][4100/10009]	eta 1:51:00 lr 0.049992	data 0.0005 (0.0056)	batch 1.1341 (1.1272)	loss 2.6696 (2.6889)	grad_norm 2.0388 (2.0521)	mem 28740MB
Train: [1/180][4150/10009]	eta 1:50:03 lr 0.049992	data 0.0003 (0.0055)	batch 1.0937 (1.1271)	loss 2.5315 (2.6887)	grad_norm 1.9554 (2.0505)	mem 28740MB
Train: [1/180][4200/10009]	eta 1:49:07 lr 0.049992	data 0.0005 (0.0054)	batch 1.1374 (1.1271)	loss 2.6045 (2.6877)	grad_norm 1.9762 (2.0490)	mem 28740MB
Train: [1/180][4250/10009]	eta 1:48:10 lr 0.049992	data 0.0005 (0.0054)	batch 1.1434 (1.1270)	loss 2.3609 (2.6869)	grad_norm 1.8532 (2.0471)	mem 28740MB
Train: [1/180][4300/10009]	eta 1:47:13 lr 0.049992	data 0.0005 (0.0053)	batch 1.1179 (1.1269)	loss 2.6237 (2.6858)	grad_norm 1.7975 (2.0455)	mem 28740MB
Train: [1/180][4350/10009]	eta 1:46:16 lr 0.049992	data 0.0005 (0.0053)	batch 1.1178 (1.1268)	loss 2.6861 (2.6853)	grad_norm 1.8151 (2.0440)	mem 28740MB
Train: [1/180][4400/10009]	eta 1:45:19 lr 0.049992	data 0.0005 (0.0052)	batch 1.1225 (1.1267)	loss 2.7125 (2.6844)	grad_norm 1.9029 (2.0424)	mem 28740MB
Train: [1/180][4450/10009]	eta 1:44:23 lr 0.049992	data 0.0005 (0.0052)	batch 1.1109 (1.1267)	loss 2.9146 (2.6839)	grad_norm 1.8658 (2.0409)	mem 28740MB
Train: [1/180][4500/10009]	eta 1:43:26 lr 0.049992	data 0.0005 (0.0051)	batch 1.1169 (1.1266)	loss 2.6870 (2.6829)	grad_norm 1.8410 (2.0394)	mem 28740MB
Train: [1/180][4550/10009]	eta 1:42:29 lr 0.049992	data 0.0005 (0.0051)	batch 1.1117 (1.1265)	loss 2.6336 (2.6824)	grad_norm 1.9204 (2.0377)	mem 28740MB
Train: [1/180][4600/10009]	eta 1:41:33 lr 0.049992	data 0.0005 (0.0050)	batch 1.1378 (1.1265)	loss 2.3000 (2.6812)	grad_norm 1.6168 (2.0365)	mem 28740MB
Train: [1/180][4650/10009]	eta 1:40:36 lr 0.049992	data 0.0005 (0.0050)	batch 1.1297 (1.1265)	loss 2.8080 (2.6807)	grad_norm 2.0213 (2.0352)	mem 28740MB
Train: [1/180][4700/10009]	eta 1:39:39 lr 0.049992	data 0.0004 (0.0049)	batch 1.1278 (1.1264)	loss 2.6993 (2.6803)	grad_norm 1.7999 (2.0341)	mem 28740MB
Train: [1/180][4750/10009]	eta 1:38:43 lr 0.049992	data 0.0005 (0.0049)	batch 1.1156 (1.1263)	loss 2.8211 (2.6797)	grad_norm 2.0299 (2.0329)	mem 28740MB
Train: [1/180][4800/10009]	eta 1:37:46 lr 0.049992	data 0.0005 (0.0048)	batch 1.1407 (1.1262)	loss 2.6512 (2.6788)	grad_norm 2.0239 (2.0315)	mem 28740MB
Train: [1/180][4850/10009]	eta 1:36:49 lr 0.049992	data 0.0005 (0.0048)	batch 1.1311 (1.1262)	loss 2.6190 (2.6779)	grad_norm 1.9383 (2.0305)	mem 28740MB
Train: [1/180][4900/10009]	eta 1:35:53 lr 0.049992	data 0.0004 (0.0047)	batch 1.1484 (1.1261)	loss 2.8244 (2.6773)	grad_norm 1.8867 (2.0291)	mem 28740MB
Train: [1/180][4950/10009]	eta 1:34:56 lr 0.049991	data 0.0004 (0.0047)	batch 1.1297 (1.1261)	loss 2.7424 (2.6767)	grad_norm 1.8443 (2.0276)	mem 28740MB
Train: [1/180][5000/10009]	eta 1:34:00 lr 0.049991	data 0.0005 (0.0047)	batch 1.1274 (1.1260)	loss 2.5980 (2.6755)	grad_norm 2.0287 (2.0262)	mem 28740MB
Train: [1/180][5050/10009]	eta 1:33:03 lr 0.049991	data 0.0005 (0.0046)	batch 1.1066 (1.1260)	loss 2.6984 (2.6741)	grad_norm 1.7777 (2.0247)	mem 28740MB
Train: [1/180][5100/10009]	eta 1:32:07 lr 0.049991	data 0.0005 (0.0046)	batch 1.1151 (1.1259)	loss 2.4692 (2.6736)	grad_norm 1.7738 (2.0232)	mem 28740MB
Train: [1/180][5150/10009]	eta 1:31:10 lr 0.049991	data 0.0005 (0.0045)	batch 1.1084 (1.1259)	loss 2.6553 (2.6725)	grad_norm 1.9866 (2.0218)	mem 28740MB
Train: [1/180][5200/10009]	eta 1:30:14 lr 0.049991	data 0.0005 (0.0045)	batch 1.1207 (1.1258)	loss 2.5515 (2.6718)	grad_norm 1.8960 (2.0206)	mem 28740MB
Train: [1/180][5250/10009]	eta 1:29:17 lr 0.049991	data 0.0005 (0.0045)	batch 1.1404 (1.1258)	loss 2.3712 (2.6710)	grad_norm 1.8618 (2.0192)	mem 28740MB
Train: [1/180][5300/10009]	eta 1:28:21 lr 0.049991	data 0.0005 (0.0044)	batch 1.1423 (1.1258)	loss 2.5725 (2.6706)	grad_norm 1.8328 (2.0180)	mem 28740MB
Train: [1/180][5350/10009]	eta 1:27:24 lr 0.049991	data 0.0005 (0.0044)	batch 1.1433 (1.1258)	loss 2.7647 (2.6698)	grad_norm 1.9285 (2.0166)	mem 28740MB
Train: [1/180][5400/10009]	eta 1:26:28 lr 0.049991	data 0.0004 (0.0044)	batch 1.1115 (1.1257)	loss 2.8288 (2.6692)	grad_norm 1.8606 (2.0156)	mem 28740MB
Train: [1/180][5450/10009]	eta 1:25:31 lr 0.049991	data 0.0005 (0.0043)	batch 1.2597 (1.1257)	loss 2.6354 (2.6684)	grad_norm 1.8649 (2.0144)	mem 28740MB
Train: [1/180][5500/10009]	eta 1:24:35 lr 0.049991	data 0.0004 (0.0043)	batch 1.1153 (1.1256)	loss 2.7656 (2.6672)	grad_norm 1.9465 (2.0133)	mem 28740MB
Train: [1/180][5550/10009]	eta 1:23:38 lr 0.049991	data 0.0005 (0.0042)	batch 1.1181 (1.1255)	loss 2.7094 (2.6664)	grad_norm 1.9329 (2.0119)	mem 28740MB
Train: [1/180][5600/10009]	eta 1:22:42 lr 0.049991	data 0.0005 (0.0042)	batch 1.0992 (1.1255)	loss 2.5561 (2.6655)	grad_norm 1.9743 (2.0105)	mem 28740MB
Train: [1/180][5650/10009]	eta 1:21:45 lr 0.049991	data 0.0005 (0.0042)	batch 1.1201 (1.1254)	loss 2.6650 (2.6648)	grad_norm 1.8564 (2.0093)	mem 28740MB
Train: [1/180][5700/10009]	eta 1:20:49 lr 0.049991	data 0.0006 (0.0042)	batch 1.1189 (1.1254)	loss 2.6808 (2.6637)	grad_norm 1.9316 (2.0080)	mem 28740MB
Train: [1/180][5750/10009]	eta 1:19:52 lr 0.049991	data 0.0005 (0.0041)	batch 1.1204 (1.1254)	loss 2.5870 (2.6631)	grad_norm 1.8802 (2.0069)	mem 28740MB
Train: [1/180][5800/10009]	eta 1:18:56 lr 0.049991	data 0.0005 (0.0041)	batch 1.1194 (1.1254)	loss 2.8382 (2.6622)	grad_norm 1.9921 (2.0058)	mem 28740MB
Train: [1/180][5850/10009]	eta 1:18:00 lr 0.049990	data 0.0005 (0.0041)	batch 1.1438 (1.1254)	loss 2.4787 (2.6616)	grad_norm 1.9321 (2.0045)	mem 28740MB
Train: [1/180][5900/10009]	eta 1:17:03 lr 0.049990	data 0.0005 (0.0040)	batch 1.1166 (1.1253)	loss 2.4801 (2.6606)	grad_norm 1.7711 (2.0032)	mem 28740MB
Train: [1/180][5950/10009]	eta 1:16:07 lr 0.049990	data 0.0005 (0.0040)	batch 1.1220 (1.1253)	loss 2.3253 (2.6599)	grad_norm 1.8138 (2.0019)	mem 28740MB
Train: [1/180][6000/10009]	eta 1:15:11 lr 0.049990	data 0.0005 (0.0040)	batch 1.1213 (1.1253)	loss 2.3847 (2.6591)	grad_norm 1.8623 (2.0007)	mem 28740MB
Train: [1/180][6050/10009]	eta 1:14:14 lr 0.049990	data 0.0005 (0.0039)	batch 1.1132 (1.1253)	loss 2.5058 (2.6583)	grad_norm 1.8808 (1.9995)	mem 28740MB
Train: [1/180][6100/10009]	eta 1:13:18 lr 0.049990	data 0.0007 (0.0039)	batch 1.1148 (1.1253)	loss 2.5949 (2.6574)	grad_norm 2.0474 (1.9981)	mem 28740MB
Train: [1/180][6150/10009]	eta 1:12:22 lr 0.049990	data 0.0005 (0.0039)	batch 1.1131 (1.1252)	loss 2.3922 (2.6563)	grad_norm 1.9359 (1.9968)	mem 28740MB
Train: [1/180][6200/10009]	eta 1:11:25 lr 0.049990	data 0.0005 (0.0039)	batch 1.1150 (1.1252)	loss 2.5237 (2.6554)	grad_norm 1.9634 (1.9957)	mem 28740MB
Train: [1/180][6250/10009]	eta 1:10:29 lr 0.049990	data 0.0005 (0.0038)	batch 1.1164 (1.1251)	loss 2.2573 (2.6544)	grad_norm 1.7010 (1.9949)	mem 28740MB
Train: [1/180][6300/10009]	eta 1:09:33 lr 0.049990	data 0.0005 (0.0038)	batch 1.1397 (1.1251)	loss 2.3495 (2.6536)	grad_norm 1.5610 (1.9938)	mem 28740MB
Train: [1/180][6350/10009]	eta 1:08:36 lr 0.049990	data 0.0005 (0.0038)	batch 1.1214 (1.1251)	loss 2.3681 (2.6528)	grad_norm 1.9657 (1.9927)	mem 28740MB
Train: [1/180][6400/10009]	eta 1:07:40 lr 0.049990	data 0.0004 (0.0038)	batch 1.1435 (1.1251)	loss 2.4524 (2.6521)	grad_norm 1.9982 (1.9915)	mem 28740MB
Train: [1/180][6450/10009]	eta 1:06:44 lr 0.049990	data 0.0007 (0.0037)	batch 1.1056 (1.1251)	loss 2.6278 (2.6513)	grad_norm 1.8085 (1.9904)	mem 28740MB
Train: [1/180][6500/10009]	eta 1:05:47 lr 0.049990	data 0.0005 (0.0037)	batch 1.1202 (1.1250)	loss 2.4323 (2.6507)	grad_norm 1.9700 (1.9891)	mem 28740MB
Train: [1/180][6550/10009]	eta 1:04:51 lr 0.049990	data 0.0005 (0.0037)	batch 1.1242 (1.1250)	loss 2.5099 (2.6502)	grad_norm 1.8806 (1.9880)	mem 28740MB
Train: [1/180][6600/10009]	eta 1:03:55 lr 0.049990	data 0.0004 (0.0037)	batch 1.1142 (1.1250)	loss 2.4352 (2.6493)	grad_norm 1.8227 (1.9869)	mem 28740MB
Train: [1/180][6650/10009]	eta 1:02:58 lr 0.049989	data 0.0005 (0.0036)	batch 1.1397 (1.1250)	loss 2.6556 (2.6486)	grad_norm 1.8736 (1.9860)	mem 28740MB
Train: [1/180][6700/10009]	eta 1:02:02 lr 0.049989	data 0.0004 (0.0036)	batch 1.1192 (1.1250)	loss 2.5160 (2.6481)	grad_norm 1.6586 (1.9850)	mem 28740MB
Train: [1/180][6750/10009]	eta 1:01:06 lr 0.049989	data 0.0005 (0.0036)	batch 1.1121 (1.1249)	loss 2.5374 (2.6475)	grad_norm 1.7816 (1.9839)	mem 28740MB
Train: [1/180][6800/10009]	eta 1:00:09 lr 0.049989	data 0.0005 (0.0036)	batch 1.1371 (1.1249)	loss 2.3848 (2.6468)	grad_norm 1.7997 (1.9828)	mem 28740MB
Train: [1/180][6850/10009]	eta 0:59:13 lr 0.049989	data 0.0005 (0.0035)	batch 1.1359 (1.1249)	loss 2.4268 (2.6461)	grad_norm 1.8832 (1.9817)	mem 28740MB
Train: [1/180][6900/10009]	eta 0:58:17 lr 0.049989	data 0.0006 (0.0035)	batch 1.1075 (1.1249)	loss 2.4422 (2.6455)	grad_norm 1.9455 (1.9807)	mem 28740MB
Train: [1/180][6950/10009]	eta 0:57:20 lr 0.049989	data 0.0005 (0.0035)	batch 1.1398 (1.1249)	loss 2.7247 (2.6452)	grad_norm 1.7347 (1.9797)	mem 28740MB
Train: [1/180][7000/10009]	eta 0:56:24 lr 0.049989	data 0.0005 (0.0035)	batch 1.1188 (1.1249)	loss 2.3612 (2.6444)	grad_norm 1.8114 (1.9786)	mem 28740MB
Train: [1/180][7050/10009]	eta 0:55:28 lr 0.049989	data 0.0005 (0.0035)	batch 1.1172 (1.1249)	loss 2.8513 (2.6438)	grad_norm 1.8880 (1.9776)	mem 28740MB
Train: [1/180][7100/10009]	eta 0:54:32 lr 0.049989	data 0.0005 (0.0034)	batch 1.1010 (1.1249)	loss 2.7552 (2.6434)	grad_norm 1.8238 (1.9765)	mem 28740MB
Train: [1/180][7150/10009]	eta 0:53:36 lr 0.049989	data 0.0005 (0.0034)	batch 1.1092 (1.1249)	loss 2.5531 (2.6428)	grad_norm 1.8830 (1.9755)	mem 28740MB
Train: [1/180][7200/10009]	eta 0:52:39 lr 0.049989	data 0.0005 (0.0034)	batch 1.1339 (1.1249)	loss 2.4145 (2.6421)	grad_norm 1.7275 (1.9745)	mem 28740MB
Train: [1/180][7250/10009]	eta 0:51:43 lr 0.049989	data 0.0004 (0.0034)	batch 1.1228 (1.1249)	loss 2.8286 (2.6414)	grad_norm 1.9917 (1.9735)	mem 28740MB
Train: [1/180][7300/10009]	eta 0:50:47 lr 0.049989	data 0.0004 (0.0034)	batch 1.1191 (1.1249)	loss 2.5269 (2.6407)	grad_norm 1.9033 (1.9725)	mem 28740MB
Train: [1/180][7350/10009]	eta 0:49:51 lr 0.049989	data 0.0003 (0.0033)	batch 1.1154 (1.1249)	loss 2.2528 (2.6397)	grad_norm 1.6800 (1.9715)	mem 28740MB
Train: [1/180][7400/10009]	eta 0:48:54 lr 0.049988	data 0.0004 (0.0033)	batch 1.1431 (1.1249)	loss 2.3036 (2.6392)	grad_norm 1.6518 (1.9705)	mem 28740MB
Train: [1/180][7450/10009]	eta 0:47:58 lr 0.049988	data 0.0005 (0.0033)	batch 1.1325 (1.1249)	loss 2.9029 (2.6387)	grad_norm 1.9579 (1.9693)	mem 28740MB
Train: [1/180][7500/10009]	eta 0:47:02 lr 0.049988	data 0.0004 (0.0033)	batch 1.1244 (1.1249)	loss 2.5149 (2.6384)	grad_norm 1.6403 (1.9682)	mem 28740MB
Train: [1/180][7550/10009]	eta 0:46:06 lr 0.049988	data 0.0005 (0.0033)	batch 1.1052 (1.1249)	loss 2.6844 (2.6378)	grad_norm 1.8062 (1.9671)	mem 28740MB
Train: [1/180][7600/10009]	eta 0:45:09 lr 0.049988	data 0.0005 (0.0032)	batch 1.1403 (1.1249)	loss 2.7370 (2.6370)	grad_norm 1.9456 (1.9660)	mem 28740MB
Train: [1/180][7650/10009]	eta 0:44:13 lr 0.049988	data 0.0005 (0.0032)	batch 1.1141 (1.1249)	loss 2.6814 (2.6363)	grad_norm 1.9558 (1.9650)	mem 28740MB
Train: [1/180][7700/10009]	eta 0:43:17 lr 0.049988	data 0.0004 (0.0032)	batch 1.1177 (1.1249)	loss 2.3290 (2.6357)	grad_norm 1.7695 (1.9641)	mem 28740MB
Train: [1/180][7750/10009]	eta 0:42:20 lr 0.049988	data 0.0005 (0.0032)	batch 1.1001 (1.1248)	loss 2.5409 (2.6350)	grad_norm 1.9432 (1.9631)	mem 28740MB
Train: [1/180][7800/10009]	eta 0:41:24 lr 0.049988	data 0.0005 (0.0032)	batch 1.2935 (1.1248)	loss 2.5810 (2.6346)	grad_norm 1.8782 (1.9620)	mem 28740MB
Train: [1/180][7850/10009]	eta 0:40:28 lr 0.049988	data 0.0005 (0.0032)	batch 1.1356 (1.1248)	loss 2.5612 (2.6338)	grad_norm 1.8525 (1.9612)	mem 28740MB
Train: [1/180][7900/10009]	eta 0:39:32 lr 0.049988	data 0.0005 (0.0031)	batch 1.1220 (1.1248)	loss 2.4440 (2.6331)	grad_norm 1.8637 (1.9603)	mem 28740MB
Train: [1/180][7950/10009]	eta 0:38:35 lr 0.049988	data 0.0005 (0.0031)	batch 1.1059 (1.1248)	loss 2.6361 (2.6324)	grad_norm 1.8770 (1.9594)	mem 28740MB
Train: [1/180][8000/10009]	eta 0:37:39 lr 0.049988	data 0.0004 (0.0031)	batch 1.1010 (1.1248)	loss 2.4821 (2.6320)	grad_norm 1.7916 (1.9585)	mem 28740MB
Train: [1/180][8050/10009]	eta 0:36:43 lr 0.049988	data 0.0005 (0.0031)	batch 1.1226 (1.1247)	loss 2.2876 (2.6313)	grad_norm 1.6692 (1.9575)	mem 28740MB
Train: [1/180][8100/10009]	eta 0:35:47 lr 0.049988	data 0.0005 (0.0031)	batch 1.1099 (1.1247)	loss 2.4695 (2.6308)	grad_norm 1.8292 (1.9567)	mem 28740MB
Train: [1/180][8150/10009]	eta 0:34:50 lr 0.049987	data 0.0004 (0.0031)	batch 1.1081 (1.1247)	loss 2.6106 (2.6304)	grad_norm 1.9227 (1.9559)	mem 28740MB
Train: [1/180][8200/10009]	eta 0:33:54 lr 0.049987	data 0.0005 (0.0030)	batch 1.1306 (1.1247)	loss 2.4110 (2.6299)	grad_norm 1.7746 (1.9551)	mem 28740MB
Train: [1/180][8250/10009]	eta 0:32:58 lr 0.049987	data 0.0005 (0.0030)	batch 1.1213 (1.1247)	loss 2.4247 (2.6293)	grad_norm 2.0772 (1.9540)	mem 28740MB
Train: [1/180][8300/10009]	eta 0:32:02 lr 0.049987	data 0.0005 (0.0030)	batch 1.1120 (1.1247)	loss 2.8080 (2.6286)	grad_norm 1.8727 (1.9530)	mem 28740MB
Train: [1/180][8350/10009]	eta 0:31:05 lr 0.049987	data 0.0005 (0.0030)	batch 1.1200 (1.1247)	loss 2.4547 (2.6281)	grad_norm 1.9202 (1.9522)	mem 28740MB
Train: [1/180][8400/10009]	eta 0:30:09 lr 0.049987	data 0.0005 (0.0030)	batch 1.1256 (1.1247)	loss 2.5055 (2.6277)	grad_norm 1.7218 (1.9512)	mem 28740MB
Train: [1/180][8450/10009]	eta 0:29:13 lr 0.049987	data 0.0005 (0.0030)	batch 1.1165 (1.1247)	loss 2.3449 (2.6272)	grad_norm 1.7354 (1.9503)	mem 28740MB
Train: [1/180][8500/10009]	eta 0:28:17 lr 0.049987	data 0.0005 (0.0030)	batch 1.1131 (1.1247)	loss 2.6278 (2.6265)	grad_norm 1.8779 (1.9495)	mem 28740MB
Train: [1/180][8550/10009]	eta 0:27:20 lr 0.049987	data 0.0004 (0.0029)	batch 1.1171 (1.1247)	loss 2.4880 (2.6260)	grad_norm 1.8165 (1.9487)	mem 28740MB
Train: [1/180][8600/10009]	eta 0:26:24 lr 0.049987	data 0.0004 (0.0029)	batch 1.1226 (1.1247)	loss 2.6597 (2.6256)	grad_norm 1.5368 (1.9477)	mem 28740MB
Train: [1/180][8650/10009]	eta 0:25:28 lr 0.049987	data 0.0004 (0.0029)	batch 1.1193 (1.1247)	loss 2.2991 (2.6250)	grad_norm 1.6764 (1.9466)	mem 28740MB
Train: [1/180][8700/10009]	eta 0:24:32 lr 0.049987	data 0.0005 (0.0029)	batch 1.0989 (1.1247)	loss 2.5685 (2.6244)	grad_norm 1.7190 (1.9458)	mem 28740MB
Train: [1/180][8750/10009]	eta 0:23:35 lr 0.049987	data 0.0004 (0.0029)	batch 1.1114 (1.1246)	loss 2.6956 (2.6240)	grad_norm 1.9125 (1.9451)	mem 28740MB
Train: [1/180][8800/10009]	eta 0:22:39 lr 0.049987	data 0.0005 (0.0029)	batch 1.1122 (1.1246)	loss 2.3948 (2.6231)	grad_norm 1.8114 (1.9442)	mem 28740MB
Train: [1/180][8850/10009]	eta 0:21:43 lr 0.049986	data 0.0005 (0.0029)	batch 1.1071 (1.1246)	loss 2.3937 (2.6226)	grad_norm 1.9017 (1.9433)	mem 28740MB
Train: [1/180][8900/10009]	eta 0:20:47 lr 0.049986	data 0.0005 (0.0028)	batch 1.1122 (1.1246)	loss 2.7551 (2.6222)	grad_norm 1.7665 (1.9423)	mem 28740MB
Train: [1/180][8950/10009]	eta 0:19:50 lr 0.049986	data 0.0005 (0.0028)	batch 1.1403 (1.1246)	loss 2.4259 (2.6216)	grad_norm 1.9237 (1.9413)	mem 28740MB
Train: [1/180][9000/10009]	eta 0:18:54 lr 0.049986	data 0.0005 (0.0028)	batch 1.1323 (1.1246)	loss 2.4044 (2.6209)	grad_norm 1.9590 (1.9405)	mem 28740MB
Train: [1/180][9050/10009]	eta 0:17:58 lr 0.049986	data 0.0005 (0.0028)	batch 1.1150 (1.1246)	loss 2.5107 (2.6203)	grad_norm 1.8473 (1.9397)	mem 28740MB
Train: [1/180][9100/10009]	eta 0:17:02 lr 0.049986	data 0.0005 (0.0028)	batch 1.1123 (1.1246)	loss 2.3491 (2.6198)	grad_norm 1.7631 (1.9387)	mem 28740MB
Train: [1/180][9150/10009]	eta 0:16:06 lr 0.049986	data 0.0004 (0.0028)	batch 1.1211 (1.1246)	loss 2.5635 (2.6194)	grad_norm 1.8487 (1.9379)	mem 28740MB
Train: [1/180][9200/10009]	eta 0:15:09 lr 0.049986	data 0.0005 (0.0028)	batch 1.1222 (1.1246)	loss 2.0149 (2.6187)	grad_norm 1.5329 (1.9370)	mem 28740MB
Train: [1/180][9250/10009]	eta 0:14:13 lr 0.049986	data 0.0004 (0.0028)	batch 1.1464 (1.1246)	loss 2.4698 (2.6179)	grad_norm 1.6386 (1.9360)	mem 28740MB
Train: [1/180][9300/10009]	eta 0:13:17 lr 0.049986	data 0.0005 (0.0027)	batch 1.1207 (1.1246)	loss 2.5620 (2.6172)	grad_norm 1.7416 (1.9351)	mem 28740MB
Train: [1/180][9350/10009]	eta 0:12:21 lr 0.049986	data 0.0005 (0.0027)	batch 1.1161 (1.1246)	loss 2.2991 (2.6167)	grad_norm 1.8202 (1.9343)	mem 28740MB
Train: [1/180][9400/10009]	eta 0:11:24 lr 0.049986	data 0.0004 (0.0027)	batch 1.1180 (1.1246)	loss 2.4039 (2.6161)	grad_norm 1.8945 (1.9333)	mem 28740MB
Train: [1/180][9450/10009]	eta 0:10:28 lr 0.049986	data 0.0005 (0.0027)	batch 1.1439 (1.1246)	loss 2.2317 (2.6154)	grad_norm 1.5952 (1.9325)	mem 28740MB
Train: [1/180][9500/10009]	eta 0:09:32 lr 0.049986	data 0.0005 (0.0027)	batch 1.1177 (1.1246)	loss 2.3391 (2.6148)	grad_norm 1.7127 (1.9316)	mem 28740MB
Train: [1/180][9550/10009]	eta 0:08:36 lr 0.049985	data 0.0004 (0.0027)	batch 1.1377 (1.1246)	loss 2.4150 (2.6141)	grad_norm 1.7572 (1.9307)	mem 28740MB
Train: [1/180][9600/10009]	eta 0:07:39 lr 0.049985	data 0.0005 (0.0027)	batch 1.1180 (1.1246)	loss 2.6262 (2.6136)	grad_norm 1.7489 (1.9298)	mem 28740MB
Train: [1/180][9650/10009]	eta 0:06:43 lr 0.049985	data 0.0005 (0.0027)	batch 1.1179 (1.1245)	loss 2.7211 (2.6133)	grad_norm 1.6947 (1.9290)	mem 28740MB
Train: [1/180][9700/10009]	eta 0:05:47 lr 0.049985	data 0.0005 (0.0027)	batch 1.1027 (1.1245)	loss 2.3669 (2.6128)	grad_norm 1.7341 (1.9282)	mem 28740MB
Train: [1/180][9750/10009]	eta 0:04:51 lr 0.049985	data 0.0006 (0.0026)	batch 1.1146 (1.1245)	loss 2.2725 (2.6123)	grad_norm 1.5481 (1.9274)	mem 28740MB
Train: [1/180][9800/10009]	eta 0:03:55 lr 0.049985	data 0.0004 (0.0026)	batch 1.1122 (1.1245)	loss 2.5686 (2.6116)	grad_norm 1.7038 (1.9267)	mem 28740MB
Train: [1/180][9850/10009]	eta 0:02:58 lr 0.049985	data 0.0007 (0.0026)	batch 1.1415 (1.1245)	loss 2.7825 (2.6111)	grad_norm 1.8323 (1.9259)	mem 28740MB
Train: [1/180][9900/10009]	eta 0:02:02 lr 0.049985	data 0.0004 (0.0026)	batch 1.1445 (1.1245)	loss 2.4783 (2.6106)	grad_norm 1.8498 (1.9254)	mem 28740MB
Train: [1/180][9950/10009]	eta 0:01:06 lr 0.049985	data 0.0005 (0.0026)	batch 1.1044 (1.1244)	loss 2.1870 (2.6102)	grad_norm 1.7584 (1.9245)	mem 28740MB
Train: [1/180][10000/10009]	eta 0:00:10 lr 0.049985	data 0.0003 (0.0026)	batch 1.1000 (1.1244)	loss 2.7103 (2.6096)	grad_norm 1.8302 (1.9237)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 1 training takes 3:07:34
Test: [0/391]	Time 13.237 (13.237)	Loss 0.7533 (0.7533)	Acc@1 82.031 (82.031)	Acc@5 93.750 (93.750)	Mem 28740MB
Test: [50/391]	Time 0.255 (0.521)	Loss 0.6513 (1.1287)	Acc@1 85.156 (72.580)	Acc@5 95.312 (90.594)	Mem 28740MB
Test: [100/391]	Time 0.278 (0.394)	Loss 1.0135 (1.1543)	Acc@1 73.438 (70.398)	Acc@5 92.969 (91.197)	Mem 28740MB
Test: [150/391]	Time 0.255 (0.352)	Loss 0.9425 (1.1218)	Acc@1 67.969 (71.420)	Acc@5 95.312 (91.546)	Mem 28740MB
Test: [200/391]	Time 0.255 (0.331)	Loss 1.6023 (1.2629)	Acc@1 57.812 (68.696)	Acc@5 85.156 (89.576)	Mem 28740MB
Test: [250/391]	Time 0.270 (0.318)	Loss 1.1431 (1.3407)	Acc@1 71.875 (67.253)	Acc@5 87.500 (88.253)	Mem 28740MB
Test: [300/391]	Time 0.270 (0.310)	Loss 1.6676 (1.4069)	Acc@1 66.406 (65.942)	Acc@5 82.031 (87.344)	Mem 28740MB
Test: [350/391]	Time 0.270 (0.303)	Loss 1.7063 (1.4618)	Acc@1 64.062 (64.868)	Acc@5 83.594 (86.525)	Mem 28740MB
 * Acc@1 65.016 Acc@5 86.646
Accuracy of the network on the 50000 test images: 65.02%
Max accuracy (after decay): 65.02%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [2/180][0/10009]	eta 2 days, 11:03:06 lr 0.049985	data 20.0833 (20.0833)	batch 21.2395 (21.2395)	loss 2.7904 (2.7904)	grad_norm 1.8659 (1.8659)	mem 28740MB
Train: [2/180][50/10009]	eta 4:11:33 lr 0.049985	data 0.0004 (0.3943)	batch 1.1214 (1.5155)	loss 2.7786 (2.4936)	grad_norm 1.9642 (1.7480)	mem 28740MB
Train: [2/180][100/10009]	eta 3:37:43 lr 0.049985	data 0.0004 (0.1993)	batch 1.1382 (1.3183)	loss 2.6262 (2.4797)	grad_norm 1.8112 (1.7333)	mem 28740MB
Train: [2/180][150/10009]	eta 3:25:53 lr 0.049985	data 0.0005 (0.1335)	batch 1.1111 (1.2530)	loss 2.5512 (2.4706)	grad_norm 1.6975 (1.7341)	mem 28740MB
Train: [2/180][200/10009]	eta 3:19:16 lr 0.049984	data 0.0009 (0.1004)	batch 1.1204 (1.2190)	loss 2.6290 (2.4699)	grad_norm 1.9183 (1.7379)	mem 28740MB
Train: [2/180][250/10009]	eta 3:15:17 lr 0.049984	data 0.0004 (0.0805)	batch 1.1450 (1.2007)	loss 2.2872 (2.4655)	grad_norm 1.6865 (1.7307)	mem 28740MB
Train: [2/180][300/10009]	eta 3:12:09 lr 0.049984	data 0.0005 (0.0672)	batch 1.1106 (1.1875)	loss 2.6705 (2.4718)	grad_norm 1.6626 (1.7339)	mem 28740MB
Train: [2/180][350/10009]	eta 3:09:46 lr 0.049984	data 0.0004 (0.0577)	batch 1.1253 (1.1788)	loss 2.4427 (2.4737)	grad_norm 1.8119 (1.7381)	mem 28740MB
Train: [2/180][400/10009]	eta 3:07:41 lr 0.049984	data 0.0005 (0.0506)	batch 1.1186 (1.1720)	loss 2.5149 (2.4751)	grad_norm 1.7541 (1.7371)	mem 28740MB
Train: [2/180][450/10009]	eta 3:05:55 lr 0.049984	data 0.0004 (0.0450)	batch 1.1351 (1.1671)	loss 2.1601 (2.4697)	grad_norm 1.7580 (1.7349)	mem 28740MB
Train: [2/180][500/10009]	eta 3:04:15 lr 0.049984	data 0.0006 (0.0406)	batch 1.1446 (1.1626)	loss 2.8511 (2.4687)	grad_norm 1.8750 (1.7362)	mem 28740MB
Train: [2/180][550/10009]	eta 3:02:40 lr 0.049984	data 0.0005 (0.0369)	batch 1.1101 (1.1588)	loss 2.6884 (2.4677)	grad_norm 1.8899 (1.7387)	mem 28740MB
Train: [2/180][600/10009]	eta 3:01:13 lr 0.049984	data 0.0005 (0.0339)	batch 1.1239 (1.1556)	loss 2.5177 (2.4686)	grad_norm 1.6169 (1.7383)	mem 28740MB
Train: [2/180][650/10009]	eta 2:59:52 lr 0.049984	data 0.0005 (0.0314)	batch 1.1414 (1.1532)	loss 2.3971 (2.4691)	grad_norm 1.7143 (1.7372)	mem 28740MB
Train: [2/180][700/10009]	eta 2:58:33 lr 0.049984	data 0.0005 (0.0292)	batch 1.1102 (1.1509)	loss 2.4566 (2.4664)	grad_norm 1.6571 (1.7381)	mem 28740MB
Train: [2/180][750/10009]	eta 2:57:16 lr 0.049984	data 0.0005 (0.0272)	batch 1.1180 (1.1488)	loss 2.1783 (2.4637)	grad_norm 1.6534 (1.7388)	mem 28740MB
Train: [2/180][800/10009]	eta 2:56:03 lr 0.049984	data 0.0005 (0.0256)	batch 1.0960 (1.1471)	loss 2.3392 (2.4638)	grad_norm 1.6005 (1.7404)	mem 28740MB
Train: [2/180][850/10009]	eta 2:54:56 lr 0.049983	data 0.0006 (0.0241)	batch 1.1014 (1.1460)	loss 2.5197 (2.4649)	grad_norm 1.7957 (1.7411)	mem 28740MB
Train: [2/180][900/10009]	eta 2:53:47 lr 0.049983	data 0.0004 (0.0228)	batch 1.1442 (1.1448)	loss 2.4885 (2.4656)	grad_norm 1.8138 (1.7412)	mem 28740MB
Train: [2/180][950/10009]	eta 2:52:41 lr 0.049983	data 0.0005 (0.0216)	batch 1.1182 (1.1437)	loss 2.4590 (2.4627)	grad_norm 1.6801 (1.7410)	mem 28740MB
Train: [2/180][1000/10009]	eta 2:51:34 lr 0.049983	data 0.0005 (0.0206)	batch 1.1212 (1.1427)	loss 2.4846 (2.4624)	grad_norm 1.7091 (1.7391)	mem 28740MB
Train: [2/180][1050/10009]	eta 2:50:30 lr 0.049983	data 0.0005 (0.0196)	batch 1.1001 (1.1420)	loss 2.3585 (2.4615)	grad_norm 1.6122 (1.7386)	mem 28740MB
Train: [2/180][1100/10009]	eta 2:49:26 lr 0.049983	data 0.0005 (0.0188)	batch 1.0998 (1.1412)	loss 2.2264 (2.4638)	grad_norm 1.6443 (1.7377)	mem 28740MB
Train: [2/180][1150/10009]	eta 2:48:22 lr 0.049983	data 0.0005 (0.0180)	batch 1.1232 (1.1404)	loss 2.3041 (2.4635)	grad_norm 1.6341 (1.7374)	mem 28740MB
Train: [2/180][1200/10009]	eta 2:47:22 lr 0.049983	data 0.0005 (0.0172)	batch 1.1251 (1.1400)	loss 2.4287 (2.4645)	grad_norm 1.7303 (1.7366)	mem 28740MB
Train: [2/180][1250/10009]	eta 2:46:20 lr 0.049983	data 0.0005 (0.0166)	batch 1.1277 (1.1395)	loss 2.5275 (2.4657)	grad_norm 1.8276 (1.7359)	mem 28740MB
Train: [2/180][1300/10009]	eta 2:45:17 lr 0.049983	data 0.0005 (0.0160)	batch 1.1230 (1.1388)	loss 2.8084 (2.4673)	grad_norm 1.7997 (1.7356)	mem 28740MB
Train: [2/180][1350/10009]	eta 2:44:14 lr 0.049983	data 0.0005 (0.0154)	batch 1.1125 (1.1381)	loss 2.6027 (2.4684)	grad_norm 1.7230 (1.7352)	mem 28740MB
Train: [2/180][1400/10009]	eta 2:43:11 lr 0.049983	data 0.0005 (0.0148)	batch 1.1194 (1.1374)	loss 2.8251 (2.4686)	grad_norm 1.9247 (1.7348)	mem 28740MB
Train: [2/180][1450/10009]	eta 2:42:11 lr 0.049982	data 0.0005 (0.0144)	batch 1.1065 (1.1370)	loss 2.4082 (2.4691)	grad_norm 1.7484 (1.7340)	mem 28740MB
Train: [2/180][1500/10009]	eta 2:41:11 lr 0.049982	data 0.0005 (0.0139)	batch 1.1231 (1.1366)	loss 2.5493 (2.4701)	grad_norm 1.7415 (1.7334)	mem 28740MB
Train: [2/180][1550/10009]	eta 2:40:11 lr 0.049982	data 0.0004 (0.0135)	batch 1.1195 (1.1363)	loss 2.5145 (2.4700)	grad_norm 1.5788 (1.7331)	mem 28740MB
Train: [2/180][1600/10009]	eta 2:39:11 lr 0.049982	data 0.0004 (0.0131)	batch 1.1202 (1.1358)	loss 2.4389 (2.4702)	grad_norm 1.7609 (1.7338)	mem 28740MB
Train: [2/180][1650/10009]	eta 2:38:10 lr 0.049982	data 0.0004 (0.0127)	batch 1.1113 (1.1354)	loss 2.4791 (2.4706)	grad_norm 1.6265 (1.7338)	mem 28740MB
Train: [2/180][1700/10009]	eta 2:37:11 lr 0.049982	data 0.0005 (0.0123)	batch 1.1496 (1.1351)	loss 2.6692 (2.4712)	grad_norm 1.8239 (1.7342)	mem 28740MB
Train: [2/180][1750/10009]	eta 2:36:11 lr 0.049982	data 0.0005 (0.0120)	batch 1.1263 (1.1347)	loss 2.4844 (2.4704)	grad_norm 1.7380 (1.7340)	mem 28740MB
Train: [2/180][1800/10009]	eta 2:35:12 lr 0.049982	data 0.0005 (0.0117)	batch 1.1365 (1.1344)	loss 2.6512 (2.4697)	grad_norm 1.6964 (1.7333)	mem 28740MB
Train: [2/180][1850/10009]	eta 2:34:12 lr 0.049982	data 0.0005 (0.0114)	batch 1.1140 (1.1340)	loss 2.4194 (2.4694)	grad_norm 1.5984 (1.7331)	mem 28740MB
Train: [2/180][1900/10009]	eta 2:33:13 lr 0.049982	data 0.0005 (0.0111)	batch 1.1148 (1.1337)	loss 2.4639 (2.4681)	grad_norm 1.7078 (1.7323)	mem 28740MB
Train: [2/180][1950/10009]	eta 2:32:14 lr 0.049982	data 0.0005 (0.0108)	batch 1.1282 (1.1334)	loss 2.4927 (2.4689)	grad_norm 1.8046 (1.7331)	mem 28740MB
Train: [2/180][2000/10009]	eta 2:31:15 lr 0.049982	data 0.0005 (0.0105)	batch 1.1186 (1.1332)	loss 2.4126 (2.4693)	grad_norm 1.6340 (1.7338)	mem 28740MB
Train: [2/180][2050/10009]	eta 2:30:17 lr 0.049981	data 0.0010 (0.0103)	batch 1.1191 (1.1329)	loss 2.3538 (2.4700)	grad_norm 1.6169 (1.7336)	mem 28740MB
Train: [2/180][2100/10009]	eta 2:29:17 lr 0.049981	data 0.0005 (0.0101)	batch 1.1038 (1.1326)	loss 2.5722 (2.4703)	grad_norm 1.8348 (1.7337)	mem 28740MB
Train: [2/180][2150/10009]	eta 2:28:19 lr 0.049981	data 0.0004 (0.0098)	batch 1.1045 (1.1323)	loss 2.6035 (2.4692)	grad_norm 1.8423 (1.7324)	mem 28740MB
Train: [2/180][2200/10009]	eta 2:27:20 lr 0.049981	data 0.0005 (0.0096)	batch 1.0968 (1.1320)	loss 2.7489 (2.4686)	grad_norm 1.9811 (1.7319)	mem 28740MB
Train: [2/180][2250/10009]	eta 2:26:23 lr 0.049981	data 0.0005 (0.0094)	batch 1.1252 (1.1320)	loss 2.3773 (2.4689)	grad_norm 1.6778 (1.7322)	mem 28740MB
Train: [2/180][2300/10009]	eta 2:25:25 lr 0.049981	data 0.0004 (0.0092)	batch 1.1075 (1.1318)	loss 2.4129 (2.4688)	grad_norm 1.6503 (1.7318)	mem 28740MB
Train: [2/180][2350/10009]	eta 2:24:27 lr 0.049981	data 0.0005 (0.0091)	batch 1.1236 (1.1316)	loss 2.7738 (2.4677)	grad_norm 1.7204 (1.7316)	mem 28740MB
Train: [2/180][2400/10009]	eta 2:23:29 lr 0.049981	data 0.0004 (0.0089)	batch 1.1136 (1.1315)	loss 2.9928 (2.4683)	grad_norm 1.8181 (1.7317)	mem 28740MB
Train: [2/180][2450/10009]	eta 2:22:31 lr 0.049981	data 0.0004 (0.0087)	batch 1.1293 (1.1313)	loss 2.4745 (2.4679)	grad_norm 1.6098 (1.7309)	mem 28740MB
Train: [2/180][2500/10009]	eta 2:21:34 lr 0.049981	data 0.0007 (0.0085)	batch 1.1160 (1.1312)	loss 2.4233 (2.4682)	grad_norm 1.6101 (1.7305)	mem 28740MB
Train: [2/180][2550/10009]	eta 2:20:36 lr 0.049981	data 0.0005 (0.0084)	batch 1.1128 (1.1310)	loss 2.5216 (2.4674)	grad_norm 1.6340 (1.7302)	mem 28740MB
Train: [2/180][2600/10009]	eta 2:19:38 lr 0.049981	data 0.0005 (0.0082)	batch 1.1121 (1.1309)	loss 2.2870 (2.4671)	grad_norm 1.6165 (1.7294)	mem 28740MB
Train: [2/180][2650/10009]	eta 2:18:41 lr 0.049980	data 0.0005 (0.0081)	batch 1.1203 (1.1307)	loss 2.4840 (2.4670)	grad_norm 1.7560 (1.7288)	mem 28740MB
Train: [2/180][2700/10009]	eta 2:17:43 lr 0.049980	data 0.0005 (0.0079)	batch 1.1183 (1.1306)	loss 2.3468 (2.4666)	grad_norm 1.6016 (1.7282)	mem 28740MB
Train: [2/180][2750/10009]	eta 2:16:46 lr 0.049980	data 0.0005 (0.0078)	batch 1.1431 (1.1305)	loss 2.4609 (2.4665)	grad_norm 1.9643 (1.7278)	mem 28740MB
Train: [2/180][2800/10009]	eta 2:15:48 lr 0.049980	data 0.0004 (0.0077)	batch 1.1223 (1.1303)	loss 2.7791 (2.4668)	grad_norm 1.6759 (1.7277)	mem 28740MB
Train: [2/180][2850/10009]	eta 2:14:51 lr 0.049980	data 0.0005 (0.0076)	batch 1.1190 (1.1302)	loss 2.2954 (2.4667)	grad_norm 1.6404 (1.7274)	mem 28740MB
Train: [2/180][2900/10009]	eta 2:13:53 lr 0.049980	data 0.0005 (0.0074)	batch 1.1008 (1.1301)	loss 2.4045 (2.4667)	grad_norm 1.7692 (1.7268)	mem 28740MB
Train: [2/180][2950/10009]	eta 2:12:56 lr 0.049980	data 0.0004 (0.0073)	batch 1.1295 (1.1300)	loss 2.4879 (2.4660)	grad_norm 1.6870 (1.7268)	mem 28740MB
Train: [2/180][3000/10009]	eta 2:11:59 lr 0.049980	data 0.0005 (0.0072)	batch 1.1162 (1.1300)	loss 2.5213 (2.4651)	grad_norm 1.7667 (1.7263)	mem 28740MB
Train: [2/180][3050/10009]	eta 2:11:02 lr 0.049980	data 0.0004 (0.0071)	batch 1.1113 (1.1299)	loss 2.4225 (2.4650)	grad_norm 1.7456 (1.7259)	mem 28740MB
Train: [2/180][3100/10009]	eta 2:10:05 lr 0.049980	data 0.0005 (0.0070)	batch 1.1343 (1.1297)	loss 2.5116 (2.4652)	grad_norm 1.5755 (1.7256)	mem 28740MB
Train: [2/180][3150/10009]	eta 2:09:08 lr 0.049980	data 0.0005 (0.0069)	batch 1.1230 (1.1296)	loss 2.3809 (2.4652)	grad_norm 1.8214 (1.7251)	mem 28740MB
Train: [2/180][3200/10009]	eta 2:08:10 lr 0.049980	data 0.0004 (0.0068)	batch 1.1187 (1.1295)	loss 2.7911 (2.4650)	grad_norm 1.7659 (1.7246)	mem 28740MB
Train: [2/180][3250/10009]	eta 2:07:12 lr 0.049979	data 0.0005 (0.0067)	batch 1.1400 (1.1293)	loss 2.5238 (2.4648)	grad_norm 1.6591 (1.7239)	mem 28740MB
Train: [2/180][3300/10009]	eta 2:06:15 lr 0.049979	data 0.0005 (0.0066)	batch 1.1092 (1.1291)	loss 2.5625 (2.4650)	grad_norm 2.0031 (1.7237)	mem 28740MB
Train: [2/180][3350/10009]	eta 2:05:17 lr 0.049979	data 0.0004 (0.0065)	batch 1.1097 (1.1289)	loss 2.2393 (2.4652)	grad_norm 1.7917 (1.7237)	mem 28740MB
Train: [2/180][3400/10009]	eta 2:04:20 lr 0.049979	data 0.0003 (0.0064)	batch 1.1179 (1.1288)	loss 2.5689 (2.4649)	grad_norm 1.6700 (1.7233)	mem 28740MB
Train: [2/180][3450/10009]	eta 2:03:22 lr 0.049979	data 0.0005 (0.0063)	batch 1.1348 (1.1286)	loss 2.5773 (2.4645)	grad_norm 1.6140 (1.7226)	mem 28740MB
Train: [2/180][3500/10009]	eta 2:02:25 lr 0.049979	data 0.0005 (0.0063)	batch 1.1156 (1.1285)	loss 2.6535 (2.4645)	grad_norm 1.7935 (1.7226)	mem 28740MB
Train: [2/180][3550/10009]	eta 2:01:28 lr 0.049979	data 0.0005 (0.0062)	batch 1.1039 (1.1284)	loss 2.3232 (2.4643)	grad_norm 1.7336 (1.7224)	mem 28740MB
Train: [2/180][3600/10009]	eta 2:00:30 lr 0.049979	data 0.0009 (0.0061)	batch 1.1037 (1.1282)	loss 2.5321 (2.4645)	grad_norm 1.8264 (1.7222)	mem 28740MB
Train: [2/180][3650/10009]	eta 1:59:33 lr 0.049979	data 0.0005 (0.0060)	batch 1.1129 (1.1281)	loss 2.2599 (2.4645)	grad_norm 1.8007 (1.7218)	mem 28740MB
Train: [2/180][3700/10009]	eta 1:58:36 lr 0.049979	data 0.0004 (0.0059)	batch 1.1132 (1.1280)	loss 2.6053 (2.4639)	grad_norm 1.6317 (1.7211)	mem 28740MB
Train: [2/180][3750/10009]	eta 1:57:39 lr 0.049979	data 0.0005 (0.0059)	batch 1.1147 (1.1279)	loss 2.5681 (2.4640)	grad_norm 1.8032 (1.7208)	mem 28740MB
Train: [2/180][3800/10009]	eta 1:56:43 lr 0.049978	data 0.0005 (0.0058)	batch 1.1043 (1.1279)	loss 2.2076 (2.4638)	grad_norm 1.6208 (1.7202)	mem 28740MB
Train: [2/180][3850/10009]	eta 1:55:46 lr 0.049978	data 0.0005 (0.0057)	batch 1.1081 (1.1278)	loss 2.5120 (2.4639)	grad_norm 1.7265 (1.7203)	mem 28740MB
Train: [2/180][3900/10009]	eta 1:54:49 lr 0.049978	data 0.0005 (0.0057)	batch 1.1081 (1.1278)	loss 1.9792 (2.4638)	grad_norm 1.5858 (1.7198)	mem 28740MB
Train: [2/180][3950/10009]	eta 1:53:52 lr 0.049978	data 0.0005 (0.0056)	batch 1.1593 (1.1277)	loss 2.3936 (2.4638)	grad_norm 1.5815 (1.7195)	mem 28740MB
Train: [2/180][4000/10009]	eta 1:52:55 lr 0.049978	data 0.0004 (0.0055)	batch 1.1170 (1.1276)	loss 2.4535 (2.4632)	grad_norm 1.8025 (1.7191)	mem 28740MB
Train: [2/180][4050/10009]	eta 1:51:58 lr 0.049978	data 0.0005 (0.0055)	batch 1.1188 (1.1275)	loss 2.2125 (2.4631)	grad_norm 1.7053 (1.7187)	mem 28740MB
Train: [2/180][4100/10009]	eta 1:51:02 lr 0.049978	data 0.0005 (0.0054)	batch 1.1439 (1.1275)	loss 2.8130 (2.4633)	grad_norm 1.6197 (1.7185)	mem 28740MB
Train: [2/180][4150/10009]	eta 1:50:05 lr 0.049978	data 0.0005 (0.0054)	batch 1.0951 (1.1274)	loss 2.4146 (2.4634)	grad_norm 1.7076 (1.7186)	mem 28740MB
Train: [2/180][4200/10009]	eta 1:49:08 lr 0.049978	data 0.0005 (0.0053)	batch 1.1223 (1.1273)	loss 2.4828 (2.4638)	grad_norm 1.7412 (1.7181)	mem 28740MB
Train: [2/180][4250/10009]	eta 1:48:12 lr 0.049978	data 0.0006 (0.0052)	batch 1.1436 (1.1273)	loss 2.5091 (2.4635)	grad_norm 1.7785 (1.7179)	mem 28740MB
Train: [2/180][4300/10009]	eta 1:47:15 lr 0.049978	data 0.0005 (0.0052)	batch 1.1359 (1.1273)	loss 2.4861 (2.4635)	grad_norm 1.4875 (1.7173)	mem 28740MB
Train: [2/180][4350/10009]	eta 1:46:19 lr 0.049977	data 0.0005 (0.0051)	batch 1.1207 (1.1273)	loss 2.2352 (2.4633)	grad_norm 1.4311 (1.7168)	mem 28740MB
Train: [2/180][4400/10009]	eta 1:45:22 lr 0.049977	data 0.0005 (0.0051)	batch 1.1181 (1.1272)	loss 2.5037 (2.4633)	grad_norm 1.7266 (1.7165)	mem 28740MB
Train: [2/180][4450/10009]	eta 1:44:25 lr 0.049977	data 0.0005 (0.0050)	batch 1.1138 (1.1271)	loss 2.5105 (2.4633)	grad_norm 1.8894 (1.7163)	mem 28740MB
Train: [2/180][4500/10009]	eta 1:43:28 lr 0.049977	data 0.0004 (0.0050)	batch 1.1284 (1.1270)	loss 2.3112 (2.4634)	grad_norm 1.7075 (1.7160)	mem 28740MB
Train: [2/180][4550/10009]	eta 1:42:31 lr 0.049977	data 0.0005 (0.0049)	batch 1.1158 (1.1269)	loss 2.4258 (2.4632)	grad_norm 1.8453 (1.7155)	mem 28740MB
Train: [2/180][4600/10009]	eta 1:41:35 lr 0.049977	data 0.0005 (0.0049)	batch 1.1209 (1.1269)	loss 2.7609 (2.4631)	grad_norm 1.6926 (1.7152)	mem 28740MB
Train: [2/180][4650/10009]	eta 1:40:38 lr 0.049977	data 0.0005 (0.0048)	batch 1.1077 (1.1269)	loss 2.8264 (2.4633)	grad_norm 1.9359 (1.7147)	mem 28740MB
Train: [2/180][4700/10009]	eta 1:39:42 lr 0.049977	data 0.0004 (0.0048)	batch 1.1500 (1.1268)	loss 2.3915 (2.4630)	grad_norm 1.6988 (1.7146)	mem 28740MB
Train: [2/180][4750/10009]	eta 1:38:45 lr 0.049977	data 0.0005 (0.0047)	batch 1.1182 (1.1268)	loss 2.3512 (2.4630)	grad_norm 1.9015 (1.7145)	mem 28740MB
Train: [2/180][4800/10009]	eta 1:37:49 lr 0.049977	data 0.0005 (0.0047)	batch 1.1195 (1.1267)	loss 2.4340 (2.4633)	grad_norm 1.7074 (1.7142)	mem 28740MB
Train: [2/180][4850/10009]	eta 1:36:52 lr 0.049976	data 0.0005 (0.0047)	batch 1.1427 (1.1267)	loss 2.5455 (2.4631)	grad_norm 1.5214 (1.7144)	mem 28740MB
Train: [2/180][4900/10009]	eta 1:35:56 lr 0.049976	data 0.0005 (0.0046)	batch 1.1212 (1.1267)	loss 2.5967 (2.4630)	grad_norm 1.8691 (1.7143)	mem 28740MB
Train: [2/180][4950/10009]	eta 1:34:59 lr 0.049976	data 0.0004 (0.0046)	batch 1.1226 (1.1266)	loss 2.4899 (2.4634)	grad_norm 1.6171 (1.7141)	mem 28740MB
Train: [2/180][5000/10009]	eta 1:34:02 lr 0.049976	data 0.0005 (0.0045)	batch 1.1032 (1.1266)	loss 2.6880 (2.4633)	grad_norm 1.9066 (1.7139)	mem 28740MB
Train: [2/180][5050/10009]	eta 1:33:06 lr 0.049976	data 0.0005 (0.0045)	batch 1.1323 (1.1266)	loss 2.6314 (2.4632)	grad_norm 1.7616 (1.7136)	mem 28740MB
Train: [2/180][5100/10009]	eta 1:32:10 lr 0.049976	data 0.0005 (0.0045)	batch 1.1419 (1.1265)	loss 2.5382 (2.4629)	grad_norm 1.6674 (1.7132)	mem 28740MB
Train: [2/180][5150/10009]	eta 1:31:13 lr 0.049976	data 0.0007 (0.0044)	batch 1.1169 (1.1264)	loss 2.1070 (2.4627)	grad_norm 1.5529 (1.7129)	mem 28740MB
Train: [2/180][5200/10009]	eta 1:30:16 lr 0.049976	data 0.0005 (0.0044)	batch 1.1110 (1.1264)	loss 2.2729 (2.4624)	grad_norm 1.5669 (1.7125)	mem 28740MB
Train: [2/180][5250/10009]	eta 1:29:19 lr 0.049976	data 0.0005 (0.0043)	batch 1.1075 (1.1263)	loss 2.4637 (2.4621)	grad_norm 1.6823 (1.7122)	mem 28740MB
Train: [2/180][5300/10009]	eta 1:28:23 lr 0.049976	data 0.0005 (0.0043)	batch 1.1168 (1.1262)	loss 2.2669 (2.4618)	grad_norm 1.6448 (1.7118)	mem 28740MB
Train: [2/180][5350/10009]	eta 1:27:26 lr 0.049976	data 0.0005 (0.0043)	batch 1.1401 (1.1261)	loss 1.9290 (2.4616)	grad_norm 1.6654 (1.7115)	mem 28740MB
Train: [2/180][5400/10009]	eta 1:26:29 lr 0.049975	data 0.0005 (0.0042)	batch 1.0951 (1.1260)	loss 2.7648 (2.4612)	grad_norm 1.8607 (1.7112)	mem 28740MB
Train: [2/180][5450/10009]	eta 1:25:33 lr 0.049975	data 0.0004 (0.0042)	batch 1.1219 (1.1259)	loss 2.4040 (2.4613)	grad_norm 1.5874 (1.7109)	mem 28740MB
Train: [2/180][5500/10009]	eta 1:24:36 lr 0.049975	data 0.0004 (0.0042)	batch 1.1204 (1.1259)	loss 2.4133 (2.4611)	grad_norm 1.5464 (1.7106)	mem 28740MB
Train: [2/180][5550/10009]	eta 1:23:40 lr 0.049975	data 0.0005 (0.0041)	batch 1.1487 (1.1259)	loss 2.4582 (2.4606)	grad_norm 1.7363 (1.7102)	mem 28740MB
Train: [2/180][5600/10009]	eta 1:22:43 lr 0.049975	data 0.0004 (0.0041)	batch 1.0990 (1.1258)	loss 2.3187 (2.4601)	grad_norm 1.6243 (1.7098)	mem 28740MB
Train: [2/180][5650/10009]	eta 1:21:47 lr 0.049975	data 0.0005 (0.0041)	batch 1.1225 (1.1259)	loss 2.4530 (2.4599)	grad_norm 1.7501 (1.7097)	mem 28740MB
Train: [2/180][5700/10009]	eta 1:20:51 lr 0.049975	data 0.0005 (0.0040)	batch 1.1207 (1.1258)	loss 1.9784 (2.4595)	grad_norm 1.5251 (1.7093)	mem 28740MB
Train: [2/180][5750/10009]	eta 1:19:54 lr 0.049975	data 0.0004 (0.0040)	batch 1.1264 (1.1258)	loss 2.1841 (2.4591)	grad_norm 1.6729 (1.7090)	mem 28740MB
Train: [2/180][5800/10009]	eta 1:18:58 lr 0.049975	data 0.0009 (0.0040)	batch 1.1242 (1.1258)	loss 2.3061 (2.4590)	grad_norm 1.6539 (1.7087)	mem 28740MB
Train: [2/180][5850/10009]	eta 1:18:02 lr 0.049975	data 0.0005 (0.0039)	batch 1.1196 (1.1258)	loss 2.1927 (2.4586)	grad_norm 1.6073 (1.7086)	mem 28740MB
Train: [2/180][5900/10009]	eta 1:17:05 lr 0.049974	data 0.0005 (0.0039)	batch 1.1136 (1.1258)	loss 2.6677 (2.4586)	grad_norm 1.6848 (1.7083)	mem 28740MB
Train: [2/180][5950/10009]	eta 1:16:09 lr 0.049974	data 0.0005 (0.0039)	batch 1.0993 (1.1257)	loss 2.4337 (2.4584)	grad_norm 1.8503 (1.7080)	mem 28740MB
Train: [2/180][6000/10009]	eta 1:15:12 lr 0.049974	data 0.0005 (0.0039)	batch 1.1239 (1.1257)	loss 2.6656 (2.4581)	grad_norm 1.6979 (1.7079)	mem 28740MB
Train: [2/180][6050/10009]	eta 1:14:16 lr 0.049974	data 0.0005 (0.0038)	batch 1.1211 (1.1257)	loss 2.3025 (2.4580)	grad_norm 1.5948 (1.7078)	mem 28740MB
Train: [2/180][6100/10009]	eta 1:13:20 lr 0.049974	data 0.0005 (0.0038)	batch 1.1501 (1.1257)	loss 2.7316 (2.4581)	grad_norm 1.7969 (1.7077)	mem 28740MB
Train: [2/180][6150/10009]	eta 1:12:23 lr 0.049974	data 0.0005 (0.0038)	batch 1.1182 (1.1257)	loss 2.8398 (2.4577)	grad_norm 1.6773 (1.7074)	mem 28740MB
Train: [2/180][6200/10009]	eta 1:11:27 lr 0.049974	data 0.0005 (0.0038)	batch 1.1163 (1.1256)	loss 2.0664 (2.4577)	grad_norm 1.6534 (1.7070)	mem 28740MB
Train: [2/180][6250/10009]	eta 1:10:31 lr 0.049974	data 0.0005 (0.0037)	batch 1.1168 (1.1256)	loss 2.3040 (2.4575)	grad_norm 1.8467 (1.7066)	mem 28740MB
Train: [2/180][6300/10009]	eta 1:09:34 lr 0.049974	data 0.0005 (0.0037)	batch 1.1007 (1.1256)	loss 2.0907 (2.4578)	grad_norm 1.6652 (1.7062)	mem 28740MB
Train: [2/180][6350/10009]	eta 1:08:38 lr 0.049974	data 0.0005 (0.0037)	batch 1.1292 (1.1256)	loss 2.2503 (2.4576)	grad_norm 1.5221 (1.7056)	mem 28740MB
Train: [2/180][6400/10009]	eta 1:07:42 lr 0.049973	data 0.0004 (0.0037)	batch 1.1047 (1.1256)	loss 2.1231 (2.4576)	grad_norm 1.6376 (1.7053)	mem 28740MB
Train: [2/180][6450/10009]	eta 1:06:45 lr 0.049973	data 0.0005 (0.0036)	batch 1.0953 (1.1255)	loss 2.2798 (2.4571)	grad_norm 1.7854 (1.7050)	mem 28740MB
Train: [2/180][6500/10009]	eta 1:05:49 lr 0.049973	data 0.0005 (0.0036)	batch 1.1413 (1.1255)	loss 2.3908 (2.4570)	grad_norm 1.7866 (1.7047)	mem 28740MB
Train: [2/180][6550/10009]	eta 1:04:52 lr 0.049973	data 0.0004 (0.0036)	batch 1.1046 (1.1254)	loss 2.4556 (2.4567)	grad_norm 1.7317 (1.7043)	mem 28740MB
Train: [2/180][6600/10009]	eta 1:03:56 lr 0.049973	data 0.0004 (0.0036)	batch 1.1201 (1.1253)	loss 2.4809 (2.4564)	grad_norm 1.6015 (1.7040)	mem 28740MB
Train: [2/180][6650/10009]	eta 1:02:59 lr 0.049973	data 0.0005 (0.0035)	batch 1.1015 (1.1253)	loss 2.5373 (2.4563)	grad_norm 1.7644 (1.7036)	mem 28740MB
Train: [2/180][6700/10009]	eta 1:02:03 lr 0.049973	data 0.0005 (0.0035)	batch 1.1058 (1.1252)	loss 2.7059 (2.4558)	grad_norm 1.6644 (1.7032)	mem 28740MB
Train: [2/180][6750/10009]	eta 1:01:07 lr 0.049973	data 0.0005 (0.0035)	batch 1.1155 (1.1252)	loss 2.4144 (2.4555)	grad_norm 1.6564 (1.7029)	mem 28740MB
Train: [2/180][6800/10009]	eta 1:00:10 lr 0.049973	data 0.0010 (0.0035)	batch 1.1308 (1.1252)	loss 2.3408 (2.4554)	grad_norm 1.5961 (1.7028)	mem 28740MB
Train: [2/180][6850/10009]	eta 0:59:14 lr 0.049973	data 0.0005 (0.0034)	batch 1.1159 (1.1252)	loss 2.4452 (2.4551)	grad_norm 1.7305 (1.7024)	mem 28740MB
Train: [2/180][6900/10009]	eta 0:58:17 lr 0.049972	data 0.0005 (0.0034)	batch 1.1122 (1.1251)	loss 2.1849 (2.4549)	grad_norm 1.3851 (1.7021)	mem 28740MB
Train: [2/180][6950/10009]	eta 0:57:21 lr 0.049972	data 0.0004 (0.0034)	batch 1.1441 (1.1251)	loss 2.3590 (2.4548)	grad_norm 1.5341 (1.7017)	mem 28740MB
Train: [2/180][7000/10009]	eta 0:56:39 lr 0.049972	data 0.0128 (0.0034)	batch 2.0087 (1.1299)	loss 2.5198 (2.4547)	grad_norm 1.7176 (1.7015)	mem 28740MB
Train: [2/180][7050/10009]	eta 0:56:02 lr 0.049972	data 0.0004 (0.0034)	batch 1.9495 (1.1362)	loss 2.4942 (2.4547)	grad_norm 1.6488 (1.7011)	mem 28740MB
Train: [2/180][7100/10009]	eta 0:55:23 lr 0.049972	data 0.0005 (0.0034)	batch 1.8090 (1.1424)	loss 2.4743 (2.4545)	grad_norm 1.6385 (1.7010)	mem 28740MB
Train: [2/180][7150/10009]	eta 0:54:43 lr 0.049972	data 0.0004 (0.0033)	batch 2.0001 (1.1485)	loss 2.5165 (2.4545)	grad_norm 1.6723 (1.7008)	mem 28740MB
Train: [2/180][7200/10009]	eta 0:54:02 lr 0.049972	data 0.0004 (0.0033)	batch 2.0133 (1.1545)	loss 2.3449 (2.4543)	grad_norm 1.6654 (1.7005)	mem 28740MB
Train: [2/180][7250/10009]	eta 0:53:21 lr 0.049972	data 0.0005 (0.0033)	batch 2.1537 (1.1603)	loss 2.3343 (2.4542)	grad_norm 1.7914 (1.7000)	mem 28740MB
Train: [2/180][7300/10009]	eta 0:52:39 lr 0.049972	data 0.0003 (0.0033)	batch 2.0041 (1.1662)	loss 2.4974 (2.4541)	grad_norm 1.5172 (1.7001)	mem 28740MB
Train: [2/180][7350/10009]	eta 0:51:56 lr 0.049972	data 0.0008 (0.0033)	batch 2.0639 (1.1720)	loss 2.6638 (2.4541)	grad_norm 1.6816 (1.6997)	mem 28740MB
Train: [2/180][7400/10009]	eta 0:51:12 lr 0.049971	data 0.0005 (0.0033)	batch 2.2690 (1.1778)	loss 2.5533 (2.4541)	grad_norm 1.6811 (1.6995)	mem 28740MB
Train: [2/180][7450/10009]	eta 0:50:28 lr 0.049971	data 0.0003 (0.0033)	batch 1.9457 (1.1833)	loss 2.4787 (2.4541)	grad_norm 1.7448 (1.6992)	mem 28740MB
Train: [2/180][7500/10009]	eta 0:49:43 lr 0.049971	data 0.0005 (0.0033)	batch 1.8771 (1.1891)	loss 2.3246 (2.4540)	grad_norm 1.5298 (1.6989)	mem 28740MB
Train: [2/180][7550/10009]	eta 0:48:57 lr 0.049971	data 0.0005 (0.0033)	batch 2.3189 (1.1945)	loss 2.2159 (2.4538)	grad_norm 1.6809 (1.6985)	mem 28740MB
Train: [2/180][7600/10009]	eta 0:48:10 lr 0.049971	data 0.0004 (0.0033)	batch 2.0039 (1.1999)	loss 2.7006 (2.4537)	grad_norm 1.8556 (1.6983)	mem 28740MB
Train: [2/180][7650/10009]	eta 0:47:23 lr 0.049971	data 0.0005 (0.0033)	batch 2.0409 (1.2054)	loss 2.4900 (2.4535)	grad_norm 1.7178 (1.6982)	mem 28740MB
Train: [2/180][7700/10009]	eta 0:46:35 lr 0.049971	data 0.0004 (0.0033)	batch 2.0479 (1.2108)	loss 2.4418 (2.4535)	grad_norm 1.5834 (1.6979)	mem 28740MB
Train: [2/180][7750/10009]	eta 0:45:46 lr 0.049971	data 0.0004 (0.0032)	batch 1.9237 (1.2160)	loss 2.2864 (2.4531)	grad_norm 1.6699 (1.6976)	mem 28740MB
Train: [2/180][7800/10009]	eta 0:44:57 lr 0.049971	data 0.0052 (0.0032)	batch 1.9695 (1.2213)	loss 2.3797 (2.4527)	grad_norm 1.5345 (1.6973)	mem 28740MB
Train: [2/180][7850/10009]	eta 0:44:07 lr 0.049970	data 0.0003 (0.0032)	batch 2.0377 (1.2265)	loss 2.6529 (2.4526)	grad_norm 1.7651 (1.6972)	mem 28740MB
Train: [2/180][7900/10009]	eta 0:43:17 lr 0.049970	data 0.0005 (0.0032)	batch 2.0488 (1.2316)	loss 2.3844 (2.4525)	grad_norm 1.7116 (1.6970)	mem 28740MB
Train: [2/180][7950/10009]	eta 0:42:20 lr 0.049970	data 0.0005 (0.0032)	batch 1.1843 (1.2341)	loss 2.5161 (2.4521)	grad_norm 1.7013 (1.6967)	mem 28740MB
Train: [2/180][8000/10009]	eta 0:41:17 lr 0.049970	data 0.0004 (0.0032)	batch 1.1314 (1.2334)	loss 2.5780 (2.4522)	grad_norm 1.7211 (1.6963)	mem 28740MB
Train: [2/180][8050/10009]	eta 0:40:14 lr 0.049970	data 0.0005 (0.0032)	batch 1.1208 (1.2327)	loss 2.5827 (2.4519)	grad_norm 1.4952 (1.6959)	mem 28740MB
Train: [2/180][8100/10009]	eta 0:39:11 lr 0.049970	data 0.0004 (0.0031)	batch 1.1075 (1.2321)	loss 2.5251 (2.4517)	grad_norm 1.5852 (1.6956)	mem 28740MB
Train: [2/180][8150/10009]	eta 0:38:09 lr 0.049970	data 0.0005 (0.0031)	batch 1.1227 (1.2314)	loss 2.6806 (2.4515)	grad_norm 1.8294 (1.6954)	mem 28740MB
Train: [2/180][8200/10009]	eta 0:37:06 lr 0.049970	data 0.0005 (0.0031)	batch 1.1260 (1.2307)	loss 2.4283 (2.4514)	grad_norm 1.6155 (1.6952)	mem 28740MB
Train: [2/180][8250/10009]	eta 0:36:03 lr 0.049970	data 0.0004 (0.0031)	batch 1.1269 (1.2301)	loss 2.4773 (2.4511)	grad_norm 1.8520 (1.6948)	mem 28740MB
Train: [2/180][8300/10009]	eta 0:35:01 lr 0.049970	data 0.0004 (0.0031)	batch 1.1875 (1.2295)	loss 2.5628 (2.4512)	grad_norm 1.6395 (1.6946)	mem 28740MB
Train: [2/180][8350/10009]	eta 0:33:59 lr 0.049969	data 0.0004 (0.0031)	batch 1.1069 (1.2296)	loss 2.1802 (2.4512)	grad_norm 1.6222 (1.6944)	mem 28740MB
Train: [2/180][8400/10009]	eta 0:32:57 lr 0.049969	data 0.0005 (0.0031)	batch 1.1440 (1.2290)	loss 2.5448 (2.4512)	grad_norm 1.8946 (1.6944)	mem 28740MB
Train: [2/180][8450/10009]	eta 0:31:54 lr 0.049969	data 0.0005 (0.0030)	batch 1.1133 (1.2283)	loss 2.1631 (2.4513)	grad_norm 1.6677 (1.6941)	mem 28740MB
Train: [2/180][8500/10009]	eta 0:30:52 lr 0.049969	data 0.0005 (0.0030)	batch 1.1728 (1.2277)	loss 2.4093 (2.4507)	grad_norm 1.7191 (1.6937)	mem 28740MB
Train: [2/180][8550/10009]	eta 0:29:50 lr 0.049969	data 0.0005 (0.0030)	batch 1.1386 (1.2271)	loss 2.5258 (2.4505)	grad_norm 1.5824 (1.6935)	mem 28740MB
Train: [2/180][8600/10009]	eta 0:28:48 lr 0.049969	data 0.0005 (0.0030)	batch 1.1171 (1.2266)	loss 2.2672 (2.4505)	grad_norm 1.5136 (1.6934)	mem 28740MB
Train: [2/180][8650/10009]	eta 0:27:46 lr 0.049969	data 0.0005 (0.0030)	batch 1.1392 (1.2260)	loss 2.2826 (2.4501)	grad_norm 1.5608 (1.6930)	mem 28740MB
Train: [2/180][8700/10009]	eta 0:26:44 lr 0.049969	data 0.0005 (0.0030)	batch 1.1150 (1.2254)	loss 2.6361 (2.4501)	grad_norm 1.7413 (1.6928)	mem 28740MB
Train: [2/180][8750/10009]	eta 0:25:42 lr 0.049969	data 0.0006 (0.0030)	batch 1.1785 (1.2248)	loss 2.1662 (2.4497)	grad_norm 1.6069 (1.6925)	mem 28740MB
Train: [2/180][8800/10009]	eta 0:24:40 lr 0.049968	data 0.0005 (0.0029)	batch 1.1057 (1.2243)	loss 2.3067 (2.4494)	grad_norm 1.6652 (1.6920)	mem 28740MB
Train: [2/180][8850/10009]	eta 0:23:38 lr 0.049968	data 0.0005 (0.0029)	batch 1.1103 (1.2238)	loss 2.2235 (2.4492)	grad_norm 1.6831 (1.6919)	mem 28740MB
Train: [2/180][8900/10009]	eta 0:22:36 lr 0.049968	data 0.0005 (0.0029)	batch 1.1034 (1.2232)	loss 2.3184 (2.4492)	grad_norm 1.6020 (1.6916)	mem 28740MB
Train: [2/180][8950/10009]	eta 0:21:34 lr 0.049968	data 0.0005 (0.0029)	batch 1.8948 (1.2228)	loss 2.3099 (2.4491)	grad_norm 1.6535 (1.6914)	mem 28740MB
Train: [2/180][9000/10009]	eta 0:20:33 lr 0.049968	data 0.0007 (0.0029)	batch 1.1088 (1.2222)	loss 2.4637 (2.4489)	grad_norm 1.5851 (1.6910)	mem 28740MB
Train: [2/180][9050/10009]	eta 0:19:31 lr 0.049968	data 0.0004 (0.0029)	batch 1.1160 (1.2217)	loss 2.3511 (2.4486)	grad_norm 1.6227 (1.6906)	mem 28740MB
Train: [2/180][9100/10009]	eta 0:18:29 lr 0.049968	data 0.0005 (0.0029)	batch 1.1110 (1.2211)	loss 2.3119 (2.4485)	grad_norm 1.5476 (1.6903)	mem 28740MB
Train: [2/180][9150/10009]	eta 0:17:28 lr 0.049968	data 0.0006 (0.0028)	batch 1.1229 (1.2205)	loss 2.5106 (2.4483)	grad_norm 1.7576 (1.6901)	mem 28740MB
Train: [2/180][9200/10009]	eta 0:16:27 lr 0.049968	data 0.0005 (0.0028)	batch 1.1185 (1.2201)	loss 2.3112 (2.4482)	grad_norm 1.5064 (1.6898)	mem 28740MB
Train: [2/180][9250/10009]	eta 0:15:25 lr 0.049967	data 0.0005 (0.0028)	batch 1.1093 (1.2195)	loss 2.3679 (2.4481)	grad_norm 1.7086 (1.6896)	mem 28740MB
Train: [2/180][9300/10009]	eta 0:14:24 lr 0.049967	data 0.0005 (0.0028)	batch 1.1133 (1.2190)	loss 2.2971 (2.4480)	grad_norm 1.6665 (1.6894)	mem 28740MB
Train: [2/180][9350/10009]	eta 0:13:22 lr 0.049967	data 0.0005 (0.0028)	batch 1.1167 (1.2185)	loss 2.4059 (2.4477)	grad_norm 1.8206 (1.6890)	mem 28740MB
Train: [2/180][9400/10009]	eta 0:12:21 lr 0.049967	data 0.0005 (0.0028)	batch 1.1170 (1.2179)	loss 2.2598 (2.4475)	grad_norm 1.5940 (1.6887)	mem 28740MB
Train: [2/180][9450/10009]	eta 0:11:20 lr 0.049967	data 0.0003 (0.0028)	batch 1.2023 (1.2174)	loss 2.3441 (2.4474)	grad_norm 1.6135 (1.6884)	mem 28740MB
Train: [2/180][9500/10009]	eta 0:10:19 lr 0.049967	data 0.0004 (0.0028)	batch 1.0811 (1.2170)	loss 2.1502 (2.4472)	grad_norm 1.7660 (1.6882)	mem 28740MB
Train: [2/180][9550/10009]	eta 0:09:18 lr 0.049967	data 0.0003 (0.0028)	batch 1.1289 (1.2166)	loss 2.6048 (2.4471)	grad_norm 1.5586 (1.6879)	mem 28740MB
Train: [2/180][9600/10009]	eta 0:08:17 lr 0.049967	data 0.0004 (0.0028)	batch 1.1548 (1.2160)	loss 2.4862 (2.4468)	grad_norm 1.7826 (1.6876)	mem 28740MB
Train: [2/180][9650/10009]	eta 0:07:16 lr 0.049967	data 0.0005 (0.0028)	batch 1.0649 (1.2155)	loss 2.2673 (2.4466)	grad_norm 1.5690 (1.6874)	mem 28740MB
Train: [2/180][9700/10009]	eta 0:06:15 lr 0.049966	data 0.0003 (0.0028)	batch 1.0465 (1.2150)	loss 2.0947 (2.4462)	grad_norm 1.5481 (1.6871)	mem 28740MB
Train: [2/180][9750/10009]	eta 0:05:14 lr 0.049966	data 0.0003 (0.0028)	batch 1.0535 (1.2145)	loss 2.6315 (2.4460)	grad_norm 1.6619 (1.6868)	mem 28740MB
Train: [2/180][9800/10009]	eta 0:04:13 lr 0.049966	data 0.0003 (0.0028)	batch 1.1208 (1.2140)	loss 2.1769 (2.4459)	grad_norm 1.6869 (1.6866)	mem 28740MB
Train: [2/180][9850/10009]	eta 0:03:12 lr 0.049966	data 0.0004 (0.0028)	batch 1.1444 (1.2135)	loss 2.5228 (2.4458)	grad_norm 1.6187 (1.6863)	mem 28740MB
Train: [2/180][9900/10009]	eta 0:02:12 lr 0.049966	data 0.0051 (0.0028)	batch 1.0603 (1.2131)	loss 2.4597 (2.4457)	grad_norm 1.7749 (1.6861)	mem 28740MB
Train: [2/180][9950/10009]	eta 0:01:11 lr 0.049966	data 0.0055 (0.0028)	batch 1.1070 (1.2125)	loss 2.5973 (2.4457)	grad_norm 1.7186 (1.6860)	mem 28740MB
Train: [2/180][10000/10009]	eta 0:00:10 lr 0.049966	data 0.0002 (0.0028)	batch 1.1173 (1.2120)	loss 2.3230 (2.4455)	grad_norm 1.6361 (1.6858)	mem 28740MB
Current slope: [array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.]), array([0., 1., 1., ..., 0., 1., 1.])] 	
EPOCH 2 training takes 3:22:10
Test: [0/391]	Time 11.952 (11.952)	Loss 0.6921 (0.6921)	Acc@1 85.938 (85.938)	Acc@5 94.531 (94.531)	Mem 28740MB
Test: [50/391]	Time 0.278 (0.511)	Loss 0.6050 (1.0660)	Acc@1 85.938 (73.882)	Acc@5 93.750 (91.008)	Mem 28740MB
Test: [100/391]	Time 0.265 (0.400)	Loss 1.0320 (1.1236)	Acc@1 73.438 (71.434)	Acc@5 93.750 (91.213)	Mem 28740MB
Test: [150/391]	Time 0.265 (0.362)	Loss 1.0471 (1.1002)	Acc@1 66.406 (72.092)	Acc@5 96.094 (91.686)	Mem 28740MB
Test: [200/391]	Time 0.262 (0.343)	Loss 1.5712 (1.2387)	Acc@1 61.719 (69.426)	Acc@5 85.938 (89.817)	Mem 28740MB
Test: [250/391]	Time 0.256 (0.330)	Loss 1.0146 (1.3103)	Acc@1 73.438 (68.152)	Acc@5 90.625 (88.670)	Mem 28740MB
Test: [300/391]	Time 0.321 (0.325)	Loss 1.4666 (1.3738)	Acc@1 71.875 (67.001)	Acc@5 84.375 (87.702)	Mem 28740MB
Test: [350/391]	Time 0.266 (0.319)	Loss 1.4747 (1.4225)	Acc@1 65.625 (66.075)	Acc@5 85.156 (87.055)	Mem 28740MB
 * Acc@1 66.212 Acc@5 87.156
Accuracy of the network on the 50000 test images: 66.21%
Max accuracy (after decay): 66.21%
manifold://experiment/default/ckpt.pth saving......
manifold://experiment/default/ckpt.pth saved !!!
Train: [3/180][0/10009]	eta 2 days, 9:52:35 lr 0.049966	data 19.3636 (19.3636)	batch 20.8168 (20.8168)	loss 2.4412 (2.4412)	grad_norm 1.5592 (1.5592)	mem 28740MB
Train: [3/180][50/10009]	eta 4:10:30 lr 0.049966	data 0.0007 (0.3810)	batch 1.1014 (1.5092)	loss 2.4564 (2.4047)	grad_norm 1.6516 (1.6025)	mem 28740MB
Train: [3/180][100/10009]	eta 3:37:38 lr 0.049966	data 0.0005 (0.1927)	batch 1.1326 (1.3178)	loss 2.6447 (2.3902)	grad_norm 1.6345 (1.5911)	mem 28740MB
Train: [3/180][150/10009]	eta 3:26:05 lr 0.049965	data 0.0005 (0.1291)	batch 1.1325 (1.2543)	loss 2.3335 (2.3883)	grad_norm 1.5800 (1.5913)	mem 28740MB
Train: [3/180][200/10009]	eta 3:19:15 lr 0.049965	data 0.0003 (0.0974)	batch 1.0610 (1.2188)	loss 2.3387 (2.3909)	grad_norm 1.4127 (1.5963)	mem 28740MB
Train: [3/180][250/10009]	eta 3:15:26 lr 0.049965	data 0.0003 (0.0785)	batch 1.1325 (1.2016)	loss 2.4730 (2.3979)	grad_norm 1.6044 (1.6016)	mem 28740MB
Train: [3/180][300/10009]	eta 3:12:33 lr 0.049965	data 0.0133 (0.0662)	batch 1.1354 (1.1900)	loss 2.5806 (2.4014)	grad_norm 1.6077 (1.6072)	mem 28740MB
Train: [3/180][350/10009]	eta 3:09:58 lr 0.049965	data 0.0003 (0.0571)	batch 1.2068 (1.1801)	loss 2.3236 (2.4027)	grad_norm 1.7221 (1.6098)	mem 28740MB
Train: [3/180][400/10009]	eta 3:07:42 lr 0.049965	data 0.0005 (0.0504)	batch 1.0510 (1.1721)	loss 2.4096 (2.4057)	grad_norm 1.5773 (1.6128)	mem 28740MB
Train: [3/180][450/10009]	eta 3:05:39 lr 0.049965	data 0.0043 (0.0451)	batch 1.0863 (1.1654)	loss 2.3290 (2.4057)	grad_norm 1.5123 (1.6141)	mem 28740MB
Train: [3/180][500/10009]	eta 3:03:48 lr 0.049965	data 0.0011 (0.0409)	batch 1.1634 (1.1598)	loss 2.0947 (2.4042)	grad_norm 1.5279 (1.6143)	mem 28740MB
Train: [3/180][550/10009]	eta 3:02:01 lr 0.049964	data 0.0006 (0.0374)	batch 1.1454 (1.1546)	loss 2.3831 (2.4035)	grad_norm 1.6293 (1.6141)	mem 28740MB
Train: [3/180][600/10009]	eta 3:00:28 lr 0.049964	data 0.0004 (0.0345)	batch 1.0661 (1.1508)	loss 2.4074 (2.4034)	grad_norm 1.6767 (1.6153)	mem 28740MB
Train: [3/180][650/10009]	eta 2:59:09 lr 0.049964	data 0.0003 (0.0321)	batch 1.0688 (1.1485)	loss 2.5824 (2.4052)	grad_norm 1.5897 (1.6162)	mem 28740MB
Train: [3/180][700/10009]	eta 2:57:39 lr 0.049964	data 0.0071 (0.0299)	batch 1.0674 (1.1451)	loss 2.2865 (2.4040)	grad_norm 1.6101 (1.6159)	mem 28740MB
Train: [3/180][750/10009]	eta 2:56:22 lr 0.049964	data 0.0004 (0.0281)	batch 1.0919 (1.1429)	loss 2.4060 (2.4037)	grad_norm 1.7624 (1.6161)	mem 28740MB
Train: [3/180][800/10009]	eta 2:55:09 lr 0.049964	data 0.0024 (0.0265)	batch 1.1022 (1.1412)	loss 2.3655 (2.4044)	grad_norm 1.6481 (1.6176)	mem 28740MB
Train: [3/180][850/10009]	eta 2:53:48 lr 0.049964	data 0.0003 (0.0251)	batch 1.0466 (1.1386)	loss 2.3629 (2.4040)	grad_norm 1.6111 (1.6167)	mem 28740MB
Train: [3/180][900/10009]	eta 2:52:33 lr 0.049964	data 0.0050 (0.0238)	batch 1.0548 (1.1366)	loss 2.2109 (2.4008)	grad_norm 1.5023 (1.6151)	mem 28740MB
Train: [3/180][950/10009]	eta 2:51:28 lr 0.049964	data 0.0004 (0.0226)	batch 1.1034 (1.1357)	loss 2.6257 (2.4001)	grad_norm 1.4928 (1.6142)	mem 28740MB
Train: [3/180][1000/10009]	eta 2:50:24 lr 0.049963	data 0.0005 (0.0215)	batch 1.1182 (1.1350)	loss 2.1964 (2.4007)	grad_norm 1.6344 (1.6139)	mem 28740MB
Train: [3/180][1050/10009]	eta 2:49:19 lr 0.049963	data 0.0005 (0.0205)	batch 1.1114 (1.1340)	loss 2.3098 (2.4011)	grad_norm 1.5446 (1.6136)	mem 28740MB
Train: [3/180][1100/10009]	eta 2:48:15 lr 0.049963	data 0.0005 (0.0196)	batch 1.1221 (1.1332)	loss 2.4450 (2.3999)	grad_norm 1.5372 (1.6146)	mem 28740MB
Train: [3/180][1150/10009]	eta 2:47:14 lr 0.049963	data 0.0005 (0.0187)	batch 1.1234 (1.1327)	loss 2.3925 (2.4001)	grad_norm 1.5884 (1.6144)	mem 28740MB
Train: [3/180][1200/10009]	eta 2:46:11 lr 0.049963	data 0.0006 (0.0180)	batch 1.1118 (1.1320)	loss 2.3944 (2.3999)	grad_norm 1.5473 (1.6142)	mem 28740MB
Train: [3/180][1250/10009]	eta 2:45:09 lr 0.049963	data 0.0005 (0.0173)	batch 1.1182 (1.1314)	loss 2.4958 (2.3997)	grad_norm 1.6459 (1.6139)	mem 28740MB
Train: [3/180][1300/10009]	eta 2:44:07 lr 0.049963	data 0.0005 (0.0166)	batch 1.1218 (1.1307)	loss 2.2927 (2.4001)	grad_norm 1.5572 (1.6148)	mem 28740MB
Train: [3/180][1350/10009]	eta 2:43:07 lr 0.049963	data 0.0005 (0.0160)	batch 1.1121 (1.1304)	loss 2.3625 (2.3992)	grad_norm 1.4306 (1.6161)	mem 28740MB
Train: [3/180][1400/10009]	eta 2:42:08 lr 0.049962	data 0.0005 (0.0155)	batch 1.1354 (1.1301)	loss 2.3573 (2.3997)	grad_norm 1.6760 (1.6155)	mem 28740MB
Train: [3/180][1450/10009]	eta 2:41:08 lr 0.049962	data 0.0005 (0.0150)	batch 1.1203 (1.1297)	loss 2.1642 (2.3980)	grad_norm 1.6045 (1.6155)	mem 28740MB
Train: [3/180][1500/10009]	eta 2:40:10 lr 0.049962	data 0.0005 (0.0145)	batch 1.1157 (1.1295)	loss 2.5122 (2.3986)	grad_norm 1.7470 (1.6159)	mem 28740MB
Train: [3/180][1550/10009]	eta 2:39:11 lr 0.049962	data 0.0005 (0.0140)	batch 1.0996 (1.1292)	loss 2.3385 (2.3972)	grad_norm 1.4507 (1.6157)	mem 28740MB
Train: [3/180][1600/10009]	eta 2:38:13 lr 0.049962	data 0.0005 (0.0136)	batch 1.1219 (1.1289)	loss 2.3267 (2.3964)	grad_norm 1.6038 (1.6155)	mem 28740MB
Train: [3/180][1650/10009]	eta 2:37:14 lr 0.049962	data 0.0005 (0.0132)	batch 1.1003 (1.1286)	loss 2.1539 (2.3945)	grad_norm 1.7338 (1.6144)	mem 28740MB
Train: [3/180][1700/10009]	eta 2:36:15 lr 0.049962	data 0.0004 (0.0128)	batch 1.1224 (1.1284)	loss 2.5458 (2.3956)	grad_norm 1.6231 (1.6150)	mem 28740MB
Train: [3/180][1750/10009]	eta 2:35:17 lr 0.049962	data 0.0005 (0.0125)	batch 1.1194 (1.1282)	loss 2.4105 (2.3951)	grad_norm 1.7550 (1.6156)	mem 28740MB
Train: [3/180][1800/10009]	eta 2:34:19 lr 0.049962	data 0.0005 (0.0122)	batch 1.1079 (1.1279)	loss 2.4251 (2.3943)	grad_norm 1.6394 (1.6150)	mem 28740MB
Train: [3/180][1850/10009]	eta 2:33:22 lr 0.049961	data 0.0005 (0.0118)	batch 1.1414 (1.1279)	loss 2.1960 (2.3937)	grad_norm 1.5215 (1.6143)	mem 28740MB
Train: [3/180][1900/10009]	eta 2:32:24 lr 0.049961	data 0.0005 (0.0115)	batch 1.1287 (1.1277)	loss 2.3260 (2.3946)	grad_norm 1.6229 (1.6137)	mem 28740MB
Train: [3/180][1950/10009]	eta 2:31:26 lr 0.049961	data 0.0005 (0.0113)	batch 1.0970 (1.1275)	loss 2.1676 (2.3933)	grad_norm 1.4696 (1.6128)	mem 28740MB
Train: [3/180][2000/10009]	eta 2:30:28 lr 0.049961	data 0.0004 (0.0110)	batch 1.1191 (1.1273)	loss 2.3455 (2.3925)	grad_norm 1.5050 (1.6128)	mem 28740MB
Train: [3/180][2050/10009]	eta 2:29:30 lr 0.049961	data 0.0006 (0.0107)	batch 1.1041 (1.1271)	loss 2.5961 (2.3923)	grad_norm 1.5796 (1.6130)	mem 28740MB
Train: [3/180][2100/10009]	eta 2:28:33 lr 0.049961	data 0.0005 (0.0105)	batch 1.1355 (1.1270)	loss 2.4496 (2.3918)	grad_norm 1.6572 (1.6125)	mem 28740MB
Train: [3/180][2150/10009]	eta 2:27:35 lr 0.049961	data 0.0005 (0.0103)	batch 1.1216 (1.1268)	loss 2.2527 (2.3915)	grad_norm 1.5637 (1.6124)	mem 28740MB
Train: [3/180][2200/10009]	eta 2:26:38 lr 0.049961	data 0.0005 (0.0100)	batch 1.1431 (1.1267)	loss 2.6142 (2.3917)	grad_norm 1.6608 (1.6124)	mem 28740MB
Train: [3/180][2250/10009]	eta 2:25:41 lr 0.049960	data 0.0004 (0.0098)	batch 1.1373 (1.1266)	loss 2.1061 (2.3917)	grad_norm 1.4038 (1.6114)	mem 28740MB
Train: [3/180][2300/10009]	eta 2:24:44 lr 0.049960	data 0.0005 (0.0096)	batch 1.1117 (1.1265)	loss 2.2931 (2.3909)	grad_norm 1.5342 (1.6107)	mem 28740MB
Train: [3/180][2350/10009]	eta 2:23:46 lr 0.049960	data 0.0005 (0.0094)	batch 1.1315 (1.1264)	loss 2.2137 (2.3911)	grad_norm 1.7821 (1.6107)	mem 28740MB
Train: [3/180][2400/10009]	eta 2:22:49 lr 0.049960	data 0.0005 (0.0092)	batch 1.1397 (1.1263)	loss 2.2943 (2.3916)	grad_norm 1.4801 (1.6105)	mem 28740MB
Train: [3/180][2450/10009]	eta 2:21:51 lr 0.049960	data 0.0005 (0.0091)	batch 1.1119 (1.1260)	loss 2.8410 (2.3917)	grad_norm 1.6511 (1.6108)	mem 28740MB
Train: [3/180][2500/10009]	eta 2:20:54 lr 0.049960	data 0.0006 (0.0089)	batch 1.1215 (1.1259)	loss 2.5378 (2.3918)	grad_norm 1.6209 (1.6109)	mem 28740MB
Train: [3/180][2550/10009]	eta 2:19:56 lr 0.049960	data 0.0005 (0.0087)	batch 1.1147 (1.1257)	loss 2.4740 (2.3918)	grad_norm 1.6385 (1.6107)	mem 28740MB
Train: [3/180][2600/10009]	eta 2:18:59 lr 0.049960	data 0.0005 (0.0086)	batch 1.1011 (1.1256)	loss 2.4506 (2.3921)	grad_norm 1.5942 (1.6105)	mem 28740MB
Train: [3/180][2650/10009]	eta 2:18:02 lr 0.049959	data 0.0005 (0.0084)	batch 1.1048 (1.1255)	loss 2.3863 (2.3915)	grad_norm 1.6011 (1.6107)	mem 28740MB
Train: [3/180][2700/10009]	eta 2:17:05 lr 0.049959	data 0.0005 (0.0083)	batch 1.1118 (1.1253)	loss 2.3873 (2.3928)	grad_norm 1.6556 (1.6107)	mem 28740MB
Train: [3/180][2750/10009]	eta 2:16:07 lr 0.049959	data 0.0005 (0.0081)	batch 1.1172 (1.1252)	loss 2.4366 (2.3922)	grad_norm 1.5903 (1.6109)	mem 28740MB
Train: [3/180][2800/10009]	eta 2:15:10 lr 0.049959	data 0.0005 (0.0080)	batch 1.1129 (1.1251)	loss 2.3761 (2.3923)	grad_norm 1.5728 (1.6109)	mem 28740MB
Train: [3/180][2850/10009]	eta 2:14:13 lr 0.049959	data 0.0004 (0.0079)	batch 1.1160 (1.1249)	loss 2.6901 (2.3930)	grad_norm 1.6780 (1.6109)	mem 28740MB
Train: [3/180][2900/10009]	eta 2:13:16 lr 0.049959	data 0.0005 (0.0077)	batch 1.1265 (1.1248)	loss 2.4862 (2.3921)	grad_norm 1.6221 (1.6105)	mem 28740MB
Train: [3/180][2950/10009]	eta 2:12:19 lr 0.049959	data 0.0005 (0.0076)	batch 1.1101 (1.1247)	loss 2.7003 (2.3922)	grad_norm 1.6791 (1.6105)	mem 28740MB
Train: [3/180][3000/10009]	eta 2:11:22 lr 0.049959	data 0.0005 (0.0075)	batch 1.1186 (1.1246)	loss 2.1888 (2.3922)	grad_norm 1.5614 (1.6104)	mem 28740MB
Train: [3/180][3050/10009]	eta 2:10:26 lr 0.049958	data 0.0004 (0.0074)	batch 1.0963 (1.1246)	loss 2.4191 (2.3925)	grad_norm 1.5732 (1.6101)	mem 28740MB
Train: [3/180][3100/10009]	eta 2:09:29 lr 0.049958	data 0.0006 (0.0073)	batch 1.1135 (1.1245)	loss 2.3203 (2.3918)	grad_norm 1.4978 (1.6098)	mem 28740MB
Train: [3/180][3150/10009]	eta 2:08:32 lr 0.049958	data 0.0004 (0.0072)	batch 1.1026 (1.1245)	loss 2.2090 (2.3915)	grad_norm 1.4466 (1.6095)	mem 28740MB
Train: [3/180][3200/10009]	eta 2:07:36 lr 0.049958	data 0.0005 (0.0071)	batch 1.1227 (1.1244)	loss 2.2360 (2.3911)	grad_norm 1.5166 (1.6092)	mem 28740MB
Train: [3/180][3250/10009]	eta 2:06:39 lr 0.049958	data 0.0005 (0.0070)	batch 1.1247 (1.1244)	loss 2.4294 (2.3914)	grad_norm 1.6107 (1.6090)	mem 28740MB
Train: [3/180][3300/10009]	eta 2:05:43 lr 0.049958	data 0.0005 (0.0069)	batch 1.1170 (1.1243)	loss 2.4429 (2.3920)	grad_norm 1.4704 (1.6094)	mem 28740MB
Train: [3/180][3350/10009]	eta 2:04:46 lr 0.049958	data 0.0005 (0.0068)	batch 1.1380 (1.1243)	loss 2.4345 (2.3917)	grad_norm 1.6482 (1.6092)	mem 28740MB
Train: [3/180][3400/10009]	eta 2:03:50 lr 0.049958	data 0.0005 (0.0067)	batch 1.1005 (1.1242)	loss 2.2936 (2.3913)	grad_norm 1.5450 (1.6090)	mem 28740MB
Train: [3/180][3450/10009]	eta 2:02:53 lr 0.049957	data 0.0005 (0.0066)	batch 1.1063 (1.1241)	loss 2.2335 (2.3912)	grad_norm 1.6769 (1.6092)	mem 28740MB
Train: [3/180][3500/10009]	eta 2:01:56 lr 0.049957	data 0.0004 (0.0065)	batch 1.1399 (1.1241)	loss 2.3831 (2.3912)	grad_norm 1.5453 (1.6088)	mem 28740MB
Train: [3/180][3550/10009]	eta 2:00:59 lr 0.049957	data 0.0005 (0.0064)	batch 1.1158 (1.1240)	loss 2.4299 (2.3909)	grad_norm 1.4473 (1.6086)	mem 28740MB
Train: [3/180][3600/10009]	eta 2:00:03 lr 0.049957	data 0.0005 (0.0063)	batch 1.1293 (1.1240)	loss 2.4636 (2.3911)	grad_norm 1.5420 (1.6084)	mem 28740MB
Train: [3/180][3650/10009]	eta 1:59:07 lr 0.049957	data 0.0005 (0.0063)	batch 1.1412 (1.1240)	loss 2.5165 (2.3914)	grad_norm 1.5530 (1.6085)	mem 28740MB
Train: [3/180][3700/10009]	eta 1:58:10 lr 0.049957	data 0.0005 (0.0062)	batch 1.1155 (1.1239)	loss 2.4660 (2.3907)	grad_norm 1.4810 (1.6082)	mem 28740MB
Train: [3/180][3750/10009]	eta 1:57:14 lr 0.049957	data 0.0005 (0.0061)	batch 1.1387 (1.1239)	loss 2.7714 (2.3908)	grad_norm 1.4978 (1.6079)	mem 28740MB
Train: [3/180][3800/10009]	eta 1:56:17 lr 0.049957	data 0.0005 (0.0060)	batch 1.1168 (1.1238)	loss 2.7273 (2.3904)	grad_norm 1.6089 (1.6075)	mem 28740MB
Train: [3/180][3850/10009]	eta 1:55:21 lr 0.049956	data 0.0005 (0.0060)	batch 1.1141 (1.1238)	loss 2.5237 (2.3898)	grad_norm 1.6316 (1.6073)	mem 28740MB
Train: [3/180][3900/10009]	eta 1:54:24 lr 0.049956	data 0.0004 (0.0059)	batch 1.1130 (1.1237)	loss 2.2728 (2.3904)	grad_norm 1.6403 (1.6071)	mem 28740MB
Train: [3/180][3950/10009]	eta 1:53:28 lr 0.049956	data 0.0004 (0.0058)	batch 1.1090 (1.1237)	loss 2.3537 (2.3903)	grad_norm 1.6674 (1.6070)	mem 28740MB
Train: [3/180][4000/10009]	eta 1:52:31 lr 0.049956	data 0.0005 (0.0058)	batch 1.0979 (1.1236)	loss 2.4309 (2.3901)	grad_norm 1.6428 (1.6071)	mem 28740MB
Train: [3/180][4050/10009]	eta 1:51:35 lr 0.049956	data 0.0005 (0.0057)	batch 1.1013 (1.1236)	loss 2.2299 (2.3900)	grad_norm 1.5951 (1.6069)	mem 28740MB
Train: [3/180][4100/10009]	eta 1:50:39 lr 0.049956	data 0.0004 (0.0056)	batch 1.1128 (1.1236)	loss 2.3673 (2.3904)	grad_norm 1.4333 (1.6068)	mem 28740MB
Train: [3/180][4150/10009]	eta 1:49:42 lr 0.049956	data 0.0004 (0.0056)	batch 1.1376 (1.1235)	loss 2.5343 (2.3904)	grad_norm 1.6640 (1.6066)	mem 28740MB
Train: [3/180][4200/10009]	eta 1:48:46 lr 0.049955	data 0.0005 (0.0055)	batch 1.1326 (1.1235)	loss 2.5572 (2.3903)	grad_norm 1.4943 (1.6065)	mem 28740MB
Train: [3/180][4250/10009]	eta 1:47:50 lr 0.049955	data 0.0005 (0.0055)	batch 1.1179 (1.1235)	loss 2.5446 (2.3904)	grad_norm 1.5431 (1.6063)	mem 28740MB
Train: [3/180][4300/10009]	eta 1:46:54 lr 0.049955	data 0.0004 (0.0054)	batch 1.1085 (1.1235)	loss 2.2798 (2.3899)	grad_norm 1.5565 (1.6058)	mem 28740MB
Train: [3/180][4350/10009]	eta 1:45:57 lr 0.049955	data 0.0005 (0.0053)	batch 1.1198 (1.1235)	loss 2.6263 (2.3899)	grad_norm 1.7781 (1.6056)	mem 28740MB
Train: [3/180][4400/10009]	eta 1:45:01 lr 0.049955	data 0.0004 (0.0053)	batch 1.0970 (1.1234)	loss 2.3847 (2.3899)	grad_norm 1.4282 (1.6054)	mem 28740MB
Train: [3/180][4450/10009]	eta 1:44:04 lr 0.049955	data 0.0005 (0.0052)	batch 1.1202 (1.1234)	loss 2.5014 (2.3897)	grad_norm 1.6118 (1.6050)	mem 28740MB
Train: [3/180][4500/10009]	eta 1:43:08 lr 0.049955	data 0.0005 (0.0052)	batch 1.1184 (1.1233)	loss 2.1613 (2.3897)	grad_norm 1.4233 (1.6049)	mem 28740MB
Train: [3/180][4550/10009]	eta 1:42:12 lr 0.049955	data 0.0006 (0.0051)	batch 1.0998 (1.1233)	loss 2.3518 (2.3896)	grad_norm 1.5500 (1.6046)	mem 28740MB
Train: [3/180][4600/10009]	eta 1:41:15 lr 0.049954	data 0.0004 (0.0051)	batch 1.1193 (1.1232)	loss 2.6423 (2.3894)	grad_norm 1.7304 (1.6043)	mem 28740MB
Train: [3/180][4650/10009]	eta 1:40:18 lr 0.049954	data 0.0005 (0.0050)	batch 1.1060 (1.1231)	loss 2.5182 (2.3894)	grad_norm 1.6327 (1.6041)	mem 28740MB
Train: [3/180][4700/10009]	eta 1:39:21 lr 0.049954	data 0.0006 (0.0050)	batch 1.1128 (1.1230)	loss 2.6261 (2.3891)	grad_norm 1.5810 (1.6040)	mem 28740MB
